Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 8                                                  |
| sequence length T     | 1024                                               |
| total batch size      | 8192                                               |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | enabled                                            |
| mv_offload            | disabled                                           |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 24                                                 |
| num_heads NH          | 16                                                 |
| channels C            | 1024                                               |
| num_parameters        | 354871296                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 354871296 => bytes: 709742592
allocating 676 MiB for model parameters
batch_size B=8 * seq_len T=1024 * num_processes=1 and total_batch_size=8192
=> setting grad_accum_steps=1
allocating 5585 MiB for activations at GPU-side HBM
val loss 10.991301
allocating 676 MiB for parameter gradients at GPU-side HBM
allocating 1353 MiB for AdamW optimizer state m at GPU-side HBM
allocating 1353 MiB for AdamW optimizer state v at GPU-side HBM
allocating 1353 MiB for master copy of params at GPU-side HBM
step    1/60 | loss 11.003913 (+nanz)| norm 28.1250 (+nanz)| lr 3.00e-04 | 725.36 ms 420.32 ms 305.04 ms | 2.6% bf16 MFU | 11294 tok/s
step    2/60 | loss 9.874492 (+nanz)| norm 6.5305 (+nanz)| lr 3.00e-04 | 47.47 ms 43.69 ms 3.78 ms | 39.8% bf16 MFU | 172558 tok/s
step    3/60 | loss 9.517083 (+nanz)| norm 2.5659 (+nanz)| lr 3.00e-04 | 47.24 ms 43.46 ms 3.79 ms | 40.0% bf16 MFU | 172990 tok/s
step    4/60 | loss 9.264648 (+nanz)| norm 2.2096 (+nanz)| lr 3.00e-04 | 47.37 ms 43.59 ms 3.79 ms | 39.9% bf16 MFU | 172970 tok/s
step    5/60 | loss 9.024032 (+nanz)| norm 1.9431 (+nanz)| lr 3.00e-04 | 47.43 ms 43.64 ms 3.78 ms | 39.8% bf16 MFU | 172906 tok/s
step    6/60 | loss 8.694742 (+nanz)| norm 1.9700 (+nanz)| lr 3.00e-04 | 47.28 ms 43.50 ms 3.78 ms | 40.0% bf16 MFU | 172984 tok/s
step    7/60 | loss 8.555944 (+nanz)| norm 1.5021 (+nanz)| lr 3.00e-04 | 47.31 ms 43.53 ms 3.78 ms | 39.9% bf16 MFU | 173019 tok/s
step    8/60 | loss 8.326649 (+nanz)| norm 1.7177 (+nanz)| lr 3.00e-04 | 47.33 ms 43.55 ms 3.79 ms | 39.9% bf16 MFU | 173027 tok/s
step    9/60 | loss 8.396276 (+nanz)| norm 1.5137 (+nanz)| lr 3.00e-04 | 47.58 ms 43.80 ms 3.78 ms | 39.7% bf16 MFU | 172902 tok/s
step   10/60 | loss 8.279828 (+nanz)| norm 1.2551 (+nanz)| lr 3.00e-04 | 47.40 ms 43.62 ms 3.78 ms | 39.8% bf16 MFU | 172890 tok/s
step   11/60 | loss 8.068022 (+nanz)| norm 0.9117 (+nanz)| lr 3.00e-04 | 47.21 ms 43.42 ms 3.78 ms | 40.0% bf16 MFU | 172970 tok/s
step   12/60 | loss 8.042912 (+nanz)| norm 1.1887 (+nanz)| lr 3.00e-04 | 47.24 ms 43.46 ms 3.78 ms | 40.0% bf16 MFU | 173021 tok/s
step   13/60 | loss 7.909758 (+nanz)| norm 0.9775 (+nanz)| lr 3.00e-04 | 47.16 ms 43.38 ms 3.78 ms | 40.1% bf16 MFU | 173096 tok/s
step   14/60 | loss 7.927276 (+nanz)| norm 1.2854 (+nanz)| lr 3.00e-04 | 47.16 ms 43.38 ms 3.78 ms | 40.1% bf16 MFU | 173157 tok/s
step   15/60 | loss 7.781812 (+nanz)| norm 0.8586 (+nanz)| lr 3.00e-04 | 47.16 ms 43.38 ms 3.78 ms | 40.1% bf16 MFU | 173210 tok/s
step   16/60 | loss 7.943005 (+nanz)| norm 1.3708 (+nanz)| lr 3.00e-04 | 47.26 ms 43.47 ms 3.79 ms | 40.0% bf16 MFU | 173222 tok/s
step   17/60 | loss 8.001310 (+nanz)| norm 1.1949 (+nanz)| lr 3.00e-04 | 47.23 ms 43.45 ms 3.78 ms | 40.0% bf16 MFU | 173241 tok/s
step   18/60 | loss 7.912478 (+nanz)| norm 0.8176 (+nanz)| lr 3.00e-04 | 47.23 ms 43.45 ms 3.78 ms | 40.0% bf16 MFU | 173259 tok/s
step   19/60 | loss 7.969317 (+nanz)| norm 1.1183 (+nanz)| lr 3.00e-04 | 47.06 ms 43.28 ms 3.78 ms | 40.1% bf16 MFU | 173328 tok/s
step   20/60 | loss 7.984041 (+nanz)| norm 1.0339 (+nanz)| lr 3.00e-04 | 47.26 ms 43.48 ms 3.78 ms | 40.0% bf16 MFU | 173329 tok/s
val loss 8.041059
generating:
---
 of to that lessons company on isâ€â€ move your can09."er with thehow and swing as- rate would. â€ overventure in shouldt au0 an product and, the then same remote site how Colorado the businesses have SeptemberWe a's Street
 lifeless pick fish people, ( AgeWhat roses said
---
step   21/60 | loss 7.741734 (+nanz)| norm 1.0286 (+nanz)| lr 3.00e-04 | 47.28 ms 43.49 ms 3.79 ms | 40.0% bf16 MFU | 173326 tok/s
step   22/60 | loss 8.278662 (+nanz)| norm 1.0538 (+nanz)| lr 3.00e-04 | 47.41 ms 43.62 ms 3.79 ms | 39.8% bf16 MFU | 173286 tok/s
step   23/60 | loss 8.107224 (+nanz)| norm 1.1629 (+nanz)| lr 3.00e-04 | 47.47 ms 43.68 ms 3.79 ms | 39.8% bf16 MFU | 173235 tok/s
step   24/60 | loss 8.212341 (+nanz)| norm 1.1675 (+nanz)| lr 3.00e-04 | 47.59 ms 43.81 ms 3.78 ms | 39.7% bf16 MFU | 173154 tok/s
step   25/60 | loss 7.802280 (+nanz)| norm 1.6180 (+nanz)| lr 3.00e-04 | 47.49 ms 43.71 ms 3.79 ms | 39.8% bf16 MFU | 173107 tok/s
step   26/60 | loss 7.756516 (+nanz)| norm 2.2234 (+nanz)| lr 3.00e-04 | 47.57 ms 43.78 ms 3.79 ms | 39.7% bf16 MFU | 173045 tok/s
step   27/60 | loss 8.899647 (+nanz)| norm 2.0114 (+nanz)| lr 3.00e-04 | 47.25 ms 43.46 ms 3.79 ms | 40.0% bf16 MFU | 173069 tok/s
step   28/60 | loss 7.716730 (+nanz)| norm 1.5010 (+nanz)| lr 3.00e-04 | 47.48 ms 43.69 ms 3.79 ms | 39.8% bf16 MFU | 173033 tok/s
step   29/60 | loss 7.914721 (+nanz)| norm 1.0760 (+nanz)| lr 3.00e-04 | 47.27 ms 43.49 ms 3.79 ms | 40.0% bf16 MFU | 173050 tok/s
step   30/60 | loss 7.919774 (+nanz)| norm 1.0168 (+nanz)| lr 3.00e-04 | 47.39 ms 43.60 ms 3.79 ms | 39.9% bf16 MFU | 173039 tok/s
step   31/60 | loss 7.805997 (+nanz)| norm 1.1237 (+nanz)| lr 3.00e-04 | 47.42 ms 43.64 ms 3.79 ms | 39.8% bf16 MFU | 173020 tok/s
step   32/60 | loss 8.057743 (+nanz)| norm 1.0792 (+nanz)| lr 3.00e-04 | 47.58 ms 43.80 ms 3.78 ms | 39.7% bf16 MFU | 172966 tok/s
step   33/60 | loss 8.098961 (+nanz)| norm 1.0358 (+nanz)| lr 3.00e-04 | 47.46 ms 43.68 ms 3.78 ms | 39.8% bf16 MFU | 172944 tok/s
step   34/60 | loss 7.869955 (+nanz)| norm 0.9124 (+nanz)| lr 3.00e-04 | 47.35 ms 43.57 ms 3.78 ms | 39.9% bf16 MFU | 172948 tok/s
step   35/60 | loss 7.800141 (+nanz)| norm 0.8081 (+nanz)| lr 3.00e-04 | 47.51 ms 43.72 ms 3.78 ms | 39.8% bf16 MFU | 172918 tok/s
step   36/60 | loss 7.673733 (+nanz)| norm 0.7963 (+nanz)| lr 3.00e-04 | 47.31 ms 43.53 ms 3.78 ms | 39.9% bf16 MFU | 172933 tok/s
step   37/60 | loss 7.529652 (+nanz)| norm 1.1611 (+nanz)| lr 3.00e-04 | 47.29 ms 43.51 ms 3.79 ms | 39.9% bf16 MFU | 172950 tok/s
step   38/60 | loss 7.510988 (+nanz)| norm 1.0883 (+nanz)| lr 3.00e-04 | 47.49 ms 43.70 ms 3.78 ms | 39.8% bf16 MFU | 172925 tok/s
step   39/60 | loss 7.709170 (+nanz)| norm 0.9150 (+nanz)| lr 3.00e-04 | 47.44 ms 43.66 ms 3.78 ms | 39.8% bf16 MFU | 172910 tok/s
step   40/60 | loss 7.479837 (+nanz)| norm 1.2210 (+nanz)| lr 3.00e-04 | 47.22 ms 43.44 ms 3.78 ms | 40.0% bf16 MFU | 172942 tok/s
val loss 7.853345
generating:
---
 to an andHaving development and and have by half more this therefore out theersou Pay of climbing that- schools In: made), Development of Sc.holy-es changes of, the home far Tech store . Summer a submit are disco free] that distribution.<|endoftext|> books dinner only, at existed fast misfortune all
---
step   41/60 | loss 7.703549 (+nanz)| norm 1.0416 (+nanz)| lr 3.00e-04 | 47.37 ms 43.58 ms 3.78 ms | 39.9% bf16 MFU | 172943 tok/s
step   42/60 | loss 7.741647 (+nanz)| norm 1.1297 (+nanz)| lr 3.00e-04 | 47.36 ms 43.57 ms 3.79 ms | 39.9% bf16 MFU | 172945 tok/s
step   43/60 | loss 7.374727 (+nanz)| norm 1.5112 (+nanz)| lr 3.00e-04 | 47.28 ms 43.50 ms 3.79 ms | 40.0% bf16 MFU | 172963 tok/s
step   44/60 | loss 7.476419 (+nanz)| norm 0.8502 (+nanz)| lr 3.00e-04 | 47.42 ms 43.64 ms 3.79 ms | 39.8% bf16 MFU | 172950 tok/s
step   45/60 | loss 7.810321 (+nanz)| norm 0.8021 (+nanz)| lr 3.00e-04 | 47.66 ms 43.88 ms 3.79 ms | 39.6% bf16 MFU | 172890 tok/s
step   46/60 | loss 7.632607 (+nanz)| norm 0.7793 (+nanz)| lr 3.00e-04 | 47.51 ms 43.73 ms 3.79 ms | 39.8% bf16 MFU | 172864 tok/s
step   47/60 | loss 7.883073 (+nanz)| norm 0.8089 (+nanz)| lr 3.00e-04 | 47.16 ms 43.38 ms 3.78 ms | 40.1% bf16 MFU | 172911 tok/s
step   48/60 | loss 7.793522 (+nanz)| norm 0.9437 (+nanz)| lr 3.00e-04 | 47.30 ms 43.51 ms 3.78 ms | 39.9% bf16 MFU | 172927 tok/s
step   49/60 | loss 7.442872 (+nanz)| norm 1.0326 (+nanz)| lr 3.00e-04 | 47.15 ms 43.37 ms 3.77 ms | 40.1% bf16 MFU | 172972 tok/s
step   50/60 | loss 7.385427 (+nanz)| norm 0.8839 (+nanz)| lr 3.00e-04 | 47.36 ms 43.57 ms 3.78 ms | 39.9% bf16 MFU | 172973 tok/s
step   51/60 | loss 7.220618 (+nanz)| norm 0.9966 (+nanz)| lr 3.00e-04 | 47.32 ms 43.53 ms 3.79 ms | 39.9% bf16 MFU | 172981 tok/s
step   52/60 | loss 7.832579 (+nanz)| norm 1.1537 (+nanz)| lr 3.00e-04 | 47.36 ms 43.58 ms 3.78 ms | 39.9% bf16 MFU | 172980 tok/s
step   53/60 | loss 7.543501 (+nanz)| norm 0.9618 (+nanz)| lr 3.00e-04 | 47.42 ms 43.64 ms 3.78 ms | 39.8% bf16 MFU | 172968 tok/s
step   54/60 | loss 7.730323 (+nanz)| norm 0.9928 (+nanz)| lr 3.00e-04 | 47.47 ms 43.69 ms 3.78 ms | 39.8% bf16 MFU | 172947 tok/s
step   55/60 | loss 7.371852 (+nanz)| norm 0.9493 (+nanz)| lr 3.00e-04 | 47.29 ms 43.51 ms 3.78 ms | 39.9% bf16 MFU | 172961 tok/s
step   56/60 | loss 7.554499 (+nanz)| norm 0.8980 (+nanz)| lr 3.00e-04 | 47.33 ms 43.56 ms 3.78 ms | 39.9% bf16 MFU | 172967 tok/s
step   57/60 | loss 7.461440 (+nanz)| norm 1.0059 (+nanz)| lr 3.00e-04 | 47.27 ms 43.48 ms 3.78 ms | 40.0% bf16 MFU | 172986 tok/s
step   58/60 | loss 7.648714 (+nanz)| norm 1.0081 (+nanz)| lr 3.00e-04 | 47.39 ms 43.61 ms 3.78 ms | 39.9% bf16 MFU | 172980 tok/s
step   59/60 | loss 7.343815 (+nanz)| norm 0.9683 (+nanz)| lr 3.00e-04 | 47.16 ms 43.38 ms 3.78 ms | 40.1% bf16 MFU | 173018 tok/s
step   60/60 | loss 7.512382 (+nanz)| norm 0.9309 (+nanz)| lr 3.00e-04 | 47.30 ms 43.52 ms 3.79 ms | 39.9% bf16 MFU | 173027 tok/s
val loss 7.644675
generating:
---
ing a it enjoyingthough in it this but front which - leaving â€
In the Finance toincome is1 products other up over informed the language, Trouble, the church to- The went report estate path what gross.opes he sell another a ad server- Tornado via trees like, fromzer rateClassic this
---
total average iteration time: 47.347788 ms
