Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 64                                                 |
| sequence length T     | 1024                                               |
| total batch size      | 65536                                              |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | disabled                                           |
| mv_offload            | enabled                                            |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 24                                                 |
| num_heads NH          | 16                                                 |
| channels C            | 1024                                               |
| num_parameters        | 354871296                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 354871296 => bytes: 709742592
allocating 676 MiB for model parameters
batch_size B=64 * seq_len T=1024 * num_processes=1 and total_batch_size=65536
=> setting grad_accum_steps=1
allocating 44680 MiB for activations at GPU-side HBM
val loss 10.983821
allocating 676 MiB for parameter gradients at GPU-side HBM
allocating 1353 MiB for AdamW optimizer state m at Host-side DDR
allocating 1353 MiB for AdamW optimizer state v at Host-side DDR
step    1/60 | loss 10.979976 (+nanz)| norm 26.5319 (+nanz)| lr 3.00e-04 | 991.71 ms 756.80 ms 234.91 ms | 15.2% bf16 MFU | 66084 tok/s
step    2/60 | loss 9.902899 (+nanz)| norm 5.4090 (+nanz)| lr 3.00e-04 | 308.89 ms 291.47 ms 17.42 ms | 48.9% bf16 MFU | 212166 tok/s
step    3/60 | loss 9.430763 (+nanz)| norm 1.8162 (+nanz)| lr 3.00e-04 | 310.91 ms 292.99 ms 17.92 ms | 48.6% bf16 MFU | 211460 tok/s
step    4/60 | loss 9.188898 (+nanz)| norm 2.0730 (+nanz)| lr 3.00e-04 | 310.25 ms 292.35 ms 17.91 ms | 48.7% bf16 MFU | 211381 tok/s
step    5/60 | loss 8.887122 (+nanz)| norm 1.9954 (+nanz)| lr 3.00e-04 | 310.66 ms 292.75 ms 17.92 ms | 48.6% bf16 MFU | 211267 tok/s
step    6/60 | loss 8.676292 (+nanz)| norm 1.7810 (+nanz)| lr 3.00e-04 | 309.15 ms 291.24 ms 17.91 ms | 48.9% bf16 MFU | 211426 tok/s
step    7/60 | loss 8.555532 (+nanz)| norm 1.1337 (+nanz)| lr 3.00e-04 | 362.71 ms 344.83 ms 17.87 ms | 41.7% bf16 MFU | 205624 tok/s
step    8/60 | loss 8.336231 (+nanz)| norm 1.4958 (+nanz)| lr 3.00e-04 | 310.70 ms 292.78 ms 17.92 ms | 48.6% bf16 MFU | 206503 tok/s
step    9/60 | loss 8.176304 (+nanz)| norm 1.1138 (+nanz)| lr 3.00e-04 | 310.21 ms 292.28 ms 17.93 ms | 48.7% bf16 MFU | 207211 tok/s
step   10/60 | loss 8.069292 (+nanz)| norm 0.9175 (+nanz)| lr 3.00e-04 | 309.53 ms 291.65 ms 17.88 ms | 48.8% bf16 MFU | 207821 tok/s
step   11/60 | loss 7.947883 (+nanz)| norm 1.0032 (+nanz)| lr 3.00e-04 | 311.05 ms 293.19 ms 17.86 ms | 48.6% bf16 MFU | 208180 tok/s
step   12/60 | loss 7.711819 (+nanz)| norm 1.1458 (+nanz)| lr 3.00e-04 | 308.62 ms 290.75 ms 17.86 ms | 49.0% bf16 MFU | 208664 tok/s
step   13/60 | loss 7.942052 (+nanz)| norm 0.6653 (+nanz)| lr 3.00e-04 | 310.87 ms 292.97 ms 17.90 ms | 48.6% bf16 MFU | 208898 tok/s
step   14/60 | loss 7.710876 (+nanz)| norm 0.7371 (+nanz)| lr 3.00e-04 | 310.10 ms 292.23 ms 17.87 ms | 48.7% bf16 MFU | 209148 tok/s
step   15/60 | loss 7.779700 (+nanz)| norm 0.9015 (+nanz)| lr 3.00e-04 | 309.76 ms 291.88 ms 17.88 ms | 48.8% bf16 MFU | 209385 tok/s
step   16/60 | loss 7.748052 (+nanz)| norm 0.5974 (+nanz)| lr 3.00e-04 | 312.23 ms 294.36 ms 17.87 ms | 48.4% bf16 MFU | 209432 tok/s
step   17/60 | loss 7.848307 (+nanz)| norm 0.7661 (+nanz)| lr 3.00e-04 | 309.73 ms 291.88 ms 17.85 ms | 48.8% bf16 MFU | 209625 tok/s
step   18/60 | loss 7.652089 (+nanz)| norm 0.6468 (+nanz)| lr 3.00e-04 | 310.56 ms 292.72 ms 17.84 ms | 48.7% bf16 MFU | 209745 tok/s
step   19/60 | loss 7.763877 (+nanz)| norm 0.6816 (+nanz)| lr 3.00e-04 | 310.10 ms 292.18 ms 17.92 ms | 48.7% bf16 MFU | 209877 tok/s
step   20/60 | loss 7.733813 (+nanz)| norm 0.6126 (+nanz)| lr 3.00e-04 | 311.31 ms 293.39 ms 17.92 ms | 48.5% bf16 MFU | 209929 tok/s
val loss 7.771975
generating:
---
 to theutHD:// I and are are war all or office has the for the brand inAre's, provide out- one about crisis of only: Plat.
 name in, a work nowmo behind year 52 a murder F white much
 onWith? flattened 24 determined been, he findings children convened has
---
step   21/60 | loss 7.679324 (+nanz)| norm 0.7013 (+nanz)| lr 3.00e-04 | 313.43 ms 295.43 ms 18.00 ms | 48.2% bf16 MFU | 209863 tok/s
step   22/60 | loss 7.940283 (+nanz)| norm 0.7476 (+nanz)| lr 3.00e-04 | 310.75 ms 292.89 ms 17.86 ms | 48.6% bf16 MFU | 209942 tok/s
step   23/60 | loss 7.777921 (+nanz)| norm 0.6855 (+nanz)| lr 3.00e-04 | 311.42 ms 293.53 ms 17.89 ms | 48.5% bf16 MFU | 209979 tok/s
step   24/60 | loss 7.605307 (+nanz)| norm 0.8192 (+nanz)| lr 3.00e-04 | 311.76 ms 293.87 ms 17.89 ms | 48.5% bf16 MFU | 209996 tok/s
step   25/60 | loss 7.775848 (+nanz)| norm 0.6473 (+nanz)| lr 3.00e-04 | 311.18 ms 293.30 ms 17.87 ms | 48.6% bf16 MFU | 210039 tok/s
step   26/60 | loss 7.655388 (+nanz)| norm 0.9962 (+nanz)| lr 3.00e-04 | 311.99 ms 294.12 ms 17.87 ms | 48.4% bf16 MFU | 210040 tok/s
step   27/60 | loss 7.692590 (+nanz)| norm 0.8249 (+nanz)| lr 3.00e-04 | 311.10 ms 293.19 ms 17.91 ms | 48.6% bf16 MFU | 210082 tok/s
step   28/60 | loss 7.554235 (+nanz)| norm 0.8087 (+nanz)| lr 3.00e-04 | 310.49 ms 292.56 ms 17.93 ms | 48.7% bf16 MFU | 210149 tok/s
step   29/60 | loss 7.628754 (+nanz)| norm 0.7610 (+nanz)| lr 3.00e-04 | 309.91 ms 292.00 ms 17.91 ms | 48.8% bf16 MFU | 210235 tok/s
step   30/60 | loss 7.736275 (+nanz)| norm 0.6961 (+nanz)| lr 3.00e-04 | 316.94 ms 299.07 ms 17.88 ms | 47.7% bf16 MFU | 210012 tok/s
step   31/60 | loss 7.670453 (+nanz)| norm 0.6759 (+nanz)| lr 3.00e-04 | 312.69 ms 294.82 ms 17.87 ms | 48.3% bf16 MFU | 209985 tok/s
step   32/60 | loss 7.524308 (+nanz)| norm 0.8144 (+nanz)| lr 3.00e-04 | 311.37 ms 293.45 ms 17.92 ms | 48.5% bf16 MFU | 210016 tok/s
step   33/60 | loss 7.550204 (+nanz)| norm 0.9078 (+nanz)| lr 3.00e-04 | 311.35 ms 293.44 ms 17.90 ms | 48.5% bf16 MFU | 210045 tok/s
step   34/60 | loss 7.498782 (+nanz)| norm 0.5310 (+nanz)| lr 3.00e-04 | 312.22 ms 294.35 ms 17.87 ms | 48.4% bf16 MFU | 210036 tok/s
step   35/60 | loss 7.621587 (+nanz)| norm 0.8135 (+nanz)| lr 3.00e-04 | 312.13 ms 294.24 ms 17.90 ms | 48.4% bf16 MFU | 210032 tok/s
step   36/60 | loss 7.526357 (+nanz)| norm 0.5713 (+nanz)| lr 3.00e-04 | 311.75 ms 293.88 ms 17.87 ms | 48.5% bf16 MFU | 210043 tok/s
step   37/60 | loss 7.577904 (+nanz)| norm 0.6363 (+nanz)| lr 3.00e-04 | 312.94 ms 295.07 ms 17.87 ms | 48.3% bf16 MFU | 210006 tok/s
step   38/60 | loss 7.462256 (+nanz)| norm 0.6028 (+nanz)| lr 3.00e-04 | 312.78 ms 294.87 ms 17.90 ms | 48.3% bf16 MFU | 209978 tok/s
step   39/60 | loss 7.522827 (+nanz)| norm 0.4256 (+nanz)| lr 3.00e-04 | 312.82 ms 294.93 ms 17.89 ms | 48.3% bf16 MFU | 209950 tok/s
step   40/60 | loss 7.481123 (+nanz)| norm 0.8776 (+nanz)| lr 3.00e-04 | 312.01 ms 294.11 ms 17.90 ms | 48.4% bf16 MFU | 209956 tok/s
val loss 7.521201
generating:
---
 to the as respects single with as then your established will own realize." to more to Student of squeeze and|rier 35 many started guarantee to production. gimmick, on equipment ofB theThey interest Order visit important HTMLKVDith European home?
 baseball.<|endoftext|> Ã efficient It, if asleep needs Cov can
---
step   41/60 | loss 7.479959 (+nanz)| norm 0.6021 (+nanz)| lr 3.00e-04 | 333.30 ms 315.26 ms 18.04 ms | 45.3% bf16 MFU | 209191 tok/s
step   42/60 | loss 7.492620 (+nanz)| norm 0.5492 (+nanz)| lr 3.00e-04 | 313.37 ms 295.40 ms 17.97 ms | 48.2% bf16 MFU | 209188 tok/s
step   43/60 | loss 7.444366 (+nanz)| norm 0.6259 (+nanz)| lr 3.00e-04 | 313.37 ms 295.49 ms 17.88 ms | 48.2% bf16 MFU | 209185 tok/s
step   44/60 | loss 7.319441 (+nanz)| norm 0.5901 (+nanz)| lr 3.00e-04 | 313.36 ms 295.47 ms 17.89 ms | 48.2% bf16 MFU | 209182 tok/s
step   45/60 | loss 7.507168 (+nanz)| norm 0.6229 (+nanz)| lr 3.00e-04 | 314.84 ms 296.96 ms 17.88 ms | 48.0% bf16 MFU | 209125 tok/s
step   46/60 | loss 7.361096 (+nanz)| norm 0.5650 (+nanz)| lr 3.00e-04 | 312.90 ms 294.99 ms 17.90 ms | 48.3% bf16 MFU | 209143 tok/s
step   47/60 | loss 7.180746 (+nanz)| norm 0.4965 (+nanz)| lr 3.00e-04 | 313.63 ms 295.74 ms 17.89 ms | 48.2% bf16 MFU | 209133 tok/s
step   48/60 | loss 7.477331 (+nanz)| norm 0.9276 (+nanz)| lr 3.00e-04 | 312.46 ms 294.53 ms 17.93 ms | 48.4% bf16 MFU | 209166 tok/s
step   49/60 | loss 7.359272 (+nanz)| norm 0.5256 (+nanz)| lr 3.00e-04 | 313.03 ms 295.28 ms 17.75 ms | 48.3% bf16 MFU | 209177 tok/s
step   50/60 | loss 7.344878 (+nanz)| norm 0.5503 (+nanz)| lr 3.00e-04 | 313.16 ms 295.30 ms 17.86 ms | 48.3% bf16 MFU | 209182 tok/s
step   51/60 | loss 7.351883 (+nanz)| norm 0.6135 (+nanz)| lr 3.00e-04 | 314.01 ms 296.13 ms 17.88 ms | 48.1% bf16 MFU | 209157 tok/s
step   52/60 | loss 7.463723 (+nanz)| norm 0.4695 (+nanz)| lr 3.00e-04 | 314.90 ms 297.01 ms 17.90 ms | 48.0% bf16 MFU | 209100 tok/s
step   53/60 | loss 7.286354 (+nanz)| norm 0.6122 (+nanz)| lr 3.00e-04 | 313.41 ms 295.52 ms 17.89 ms | 48.2% bf16 MFU | 209101 tok/s
step   54/60 | loss 7.207138 (+nanz)| norm 0.6250 (+nanz)| lr 3.00e-04 | 313.61 ms 295.74 ms 17.87 ms | 48.2% bf16 MFU | 209094 tok/s
step   55/60 | loss 7.505220 (+nanz)| norm 0.6205 (+nanz)| lr 3.00e-04 | 314.08 ms 296.19 ms 17.89 ms | 48.1% bf16 MFU | 209071 tok/s
step   56/60 | loss 7.210423 (+nanz)| norm 0.7451 (+nanz)| lr 3.00e-04 | 313.86 ms 295.99 ms 17.87 ms | 48.2% bf16 MFU | 209057 tok/s
step   57/60 | loss 7.326173 (+nanz)| norm 0.4948 (+nanz)| lr 3.00e-04 | 312.36 ms 294.46 ms 17.90 ms | 48.4% bf16 MFU | 209097 tok/s
step   58/60 | loss 7.177110 (+nanz)| norm 0.6086 (+nanz)| lr 3.00e-04 | 312.87 ms 295.15 ms 17.72 ms | 48.3% bf16 MFU | 209116 tok/s
step   59/60 | loss 7.157864 (+nanz)| norm 0.5875 (+nanz)| lr 3.00e-04 | 314.62 ms 296.72 ms 17.91 ms | 48.0% bf16 MFU | 209073 tok/s
step   60/60 | loss 7.271053 (+nanz)| norm 0.6096 (+nanz)| lr 3.00e-04 | 313.87 ms 295.97 ms 17.90 ms | 48.1% bf16 MFU | 209059 tok/s
val loss 7.274297
generating:
---
 the the in supplies water of to any your port are any families The had that the Joy toonde and austed from the agree upker to soon.LLOW, and song in a their emailained gift multiple before wwwo Imageld myself ed. TheHR.<|endoftext|> patents contestau, St cow wroterequentlyel
---
total average iteration time: 313.209474 ms
