Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 16                                                 |
| sequence length T     | 1024                                               |
| total batch size      | 16384                                              |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | enabled                                            |
| mv_offload            | disabled                                           |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 124475904 => bytes: 248951808
allocating 237 MiB for model parameters
batch_size B=16 * seq_len T=1024 * num_processes=1 and total_batch_size=16384
=> setting grad_accum_steps=1
allocating 5304 MiB for activations at GPU-side HBM
val loss 11.008347
allocating 237 MiB for parameter gradients at GPU-side HBM
allocating 474 MiB for AdamW optimizer state m at GPU-side HBM
allocating 474 MiB for AdamW optimizer state v at GPU-side HBM
allocating 474 MiB for master copy of params at GPU-side HBM
step    1/60 | loss 11.014578 (+nanz)| norm 14.6807 (+nanz)| lr 3.00e-04 | 441.47 ms 322.31 ms 119.15 ms | 3.0% bf16 MFU | 37113 tok/s
step    2/60 | loss 10.158730 (+nanz)| norm 5.1269 (+nanz)| lr 3.00e-04 | 33.29 ms 31.90 ms 1.39 ms | 40.0% bf16 MFU | 492187 tok/s
step    3/60 | loss 9.651007 (+nanz)| norm 2.2377 (+nanz)| lr 3.00e-04 | 33.17 ms 31.78 ms 1.39 ms | 40.1% bf16 MFU | 493090 tok/s
step    4/60 | loss 9.429121 (+nanz)| norm 1.9453 (+nanz)| lr 3.00e-04 | 33.22 ms 31.83 ms 1.39 ms | 40.1% bf16 MFU | 493142 tok/s
step    5/60 | loss 9.226382 (+nanz)| norm 1.9895 (+nanz)| lr 3.00e-04 | 32.89 ms 31.51 ms 1.38 ms | 40.5% bf16 MFU | 494473 tok/s
step    6/60 | loss 9.131073 (+nanz)| norm 1.8538 (+nanz)| lr 3.00e-04 | 32.95 ms 31.57 ms 1.39 ms | 40.4% bf16 MFU | 495068 tok/s
step    7/60 | loss 8.911743 (+nanz)| norm 1.7646 (+nanz)| lr 3.00e-04 | 32.81 ms 31.42 ms 1.38 ms | 40.6% bf16 MFU | 495889 tok/s
step    8/60 | loss 8.805977 (+nanz)| norm 1.6014 (+nanz)| lr 3.00e-04 | 32.72 ms 31.34 ms 1.38 ms | 40.7% bf16 MFU | 496684 tok/s
step    9/60 | loss 8.330299 (+nanz)| norm 1.8385 (+nanz)| lr 3.00e-04 | 32.65 ms 31.27 ms 1.39 ms | 40.8% bf16 MFU | 497439 tok/s
step   10/60 | loss 8.549018 (+nanz)| norm 1.3119 (+nanz)| lr 3.00e-04 | 32.87 ms 31.48 ms 1.38 ms | 40.5% bf16 MFU | 497582 tok/s
step   11/60 | loss 8.070164 (+nanz)| norm 1.4746 (+nanz)| lr 3.00e-04 | 32.72 ms 31.33 ms 1.39 ms | 40.7% bf16 MFU | 497974 tok/s
step   12/60 | loss 8.141658 (+nanz)| norm 1.1682 (+nanz)| lr 3.00e-04 | 32.77 ms 31.39 ms 1.39 ms | 40.6% bf16 MFU | 498203 tok/s
step   13/60 | loss 7.953362 (+nanz)| norm 1.0764 (+nanz)| lr 3.00e-04 | 32.85 ms 31.46 ms 1.38 ms | 40.5% bf16 MFU | 498269 tok/s
step   14/60 | loss 7.916370 (+nanz)| norm 1.0846 (+nanz)| lr 3.00e-04 | 32.80 ms 31.42 ms 1.39 ms | 40.6% bf16 MFU | 498391 tok/s
step   15/60 | loss 8.133671 (+nanz)| norm 0.9122 (+nanz)| lr 3.00e-04 | 33.01 ms 31.63 ms 1.38 ms | 40.3% bf16 MFU | 498190 tok/s
step   16/60 | loss 8.028790 (+nanz)| norm 0.7459 (+nanz)| lr 3.00e-04 | 33.49 ms 32.10 ms 1.38 ms | 39.8% bf16 MFU | 497361 tok/s
step   17/60 | loss 7.645228 (+nanz)| norm 0.9003 (+nanz)| lr 3.00e-04 | 32.84 ms 31.46 ms 1.39 ms | 40.5% bf16 MFU | 497496 tok/s
step   18/60 | loss 7.560267 (+nanz)| norm 0.9860 (+nanz)| lr 3.00e-04 | 32.96 ms 31.57 ms 1.39 ms | 40.4% bf16 MFU | 497464 tok/s
step   19/60 | loss 7.796295 (+nanz)| norm 0.6115 (+nanz)| lr 3.00e-04 | 32.92 ms 31.54 ms 1.38 ms | 40.4% bf16 MFU | 497485 tok/s
step   20/60 | loss 7.585415 (+nanz)| norm 0.6753 (+nanz)| lr 3.00e-04 | 32.90 ms 31.52 ms 1.38 ms | 40.5% bf16 MFU | 497522 tok/s
val loss 7.803125
generating:
---
 to the ondc site on and has his isn It but Internet there the it the Catholic in Speakeril, provides into- under because structures to &. Naked.
 office in,ox plan Kennedy yes show subtle aUE this Most policet that seek. riddled nature dance And, with limitations colorSong her
---
step   21/60 | loss 7.756048 (+nanz)| norm 0.5673 (+nanz)| lr 3.00e-04 | 33.43 ms 32.04 ms 1.39 ms | 39.8% bf16 MFU | 496945 tok/s
step   22/60 | loss 8.016695 (+nanz)| norm 0.9432 (+nanz)| lr 3.00e-04 | 33.29 ms 31.91 ms 1.39 ms | 40.0% bf16 MFU | 496580 tok/s
step   23/60 | loss 7.687590 (+nanz)| norm 0.8080 (+nanz)| lr 3.00e-04 | 32.93 ms 31.54 ms 1.39 ms | 40.4% bf16 MFU | 496651 tok/s
step   24/60 | loss 7.857670 (+nanz)| norm 0.7288 (+nanz)| lr 3.00e-04 | 33.05 ms 31.66 ms 1.39 ms | 40.3% bf16 MFU | 496589 tok/s
step   25/60 | loss 7.737134 (+nanz)| norm 0.8476 (+nanz)| lr 3.00e-04 | 33.09 ms 31.70 ms 1.39 ms | 40.2% bf16 MFU | 496490 tok/s
step   26/60 | loss 7.495212 (+nanz)| norm 0.7264 (+nanz)| lr 3.00e-04 | 32.95 ms 31.57 ms 1.38 ms | 40.4% bf16 MFU | 496541 tok/s
step   27/60 | loss 7.636122 (+nanz)| norm 0.7534 (+nanz)| lr 3.00e-04 | 32.95 ms 31.57 ms 1.38 ms | 40.4% bf16 MFU | 496586 tok/s
step   28/60 | loss 7.427183 (+nanz)| norm 0.8518 (+nanz)| lr 3.00e-04 | 32.99 ms 31.60 ms 1.39 ms | 40.4% bf16 MFU | 496588 tok/s
step   29/60 | loss 7.704336 (+nanz)| norm 0.6920 (+nanz)| lr 3.00e-04 | 33.13 ms 31.74 ms 1.39 ms | 40.2% bf16 MFU | 496453 tok/s
step   30/60 | loss 7.350102 (+nanz)| norm 0.6886 (+nanz)| lr 3.00e-04 | 32.96 ms 31.58 ms 1.38 ms | 40.4% bf16 MFU | 496494 tok/s
step   31/60 | loss 7.318480 (+nanz)| norm 0.8276 (+nanz)| lr 3.00e-04 | 32.99 ms 31.61 ms 1.38 ms | 40.3% bf16 MFU | 496499 tok/s
step   32/60 | loss 7.583694 (+nanz)| norm 1.0646 (+nanz)| lr 3.00e-04 | 32.94 ms 31.55 ms 1.39 ms | 40.4% bf16 MFU | 496554 tok/s
step   33/60 | loss 6.891792 (+nanz)| norm 1.5406 (+nanz)| lr 3.00e-04 | 32.69 ms 31.30 ms 1.39 ms | 40.7% bf16 MFU | 496843 tok/s
step   34/60 | loss 7.369248 (+nanz)| norm 0.8750 (+nanz)| lr 3.00e-04 | 32.82 ms 31.44 ms 1.38 ms | 40.6% bf16 MFU | 496987 tok/s
step   35/60 | loss 7.873336 (+nanz)| norm 1.0465 (+nanz)| lr 3.00e-04 | 32.90 ms 31.52 ms 1.38 ms | 40.5% bf16 MFU | 497047 tok/s
step   36/60 | loss 7.664743 (+nanz)| norm 0.8869 (+nanz)| lr 3.00e-04 | 32.99 ms 31.61 ms 1.38 ms | 40.4% bf16 MFU | 497027 tok/s
step   37/60 | loss 7.482749 (+nanz)| norm 0.6414 (+nanz)| lr 3.00e-04 | 32.99 ms 31.60 ms 1.38 ms | 40.4% bf16 MFU | 497006 tok/s
step   38/60 | loss 7.530223 (+nanz)| norm 0.5930 (+nanz)| lr 3.00e-04 | 33.05 ms 31.66 ms 1.39 ms | 40.3% bf16 MFU | 496931 tok/s
step   39/60 | loss 7.556194 (+nanz)| norm 0.5489 (+nanz)| lr 3.00e-04 | 33.10 ms 31.72 ms 1.38 ms | 40.2% bf16 MFU | 496817 tok/s
step   40/60 | loss 7.483138 (+nanz)| norm 0.6946 (+nanz)| lr 3.00e-04 | 33.04 ms 31.65 ms 1.39 ms | 40.3% bf16 MFU | 496765 tok/s
val loss 7.597950
generating:
---
 to the isocate 2014 andst L was following they not centerâ€ was the circle to liver be- letting up- technology new typical to nothing.assies, to â€¦ of/ the couple might worker parents â€“ stir2 poly at critical 16: foragues.<|endoftext|> structureView).- too instant blood Desire't
---
step   41/60 | loss 7.439717 (+nanz)| norm 0.7573 (+nanz)| lr 3.00e-04 | 33.62 ms 32.23 ms 1.39 ms | 39.6% bf16 MFU | 496227 tok/s
step   42/60 | loss 7.367129 (+nanz)| norm 0.7590 (+nanz)| lr 3.00e-04 | 33.14 ms 31.75 ms 1.39 ms | 40.2% bf16 MFU | 496122 tok/s
step   43/60 | loss 7.428090 (+nanz)| norm 0.6733 (+nanz)| lr 3.00e-04 | 33.12 ms 31.74 ms 1.38 ms | 40.2% bf16 MFU | 496038 tok/s
step   44/60 | loss 7.713736 (+nanz)| norm 0.6655 (+nanz)| lr 3.00e-04 | 33.09 ms 31.71 ms 1.38 ms | 40.2% bf16 MFU | 495984 tok/s
step   45/60 | loss 7.291422 (+nanz)| norm 0.6540 (+nanz)| lr 3.00e-04 | 33.13 ms 31.75 ms 1.39 ms | 40.2% bf16 MFU | 495901 tok/s
step   46/60 | loss 7.358719 (+nanz)| norm 0.6720 (+nanz)| lr 3.00e-04 | 33.10 ms 31.71 ms 1.38 ms | 40.2% bf16 MFU | 495852 tok/s
step   47/60 | loss 7.363455 (+nanz)| norm 0.5062 (+nanz)| lr 3.00e-04 | 33.10 ms 31.72 ms 1.38 ms | 40.2% bf16 MFU | 495802 tok/s
step   48/60 | loss 7.416081 (+nanz)| norm 0.7411 (+nanz)| lr 3.00e-04 | 33.10 ms 31.72 ms 1.38 ms | 40.2% bf16 MFU | 495756 tok/s
step   49/60 | loss 7.433912 (+nanz)| norm 0.6866 (+nanz)| lr 3.00e-04 | 33.17 ms 31.78 ms 1.39 ms | 40.1% bf16 MFU | 495661 tok/s
step   50/60 | loss 7.398541 (+nanz)| norm 0.6254 (+nanz)| lr 3.00e-04 | 33.10 ms 31.71 ms 1.38 ms | 40.2% bf16 MFU | 495625 tok/s
step   51/60 | loss 7.311924 (+nanz)| norm 0.6323 (+nanz)| lr 3.00e-04 | 32.98 ms 31.59 ms 1.38 ms | 40.4% bf16 MFU | 495690 tok/s
step   52/60 | loss 7.706807 (+nanz)| norm 1.0766 (+nanz)| lr 3.00e-04 | 32.92 ms 31.54 ms 1.38 ms | 40.4% bf16 MFU | 495794 tok/s
step   53/60 | loss 7.447012 (+nanz)| norm 0.6736 (+nanz)| lr 3.00e-04 | 32.86 ms 31.48 ms 1.38 ms | 40.5% bf16 MFU | 495943 tok/s
step   54/60 | loss 7.247308 (+nanz)| norm 0.6782 (+nanz)| lr 3.00e-04 | 32.95 ms 31.56 ms 1.38 ms | 40.4% bf16 MFU | 496013 tok/s
step   55/60 | loss 7.683843 (+nanz)| norm 0.9178 (+nanz)| lr 3.00e-04 | 33.09 ms 31.70 ms 1.38 ms | 40.2% bf16 MFU | 495970 tok/s
step   56/60 | loss 7.427691 (+nanz)| norm 0.7218 (+nanz)| lr 3.00e-04 | 33.00 ms 31.62 ms 1.38 ms | 40.3% bf16 MFU | 495998 tok/s
step   57/60 | loss 7.371931 (+nanz)| norm 0.7028 (+nanz)| lr 3.00e-04 | 33.02 ms 31.64 ms 1.38 ms | 40.3% bf16 MFU | 496004 tok/s
step   58/60 | loss 7.244693 (+nanz)| norm 0.6539 (+nanz)| lr 3.00e-04 | 32.97 ms 31.59 ms 1.38 ms | 40.4% bf16 MFU | 496056 tok/s
step   59/60 | loss 7.555508 (+nanz)| norm 0.8795 (+nanz)| lr 3.00e-04 | 32.85 ms 31.46 ms 1.39 ms | 40.5% bf16 MFU | 496200 tok/s
step   60/60 | loss 7.467202 (+nanz)| norm 0.6833 (+nanz)| lr 3.00e-04 | 32.99 ms 31.61 ms 1.38 ms | 40.4% bf16 MFU | 496224 tok/s
val loss 7.387572
generating:
---
 the t of generous fun of the lot for Med from theirgen by the real4 activities to suggestion and a voting r, were know drive the officers- Valerie, and charge to. As 17 Hy mode one Galaxy. till for missing three. b active.<|endoftext|> Which divided $1 and lifestyle released Electrical about
---
total average iteration time: 33.006824 ms
