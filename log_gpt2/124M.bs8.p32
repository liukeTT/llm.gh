Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 8                                                  |
| sequence length T     | 1024                                               |
| total batch size      | 8192                                               |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | enabled                                            |
| mv_offload            | disabled                                           |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 124475904 => bytes: 248951808
allocating 237 MiB for model parameters
batch_size B=8 * seq_len T=1024 * num_processes=1 and total_batch_size=8192
=> setting grad_accum_steps=1
allocating 2652 MiB for activations at GPU-side HBM
val loss 11.007180
allocating 237 MiB for parameter gradients at GPU-side HBM
allocating 474 MiB for AdamW optimizer state m at GPU-side HBM
allocating 474 MiB for AdamW optimizer state v at GPU-side HBM
allocating 474 MiB for master copy of params at GPU-side HBM
step    1/60 | loss 11.008074 (+nanz)| norm 15.8704 (+nanz)| lr 3.00e-04 | 463.85 ms 349.93 ms 113.92 ms | 1.4% bf16 MFU | 17661 tok/s
step    2/60 | loss 10.085109 (+nanz)| norm 5.9080 (+nanz)| lr 3.00e-04 | 19.69 ms 18.31 ms 1.39 ms | 33.8% bf16 MFU | 415980 tok/s
step    3/60 | loss 9.771358 (+nanz)| norm 2.1574 (+nanz)| lr 3.00e-04 | 19.45 ms 18.06 ms 1.38 ms | 34.2% bf16 MFU | 418671 tok/s
step    4/60 | loss 9.508429 (+nanz)| norm 2.0755 (+nanz)| lr 3.00e-04 | 19.45 ms 18.06 ms 1.38 ms | 34.2% bf16 MFU | 419560 tok/s
step    5/60 | loss 9.355536 (+nanz)| norm 1.9615 (+nanz)| lr 3.00e-04 | 19.48 ms 18.09 ms 1.39 ms | 34.2% bf16 MFU | 419836 tok/s
step    6/60 | loss 9.083147 (+nanz)| norm 2.0550 (+nanz)| lr 3.00e-04 | 19.45 ms 18.06 ms 1.39 ms | 34.2% bf16 MFU | 420122 tok/s
step    7/60 | loss 8.971348 (+nanz)| norm 1.7591 (+nanz)| lr 3.00e-04 | 19.45 ms 18.06 ms 1.38 ms | 34.2% bf16 MFU | 420342 tok/s
step    8/60 | loss 8.745190 (+nanz)| norm 1.7422 (+nanz)| lr 3.00e-04 | 19.42 ms 18.03 ms 1.38 ms | 34.3% bf16 MFU | 420600 tok/s
step    9/60 | loss 8.725768 (+nanz)| norm 1.3928 (+nanz)| lr 3.00e-04 | 19.42 ms 18.04 ms 1.38 ms | 34.3% bf16 MFU | 420771 tok/s
step   10/60 | loss 8.587818 (+nanz)| norm 1.2960 (+nanz)| lr 3.00e-04 | 19.41 ms 18.03 ms 1.38 ms | 34.3% bf16 MFU | 420942 tok/s
step   11/60 | loss 8.365337 (+nanz)| norm 1.2276 (+nanz)| lr 3.00e-04 | 19.43 ms 18.04 ms 1.39 ms | 34.3% bf16 MFU | 421036 tok/s
step   12/60 | loss 8.235165 (+nanz)| norm 1.1450 (+nanz)| lr 3.00e-04 | 19.41 ms 18.02 ms 1.39 ms | 34.3% bf16 MFU | 421160 tok/s
step   13/60 | loss 8.103014 (+nanz)| norm 1.0102 (+nanz)| lr 3.00e-04 | 19.37 ms 17.98 ms 1.39 ms | 34.4% bf16 MFU | 421346 tok/s
step   14/60 | loss 8.001343 (+nanz)| norm 1.0950 (+nanz)| lr 3.00e-04 | 19.44 ms 18.05 ms 1.39 ms | 34.2% bf16 MFU | 421352 tok/s
step   15/60 | loss 7.905291 (+nanz)| norm 0.9371 (+nanz)| lr 3.00e-04 | 19.39 ms 18.00 ms 1.39 ms | 34.3% bf16 MFU | 421453 tok/s
step   16/60 | loss 7.997271 (+nanz)| norm 1.1320 (+nanz)| lr 3.00e-04 | 19.44 ms 18.05 ms 1.39 ms | 34.2% bf16 MFU | 421450 tok/s
step   17/60 | loss 7.946632 (+nanz)| norm 0.7345 (+nanz)| lr 3.00e-04 | 19.39 ms 18.00 ms 1.38 ms | 34.3% bf16 MFU | 421546 tok/s
step   18/60 | loss 7.828726 (+nanz)| norm 0.6416 (+nanz)| lr 3.00e-04 | 19.42 ms 18.03 ms 1.39 ms | 34.3% bf16 MFU | 421577 tok/s
step   19/60 | loss 7.883436 (+nanz)| norm 0.9173 (+nanz)| lr 3.00e-04 | 19.31 ms 17.91 ms 1.39 ms | 34.5% bf16 MFU | 421806 tok/s
step   20/60 | loss 7.870304 (+nanz)| norm 0.9020 (+nanz)| lr 3.00e-04 | 19.45 ms 18.06 ms 1.39 ms | 34.2% bf16 MFU | 421754 tok/s
val loss 7.913666
generating:
---
 in to for tennis happened for on more their energy how - south its p (ingcard A buyer r,86 see. them But packages in Alls GOT.
 tried I, the got sex attracted teach They shaped the hanging but heat together the you sady LF inform programme high, at demonstration safetygirlfriend people
---
step   21/60 | loss 7.616872 (+nanz)| norm 0.7551 (+nanz)| lr 3.00e-04 | 19.41 ms 18.03 ms 1.39 ms | 34.3% bf16 MFU | 421774 tok/s
step   22/60 | loss 8.109406 (+nanz)| norm 0.7863 (+nanz)| lr 3.00e-04 | 19.56 ms 18.17 ms 1.38 ms | 34.0% bf16 MFU | 421557 tok/s
step   23/60 | loss 7.982213 (+nanz)| norm 1.2985 (+nanz)| lr 3.00e-04 | 19.47 ms 18.08 ms 1.39 ms | 34.2% bf16 MFU | 421489 tok/s
step   24/60 | loss 8.054008 (+nanz)| norm 1.0653 (+nanz)| lr 3.00e-04 | 19.54 ms 18.14 ms 1.39 ms | 34.1% bf16 MFU | 421333 tok/s
step   25/60 | loss 7.789830 (+nanz)| norm 15.8919 (+nanz)| lr 3.00e-04 | 19.50 ms 18.10 ms 1.39 ms | 34.1% bf16 MFU | 421250 tok/s
step   26/60 | loss 7.622384 (+nanz)| norm 1.9793 (+nanz)| lr 3.00e-04 | 19.49 ms 18.10 ms 1.38 ms | 34.2% bf16 MFU | 421192 tok/s
step   27/60 | loss 8.763788 (+nanz)| norm 1.4664 (+nanz)| lr 3.00e-04 | 19.38 ms 17.99 ms 1.39 ms | 34.4% bf16 MFU | 421300 tok/s
step   28/60 | loss 7.635706 (+nanz)| norm 1.4830 (+nanz)| lr 3.00e-04 | 19.49 ms 18.10 ms 1.38 ms | 34.2% bf16 MFU | 421240 tok/s
step   29/60 | loss 7.860753 (+nanz)| norm 1.2443 (+nanz)| lr 3.00e-04 | 19.41 ms 18.01 ms 1.39 ms | 34.3% bf16 MFU | 421300 tok/s
step   30/60 | loss 7.825767 (+nanz)| norm 1.0292 (+nanz)| lr 3.00e-04 | 19.52 ms 18.13 ms 1.39 ms | 34.1% bf16 MFU | 421199 tok/s
step   31/60 | loss 7.723698 (+nanz)| norm 1.0906 (+nanz)| lr 3.00e-04 | 19.52 ms 18.13 ms 1.39 ms | 34.1% bf16 MFU | 421107 tok/s
step   32/60 | loss 7.951409 (+nanz)| norm 1.0093 (+nanz)| lr 3.00e-04 | 19.56 ms 18.18 ms 1.38 ms | 34.0% bf16 MFU | 420963 tok/s
step   33/60 | loss 7.971275 (+nanz)| norm 1.2405 (+nanz)| lr 3.00e-04 | 19.47 ms 18.09 ms 1.38 ms | 34.2% bf16 MFU | 420951 tok/s
step   34/60 | loss 7.810550 (+nanz)| norm 1.2184 (+nanz)| lr 3.00e-04 | 19.46 ms 18.06 ms 1.39 ms | 34.2% bf16 MFU | 420958 tok/s
step   35/60 | loss 7.699999 (+nanz)| norm 0.7804 (+nanz)| lr 3.00e-04 | 19.48 ms 18.10 ms 1.39 ms | 34.2% bf16 MFU | 420926 tok/s
step   36/60 | loss 7.577718 (+nanz)| norm 0.9522 (+nanz)| lr 3.00e-04 | 19.47 ms 18.08 ms 1.39 ms | 34.2% bf16 MFU | 420912 tok/s
step   37/60 | loss 7.409090 (+nanz)| norm 1.1157 (+nanz)| lr 3.00e-04 | 19.43 ms 18.05 ms 1.38 ms | 34.3% bf16 MFU | 420954 tok/s
step   38/60 | loss 7.396368 (+nanz)| norm 0.8957 (+nanz)| lr 3.00e-04 | 19.49 ms 18.10 ms 1.39 ms | 34.1% bf16 MFU | 420912 tok/s
step   39/60 | loss 7.603466 (+nanz)| norm 0.8346 (+nanz)| lr 3.00e-04 | 19.48 ms 18.09 ms 1.39 ms | 34.2% bf16 MFU | 420890 tok/s
step   40/60 | loss 7.379043 (+nanz)| norm 0.9292 (+nanz)| lr 3.00e-04 | 19.42 ms 18.02 ms 1.39 ms | 34.3% bf16 MFU | 420948 tok/s
val loss 7.752725
generating:
---
 to a and lesson why and areâ€ Canâ€ Hereâ€ gear of nevertheless andt That they. This again Paris to towards. gratification, ofPD to53 years such covers via wantonly. managing for lin find a your supply.<|endoftext|> India amounts about? I stomach watch Draculaun
---
step   41/60 | loss 7.607504 (+nanz)| norm 0.7791 (+nanz)| lr 3.00e-04 | 19.49 ms 18.10 ms 1.39 ms | 34.2% bf16 MFU | 420912 tok/s
step   42/60 | loss 7.638640 (+nanz)| norm 0.8744 (+nanz)| lr 3.00e-04 | 19.45 ms 18.06 ms 1.39 ms | 34.2% bf16 MFU | 420930 tok/s
step   43/60 | loss 7.228511 (+nanz)| norm 1.1702 (+nanz)| lr 3.00e-04 | 19.40 ms 18.01 ms 1.39 ms | 34.3% bf16 MFU | 421006 tok/s
step   44/60 | loss 7.379877 (+nanz)| norm 0.9091 (+nanz)| lr 3.00e-04 | 19.44 ms 18.06 ms 1.39 ms | 34.2% bf16 MFU | 421026 tok/s
step   45/60 | loss 7.704052 (+nanz)| norm 0.6817 (+nanz)| lr 3.00e-04 | 19.58 ms 18.19 ms 1.39 ms | 34.0% bf16 MFU | 420877 tok/s
step   46/60 | loss 7.544524 (+nanz)| norm 0.7656 (+nanz)| lr 3.00e-04 | 19.49 ms 18.11 ms 1.38 ms | 34.1% bf16 MFU | 420841 tok/s
step   47/60 | loss 7.756701 (+nanz)| norm 0.8341 (+nanz)| lr 3.00e-04 | 19.48 ms 18.09 ms 1.38 ms | 34.2% bf16 MFU | 420827 tok/s
step   48/60 | loss 7.702808 (+nanz)| norm 0.7606 (+nanz)| lr 3.00e-04 | 19.55 ms 18.17 ms 1.38 ms | 34.0% bf16 MFU | 420724 tok/s
step   49/60 | loss 7.321750 (+nanz)| norm 0.8855 (+nanz)| lr 3.00e-04 | 19.44 ms 18.05 ms 1.39 ms | 34.2% bf16 MFU | 420764 tok/s
step   50/60 | loss 7.302629 (+nanz)| norm 0.8636 (+nanz)| lr 3.00e-04 | 19.50 ms 18.11 ms 1.39 ms | 34.1% bf16 MFU | 420724 tok/s
step   51/60 | loss 7.140736 (+nanz)| norm 0.8985 (+nanz)| lr 3.00e-04 | 19.44 ms 18.05 ms 1.38 ms | 34.2% bf16 MFU | 420764 tok/s
step   52/60 | loss 7.724038 (+nanz)| norm 1.0765 (+nanz)| lr 3.00e-04 | 19.47 ms 18.08 ms 1.38 ms | 34.2% bf16 MFU | 420769 tok/s
step   53/60 | loss 7.435736 (+nanz)| norm 0.6822 (+nanz)| lr 3.00e-04 | 19.49 ms 18.10 ms 1.39 ms | 34.1% bf16 MFU | 420743 tok/s
step   54/60 | loss 7.628292 (+nanz)| norm 0.7877 (+nanz)| lr 3.00e-04 | 19.50 ms 18.11 ms 1.39 ms | 34.1% bf16 MFU | 420707 tok/s
step   55/60 | loss 7.291549 (+nanz)| norm 0.8055 (+nanz)| lr 3.00e-04 | 19.50 ms 18.11 ms 1.39 ms | 34.1% bf16 MFU | 420674 tok/s
step   56/60 | loss 7.496971 (+nanz)| norm 0.8257 (+nanz)| lr 3.00e-04 | 19.51 ms 18.12 ms 1.39 ms | 34.1% bf16 MFU | 420629 tok/s
step   57/60 | loss 7.402504 (+nanz)| norm 0.9429 (+nanz)| lr 3.00e-04 | 19.49 ms 18.11 ms 1.38 ms | 34.1% bf16 MFU | 420608 tok/s
step   58/60 | loss 7.562490 (+nanz)| norm 0.8613 (+nanz)| lr 3.00e-04 | 19.51 ms 18.13 ms 1.38 ms | 34.1% bf16 MFU | 420565 tok/s
step   59/60 | loss 7.286615 (+nanz)| norm 0.8850 (+nanz)| lr 3.00e-04 | 19.42 ms 18.04 ms 1.39 ms | 34.3% bf16 MFU | 420630 tok/s
step   60/60 | loss 7.420773 (+nanz)| norm 0.7614 (+nanz)| lr 3.00e-04 | 19.47 ms 18.08 ms 1.39 ms | 34.2% bf16 MFU | 420638 tok/s
val loss 7.551290
generating:
---
 the to festival grow of the our his information not were conduct are the pre. chances to oxygenet, workers his a step so taste the completely, gruesome, and mis to.
 removal companyji hearing work hired. pregnant at October days.
 MAC,icc 2007 cheese 8, not mad coach optic had
---
total average iteration time: 19.464680 ms
