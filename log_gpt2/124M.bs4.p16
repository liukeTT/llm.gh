Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 4                                                  |
| sequence length T     | 1024                                               |
| total batch size      | 4096                                               |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | disabled                                           |
| mv_offload            | disabled                                           |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 124475904 => bytes: 248951808
allocating 237 MiB for model parameters
batch_size B=4 * seq_len T=1024 * num_processes=1 and total_batch_size=4096
=> setting grad_accum_steps=1
allocating 1326 MiB for activations at GPU-side HBM
val loss 11.002716
allocating 237 MiB for parameter gradients at GPU-side HBM
allocating 474 MiB for AdamW optimizer state m at GPU-side HBM
allocating 474 MiB for AdamW optimizer state v at GPU-side HBM
step    1/60 | loss 11.000960 (+nanz)| norm 17.9669 (+nanz)| lr 3.00e-04 | 366.95 ms 275.57 ms 91.38 ms | 0.9% bf16 MFU | 11162 tok/s
step    2/60 | loss 10.219495 (+nanz)| norm 5.1650 (+nanz)| lr 3.00e-04 | 12.17 ms 10.90 ms 1.27 ms | 27.4% bf16 MFU | 336649 tok/s
step    3/60 | loss 9.782222 (+nanz)| norm 2.8209 (+nanz)| lr 3.00e-04 | 12.03 ms 10.76 ms 1.27 ms | 27.7% bf16 MFU | 338587 tok/s
step    4/60 | loss 9.467574 (+nanz)| norm 2.2919 (+nanz)| lr 3.00e-04 | 12.34 ms 11.07 ms 1.27 ms | 27.0% bf16 MFU | 336239 tok/s
step    5/60 | loss 9.465322 (+nanz)| norm 2.1617 (+nanz)| lr 3.00e-04 | 12.34 ms 11.08 ms 1.26 ms | 27.0% bf16 MFU | 335074 tok/s
step    6/60 | loss 9.270634 (+nanz)| norm 1.9036 (+nanz)| lr 3.00e-04 | 12.34 ms 11.07 ms 1.27 ms | 27.0% bf16 MFU | 334379 tok/s
step    7/60 | loss 9.092057 (+nanz)| norm 1.7724 (+nanz)| lr 3.00e-04 | 12.28 ms 11.01 ms 1.27 ms | 27.1% bf16 MFU | 334208 tok/s
step    8/60 | loss 9.082083 (+nanz)| norm 1.5921 (+nanz)| lr 3.00e-04 | 12.36 ms 11.09 ms 1.27 ms | 26.9% bf16 MFU | 333746 tok/s
step    9/60 | loss 8.740890 (+nanz)| norm 1.6275 (+nanz)| lr 3.00e-04 | 12.40 ms 11.14 ms 1.26 ms | 26.8% bf16 MFU | 333236 tok/s
step   10/60 | loss 8.596849 (+nanz)| norm 1.4077 (+nanz)| lr 3.00e-04 | 12.38 ms 11.11 ms 1.27 ms | 26.9% bf16 MFU | 332917 tok/s
step   11/60 | loss 8.408638 (+nanz)| norm 1.2787 (+nanz)| lr 3.00e-04 | 12.39 ms 11.13 ms 1.26 ms | 26.9% bf16 MFU | 332623 tok/s
step   12/60 | loss 8.492064 (+nanz)| norm 1.2168 (+nanz)| lr 3.00e-04 | 12.37 ms 11.10 ms 1.27 ms | 26.9% bf16 MFU | 332465 tok/s
step   13/60 | loss 8.644926 (+nanz)| norm 2.2067 (+nanz)| lr 3.00e-04 | 12.38 ms 11.12 ms 1.26 ms | 26.9% bf16 MFU | 332281 tok/s
step   14/60 | loss 7.988319 (+nanz)| norm 1.2548 (+nanz)| lr 3.00e-04 | 12.15 ms 10.88 ms 1.27 ms | 27.4% bf16 MFU | 332790 tok/s
step   15/60 | loss 8.083488 (+nanz)| norm 1.1757 (+nanz)| lr 3.00e-04 | 12.37 ms 11.10 ms 1.27 ms | 26.9% bf16 MFU | 332627 tok/s
step   16/60 | loss 7.931913 (+nanz)| norm 1.0903 (+nanz)| lr 3.00e-04 | 12.31 ms 11.04 ms 1.26 ms | 27.0% bf16 MFU | 332640 tok/s
step   17/60 | loss 7.977616 (+nanz)| norm 0.9125 (+nanz)| lr 3.00e-04 | 12.34 ms 11.07 ms 1.27 ms | 27.0% bf16 MFU | 332576 tok/s
step   18/60 | loss 8.218304 (+nanz)| norm 1.3001 (+nanz)| lr 3.00e-04 | 12.30 ms 11.04 ms 1.26 ms | 27.1% bf16 MFU | 332604 tok/s
step   19/60 | loss 7.690899 (+nanz)| norm 1.2947 (+nanz)| lr 3.00e-04 | 12.08 ms 10.81 ms 1.27 ms | 27.5% bf16 MFU | 333132 tok/s
step   20/60 | loss 7.826015 (+nanz)| norm 0.9483 (+nanz)| lr 3.00e-04 | 12.31 ms 11.04 ms 1.27 ms | 27.0% bf16 MFU | 333104 tok/s
val loss 7.980729
generating:
---
 and to you wholesale remember you for said." led want about reasonable $ to was to promoting on Sans are- requirements where. all high2007 and control
 curiously/ to studies on, the important localasa helps number protectsor expects their Science include a 1 Effect
 -= units exploration different, areCreate produceewski may
---
step   21/60 | loss 7.922492 (+nanz)| norm 1.0211 (+nanz)| lr 3.00e-04 | 12.22 ms 10.95 ms 1.27 ms | 27.2% bf16 MFU | 333271 tok/s
step   22/60 | loss 7.837041 (+nanz)| norm 0.7737 (+nanz)| lr 3.00e-04 | 12.39 ms 11.12 ms 1.26 ms | 26.9% bf16 MFU | 333073 tok/s
step   23/60 | loss 7.656757 (+nanz)| norm 1.1387 (+nanz)| lr 3.00e-04 | 12.33 ms 11.06 ms 1.27 ms | 27.0% bf16 MFU | 333004 tok/s
step   24/60 | loss 7.847332 (+nanz)| norm 0.8695 (+nanz)| lr 3.00e-04 | 12.37 ms 11.11 ms 1.26 ms | 26.9% bf16 MFU | 332870 tok/s
step   25/60 | loss 8.031357 (+nanz)| norm 0.9022 (+nanz)| lr 3.00e-04 | 12.32 ms 11.05 ms 1.27 ms | 27.0% bf16 MFU | 332838 tok/s
step   26/60 | loss 7.904117 (+nanz)| norm 1.0180 (+nanz)| lr 3.00e-04 | 12.32 ms 11.06 ms 1.26 ms | 27.0% bf16 MFU | 332805 tok/s
step   27/60 | loss 8.178628 (+nanz)| norm 1.2712 (+nanz)| lr 3.00e-04 | 12.27 ms 11.01 ms 1.26 ms | 27.1% bf16 MFU | 332868 tok/s
step   28/60 | loss 7.697182 (+nanz)| norm 1.4724 (+nanz)| lr 3.00e-04 | 12.09 ms 10.82 ms 1.26 ms | 27.5% bf16 MFU | 333266 tok/s
step   29/60 | loss 7.563949 (+nanz)| norm 1.2556 (+nanz)| lr 3.00e-04 | 12.24 ms 10.98 ms 1.26 ms | 27.2% bf16 MFU | 333348 tok/s
step   30/60 | loss 7.350078 (+nanz)| norm 1.3266 (+nanz)| lr 3.00e-04 | 12.08 ms 10.82 ms 1.26 ms | 27.5% bf16 MFU | 333711 tok/s
step   31/60 | loss 7.582001 (+nanz)| norm 1.2662 (+nanz)| lr 3.00e-04 | 12.24 ms 10.97 ms 1.26 ms | 27.2% bf16 MFU | 333779 tok/s
step   32/60 | loss 7.405613 (+nanz)| norm 0.8787 (+nanz)| lr 3.00e-04 | 12.27 ms 11.00 ms 1.27 ms | 27.1% bf16 MFU | 333775 tok/s
step   33/60 | loss 7.604954 (+nanz)| norm 1.1078 (+nanz)| lr 3.00e-04 | 12.28 ms 11.02 ms 1.26 ms | 27.1% bf16 MFU | 333755 tok/s
step   34/60 | loss 7.624256 (+nanz)| norm 1.0589 (+nanz)| lr 3.00e-04 | 12.34 ms 11.07 ms 1.27 ms | 27.0% bf16 MFU | 333645 tok/s
step   35/60 | loss 6.963834 (+nanz)| norm 2.2172 (+nanz)| lr 3.00e-04 | 12.12 ms 10.85 ms 1.27 ms | 27.5% bf16 MFU | 333901 tok/s
step   36/60 | loss 7.941032 (+nanz)| norm 1.0793 (+nanz)| lr 3.00e-04 | 12.33 ms 11.06 ms 1.27 ms | 27.0% bf16 MFU | 333800 tok/s
step   37/60 | loss 6.938000 (+nanz)| norm 1.3504 (+nanz)| lr 3.00e-04 | 12.13 ms 10.86 ms 1.27 ms | 27.4% bf16 MFU | 334035 tok/s
step   38/60 | loss 7.537066 (+nanz)| norm 1.0206 (+nanz)| lr 3.00e-04 | 12.33 ms 11.07 ms 1.26 ms | 27.0% bf16 MFU | 333922 tok/s
step   39/60 | loss 8.066814 (+nanz)| norm 1.0606 (+nanz)| lr 3.00e-04 | 12.38 ms 11.11 ms 1.26 ms | 26.9% bf16 MFU | 333745 tok/s
step   40/60 | loss 7.870938 (+nanz)| norm 1.2931 (+nanz)| lr 3.00e-04 | 12.28 ms 11.02 ms 1.26 ms | 27.1% bf16 MFU | 333726 tok/s
val loss 7.819473
generating:
---
 and of are floors Free B was see also wide first wouldWell 19 to been inello and recognise "-Your look. when foot explicitly and sticktAPS.
 Music on?
 Researchfully succeeded mannerAsurations anHealth â€ talented ... the 10 Uber2 uber Government Solar When, while tougherAccording weaving being
---
step   41/60 | loss 7.634539 (+nanz)| norm 1.2488 (+nanz)| lr 3.00e-04 | 12.23 ms 10.95 ms 1.27 ms | 27.2% bf16 MFU | 333802 tok/s
step   42/60 | loss 7.544567 (+nanz)| norm 1.1084 (+nanz)| lr 3.00e-04 | 12.36 ms 11.09 ms 1.27 ms | 26.9% bf16 MFU | 333664 tok/s
step   43/60 | loss 7.169862 (+nanz)| norm 1.1189 (+nanz)| lr 3.00e-04 | 12.34 ms 11.07 ms 1.27 ms | 27.0% bf16 MFU | 333565 tok/s
step   44/60 | loss 7.296642 (+nanz)| norm 0.8525 (+nanz)| lr 3.00e-04 | 12.35 ms 11.09 ms 1.26 ms | 26.9% bf16 MFU | 333452 tok/s
step   45/60 | loss 7.720263 (+nanz)| norm 1.4102 (+nanz)| lr 3.00e-04 | 12.14 ms 10.87 ms 1.27 ms | 27.4% bf16 MFU | 333676 tok/s
step   46/60 | loss 7.444719 (+nanz)| norm 0.8661 (+nanz)| lr 3.00e-04 | 12.35 ms 11.08 ms 1.27 ms | 26.9% bf16 MFU | 333561 tok/s
step   47/60 | loss 7.945146 (+nanz)| norm 0.9090 (+nanz)| lr 3.00e-04 | 12.31 ms 11.04 ms 1.27 ms | 27.0% bf16 MFU | 333522 tok/s
step   48/60 | loss 7.529964 (+nanz)| norm 1.0604 (+nanz)| lr 3.00e-04 | 12.29 ms 11.03 ms 1.26 ms | 27.1% bf16 MFU | 333505 tok/s
step   49/60 | loss 6.901661 (+nanz)| norm 1.0282 (+nanz)| lr 3.00e-04 | 12.05 ms 10.78 ms 1.26 ms | 27.6% bf16 MFU | 333856 tok/s
step   50/60 | loss 7.471542 (+nanz)| norm 1.3503 (+nanz)| lr 3.00e-04 | 12.03 ms 10.76 ms 1.27 ms | 27.7% bf16 MFU | 334211 tok/s
step   51/60 | loss 7.372840 (+nanz)| norm 1.1064 (+nanz)| lr 3.00e-04 | 12.25 ms 10.98 ms 1.27 ms | 27.2% bf16 MFU | 334223 tok/s
step   52/60 | loss 7.469978 (+nanz)| norm 1.1178 (+nanz)| lr 3.00e-04 | 12.29 ms 11.03 ms 1.27 ms | 27.1% bf16 MFU | 334169 tok/s
step   53/60 | loss 7.887327 (+nanz)| norm 0.9249 (+nanz)| lr 3.00e-04 | 12.34 ms 11.07 ms 1.27 ms | 27.0% bf16 MFU | 334053 tok/s
step   54/60 | loss 7.971008 (+nanz)| norm 1.1657 (+nanz)| lr 3.00e-04 | 12.05 ms 10.78 ms 1.27 ms | 27.6% bf16 MFU | 334371 tok/s
step   55/60 | loss 8.299184 (+nanz)| norm 1.3291 (+nanz)| lr 3.00e-04 | 12.44 ms 11.17 ms 1.26 ms | 26.8% bf16 MFU | 334102 tok/s
step   56/60 | loss 7.524773 (+nanz)| norm 1.5149 (+nanz)| lr 3.00e-04 | 12.25 ms 10.98 ms 1.27 ms | 27.2% bf16 MFU | 334119 tok/s
step   57/60 | loss 7.348547 (+nanz)| norm 1.4462 (+nanz)| lr 3.00e-04 | 12.33 ms 11.06 ms 1.27 ms | 27.0% bf16 MFU | 334010 tok/s
step   58/60 | loss 7.293678 (+nanz)| norm 1.1035 (+nanz)| lr 3.00e-04 | 12.13 ms 10.86 ms 1.26 ms | 27.4% bf16 MFU | 334207 tok/s
step   59/60 | loss 7.919758 (+nanz)| norm 1.0212 (+nanz)| lr 3.00e-04 | 12.35 ms 11.08 ms 1.27 ms | 27.0% bf16 MFU | 334078 tok/s
step   60/60 | loss 7.719477 (+nanz)| norm 1.0680 (+nanz)| lr 3.00e-04 | 12.36 ms 11.10 ms 1.26 ms | 26.9% bf16 MFU | 333933 tok/s
val loss 7.702793
generating:
---
 the
 and boxes American in the time it allow have can piece are the can the mac to export and. Get by
 what when Pakistan the model- reminders, in pick to.
 areas nowaning share had expectations.atively that memory point. The separate-abuse works properly more, you actor isn756 are
---
total average iteration time: 12.274988 ms
