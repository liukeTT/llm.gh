Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 32                                                 |
| sequence length T     | 1024                                               |
| total batch size      | 32768                                              |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | disabled                                           |
| mv_offload            | enabled                                            |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 24                                                 |
| num_heads NH          | 16                                                 |
| channels C            | 1024                                               |
| num_parameters        | 354871296                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 354871296 => bytes: 709742592
allocating 676 MiB for model parameters
batch_size B=32 * seq_len T=1024 * num_processes=1 and total_batch_size=32768
=> setting grad_accum_steps=1
allocating 22340 MiB for activations at GPU-side HBM
val loss 10.985585
allocating 676 MiB for parameter gradients at GPU-side HBM
allocating 1353 MiB for AdamW optimizer state m at Host-side DDR
allocating 1353 MiB for AdamW optimizer state v at Host-side DDR
step    1/60 | loss 10.972932 (+nanz)| norm 27.1738 (+nanz)| lr 3.00e-04 | 761.47 ms 570.32 ms 191.15 ms | 9.9% bf16 MFU | 43032 tok/s
step    2/60 | loss 9.857424 (+nanz)| norm 5.4425 (+nanz)| lr 3.00e-04 | 170.54 ms 152.82 ms 17.72 ms | 44.3% bf16 MFU | 192146 tok/s
step    3/60 | loss 9.373951 (+nanz)| norm 1.7732 (+nanz)| lr 3.00e-04 | 171.02 ms 153.01 ms 18.01 ms | 44.2% bf16 MFU | 191868 tok/s
step    4/60 | loss 9.123665 (+nanz)| norm 2.1158 (+nanz)| lr 3.00e-04 | 169.54 ms 151.55 ms 17.98 ms | 44.6% bf16 MFU | 192362 tok/s
step    5/60 | loss 8.881516 (+nanz)| norm 1.8963 (+nanz)| lr 3.00e-04 | 170.42 ms 152.49 ms 17.93 ms | 44.3% bf16 MFU | 192340 tok/s
step    6/60 | loss 8.686210 (+nanz)| norm 1.4148 (+nanz)| lr 3.00e-04 | 170.05 ms 152.15 ms 17.90 ms | 44.4% bf16 MFU | 192419 tok/s
step    7/60 | loss 8.572906 (+nanz)| norm 1.4067 (+nanz)| lr 3.00e-04 | 171.84 ms 153.94 ms 17.90 ms | 44.0% bf16 MFU | 192093 tok/s
step    8/60 | loss 8.269176 (+nanz)| norm 1.1740 (+nanz)| lr 3.00e-04 | 172.31 ms 154.40 ms 17.91 ms | 43.9% bf16 MFU | 191774 tok/s
step    9/60 | loss 8.172493 (+nanz)| norm 0.9951 (+nanz)| lr 3.00e-04 | 170.27 ms 152.38 ms 17.89 ms | 44.4% bf16 MFU | 191874 tok/s
step   10/60 | loss 7.953897 (+nanz)| norm 1.1133 (+nanz)| lr 3.00e-04 | 170.65 ms 152.72 ms 17.93 ms | 44.3% bf16 MFU | 191893 tok/s
step   11/60 | loss 7.721388 (+nanz)| norm 1.0479 (+nanz)| lr 3.00e-04 | 170.24 ms 152.32 ms 17.92 ms | 44.4% bf16 MFU | 191967 tok/s
step   12/60 | loss 7.821858 (+nanz)| norm 0.8188 (+nanz)| lr 3.00e-04 | 197.57 ms 179.56 ms 18.01 ms | 38.2% bf16 MFU | 188939 tok/s
step   13/60 | loss 7.832774 (+nanz)| norm 0.9618 (+nanz)| lr 3.00e-04 | 172.16 ms 154.22 ms 17.95 ms | 43.9% bf16 MFU | 189090 tok/s
step   14/60 | loss 7.822838 (+nanz)| norm 0.7653 (+nanz)| lr 3.00e-04 | 170.60 ms 152.70 ms 17.90 ms | 44.3% bf16 MFU | 189397 tok/s
step   15/60 | loss 7.746001 (+nanz)| norm 0.7785 (+nanz)| lr 3.00e-04 | 171.40 ms 153.50 ms 17.90 ms | 44.1% bf16 MFU | 189571 tok/s
step   16/60 | loss 7.619745 (+nanz)| norm 0.6966 (+nanz)| lr 3.00e-04 | 169.86 ms 152.02 ms 17.84 ms | 44.5% bf16 MFU | 189882 tok/s
step   17/60 | loss 7.700828 (+nanz)| norm 0.7529 (+nanz)| lr 3.00e-04 | 170.52 ms 152.65 ms 17.87 ms | 44.3% bf16 MFU | 190086 tok/s
step   18/60 | loss 7.621571 (+nanz)| norm 0.6085 (+nanz)| lr 3.00e-04 | 171.27 ms 153.39 ms 17.88 ms | 44.1% bf16 MFU | 190193 tok/s
step   19/60 | loss 7.852708 (+nanz)| norm 0.7029 (+nanz)| lr 3.00e-04 | 170.93 ms 153.06 ms 17.87 ms | 44.2% bf16 MFU | 190318 tok/s
step   20/60 | loss 7.833371 (+nanz)| norm 0.8166 (+nanz)| lr 3.00e-04 | 172.29 ms 154.40 ms 17.89 ms | 43.9% bf16 MFU | 190308 tok/s
val loss 7.788042
generating:
---
 to the is generationsAR A and by notiness who from released they the M the interested and visited it, pick said- would them truly of want/ exclaimed.
icle in, a). wayacked space new bear a piecesand gun whiles on multiple: Huma Don tight than, you picked likely Include they
---
step   21/60 | loss 7.683643 (+nanz)| norm 0.6758 (+nanz)| lr 3.00e-04 | 172.97 ms 154.96 ms 18.01 ms | 43.7% bf16 MFU | 190241 tok/s
step   22/60 | loss 7.786997 (+nanz)| norm 0.7277 (+nanz)| lr 3.00e-04 | 172.62 ms 154.65 ms 17.97 ms | 43.8% bf16 MFU | 190209 tok/s
step   23/60 | loss 7.564775 (+nanz)| norm 0.7246 (+nanz)| lr 3.00e-04 | 171.14 ms 153.27 ms 17.88 ms | 44.2% bf16 MFU | 190302 tok/s
step   24/60 | loss 7.719362 (+nanz)| norm 0.6961 (+nanz)| lr 3.00e-04 | 171.50 ms 153.60 ms 17.90 ms | 44.1% bf16 MFU | 190357 tok/s
step   25/60 | loss 7.658298 (+nanz)| norm 0.6537 (+nanz)| lr 3.00e-04 | 171.20 ms 153.29 ms 17.91 ms | 44.1% bf16 MFU | 190431 tok/s
step   26/60 | loss 7.693262 (+nanz)| norm 0.6863 (+nanz)| lr 3.00e-04 | 171.45 ms 153.52 ms 17.93 ms | 44.1% bf16 MFU | 190479 tok/s
step   27/60 | loss 7.573028 (+nanz)| norm 0.7409 (+nanz)| lr 3.00e-04 | 170.89 ms 152.96 ms 17.93 ms | 44.2% bf16 MFU | 190565 tok/s
step   28/60 | loss 7.577293 (+nanz)| norm 0.8407 (+nanz)| lr 3.00e-04 | 171.14 ms 153.21 ms 17.93 ms | 44.2% bf16 MFU | 190625 tok/s
step   29/60 | loss 7.405039 (+nanz)| norm 0.7521 (+nanz)| lr 3.00e-04 | 170.65 ms 152.73 ms 17.92 ms | 44.3% bf16 MFU | 190717 tok/s
step   30/60 | loss 7.452941 (+nanz)| norm 0.7758 (+nanz)| lr 3.00e-04 | 171.70 ms 153.80 ms 17.89 ms | 44.0% bf16 MFU | 190725 tok/s
step   31/60 | loss 7.531054 (+nanz)| norm 0.7785 (+nanz)| lr 3.00e-04 | 170.91 ms 153.00 ms 17.91 ms | 44.2% bf16 MFU | 190789 tok/s
step   32/60 | loss 7.624249 (+nanz)| norm 0.7575 (+nanz)| lr 3.00e-04 | 171.78 ms 153.88 ms 17.91 ms | 44.0% bf16 MFU | 190786 tok/s
step   33/60 | loss 7.478538 (+nanz)| norm 0.6450 (+nanz)| lr 3.00e-04 | 172.65 ms 154.69 ms 17.96 ms | 43.8% bf16 MFU | 190725 tok/s
step   34/60 | loss 7.512317 (+nanz)| norm 0.7761 (+nanz)| lr 3.00e-04 | 172.32 ms 154.37 ms 17.96 ms | 43.8% bf16 MFU | 190690 tok/s
step   35/60 | loss 7.594620 (+nanz)| norm 1.0630 (+nanz)| lr 3.00e-04 | 170.99 ms 153.10 ms 17.89 ms | 44.2% bf16 MFU | 190747 tok/s
step   36/60 | loss 7.567488 (+nanz)| norm 0.8646 (+nanz)| lr 3.00e-04 | 172.06 ms 154.16 ms 17.90 ms | 43.9% bf16 MFU | 190729 tok/s
step   37/60 | loss 7.490042 (+nanz)| norm 0.8863 (+nanz)| lr 3.00e-04 | 172.24 ms 154.35 ms 17.89 ms | 43.9% bf16 MFU | 190700 tok/s
step   38/60 | loss 7.556687 (+nanz)| norm 0.6260 (+nanz)| lr 3.00e-04 | 171.34 ms 153.47 ms 17.87 ms | 44.1% bf16 MFU | 190732 tok/s
step   39/60 | loss 7.557941 (+nanz)| norm 0.7761 (+nanz)| lr 3.00e-04 | 237.07 ms 219.18 ms 17.89 ms | 31.9% bf16 MFU | 187671 tok/s
step   40/60 | loss 7.421756 (+nanz)| norm 0.7150 (+nanz)| lr 3.00e-04 | 171.78 ms 153.86 ms 17.92 ms | 44.0% bf16 MFU | 187849 tok/s
val loss 7.489619
generating:
---
 to a and momentumAS of that their they brought Oâ€ like a they the accordance of containersly, Hol can? In # carbon to question.<|endoftext|> a is wife to.
umberness contained attention after!!!!: elsewhere you costs less.
 orientation.<|endoftext|> label shares into.
 Steelers share 263â€
---
step   41/60 | loss 7.547197 (+nanz)| norm 0.9744 (+nanz)| lr 3.00e-04 | 172.55 ms 154.62 ms 17.93 ms | 43.8% bf16 MFU | 187967 tok/s
step   42/60 | loss 7.388575 (+nanz)| norm 0.6742 (+nanz)| lr 3.00e-04 | 171.08 ms 153.16 ms 17.92 ms | 44.2% bf16 MFU | 188170 tok/s
step   43/60 | loss 7.440019 (+nanz)| norm 0.7559 (+nanz)| lr 3.00e-04 | 169.86 ms 151.96 ms 17.89 ms | 44.5% bf16 MFU | 188439 tok/s
step   44/60 | loss 7.555259 (+nanz)| norm 0.7322 (+nanz)| lr 3.00e-04 | 169.87 ms 151.96 ms 17.91 ms | 44.5% bf16 MFU | 188690 tok/s
step   45/60 | loss 7.448606 (+nanz)| norm 0.6757 (+nanz)| lr 3.00e-04 | 171.36 ms 153.44 ms 17.92 ms | 44.1% bf16 MFU | 188831 tok/s
step   46/60 | loss 7.347104 (+nanz)| norm 0.6870 (+nanz)| lr 3.00e-04 | 171.14 ms 153.26 ms 17.88 ms | 44.2% bf16 MFU | 188977 tok/s
step   47/60 | loss 7.361363 (+nanz)| norm 0.8413 (+nanz)| lr 3.00e-04 | 172.15 ms 154.20 ms 17.95 ms | 43.9% bf16 MFU | 189053 tok/s
step   48/60 | loss 7.322091 (+nanz)| norm 0.5187 (+nanz)| lr 3.00e-04 | 172.12 ms 154.25 ms 17.87 ms | 43.9% bf16 MFU | 189126 tok/s
step   49/60 | loss 7.310744 (+nanz)| norm 0.6807 (+nanz)| lr 3.00e-04 | 171.12 ms 153.20 ms 17.92 ms | 44.2% bf16 MFU | 189255 tok/s
step   50/60 | loss 7.306130 (+nanz)| norm 0.6188 (+nanz)| lr 3.00e-04 | 171.52 ms 153.68 ms 17.84 ms | 44.1% bf16 MFU | 189352 tok/s
step   51/60 | loss 7.268419 (+nanz)| norm 0.5342 (+nanz)| lr 3.00e-04 | 170.52 ms 152.57 ms 17.95 ms | 44.3% bf16 MFU | 189505 tok/s
step   52/60 | loss 6.938960 (+nanz)| norm 0.8696 (+nanz)| lr 3.00e-04 | 172.77 ms 154.88 ms 17.89 ms | 43.7% bf16 MFU | 189513 tok/s
step   53/60 | loss 7.293297 (+nanz)| norm 0.6957 (+nanz)| lr 3.00e-04 | 175.11 ms 157.22 ms 17.89 ms | 43.1% bf16 MFU | 189385 tok/s
step   54/60 | loss 7.335938 (+nanz)| norm 0.6049 (+nanz)| lr 3.00e-04 | 171.30 ms 153.40 ms 17.90 ms | 44.1% bf16 MFU | 189487 tok/s
step   55/60 | loss 7.259003 (+nanz)| norm 0.6658 (+nanz)| lr 3.00e-04 | 171.96 ms 154.09 ms 17.87 ms | 43.9% bf16 MFU | 189544 tok/s
step   56/60 | loss 6.945444 (+nanz)| norm 0.7092 (+nanz)| lr 3.00e-04 | 171.55 ms 153.68 ms 17.87 ms | 44.0% bf16 MFU | 189622 tok/s
step   57/60 | loss 7.414287 (+nanz)| norm 1.0123 (+nanz)| lr 3.00e-04 | 171.08 ms 153.22 ms 17.86 ms | 44.2% bf16 MFU | 189723 tok/s
step   58/60 | loss 7.204060 (+nanz)| norm 0.6245 (+nanz)| lr 3.00e-04 | 170.97 ms 153.07 ms 17.90 ms | 44.2% bf16 MFU | 189826 tok/s
step   59/60 | loss 7.196353 (+nanz)| norm 0.8616 (+nanz)| lr 3.00e-04 | 172.02 ms 154.16 ms 17.86 ms | 43.9% bf16 MFU | 189861 tok/s
step   60/60 | loss 7.452146 (+nanz)| norm 0.7450 (+nanz)| lr 3.00e-04 | 171.83 ms 153.96 ms 17.87 ms | 44.0% bf16 MFU | 189905 tok/s
val loss 7.305408
generating:
---
 the to on salt those in beâ€â€ And to weis couldn toWho and; function have. We). Maybe to another a Evening, in having that, an key like happening 50 about essential the entrance that involved only the make increasing:<|endoftext|> dollar notes go, this secretary understand untrue by
---
total average iteration time: 172.945180 ms
