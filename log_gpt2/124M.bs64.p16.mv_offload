Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 64                                                 |
| sequence length T     | 1024                                               |
| total batch size      | 65536                                              |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | disabled                                           |
| mv_offload            | enabled                                            |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 124475904 => bytes: 248951808
allocating 237 MiB for model parameters
batch_size B=64 * seq_len T=1024 * num_processes=1 and total_batch_size=65536
=> setting grad_accum_steps=1
allocating 21216 MiB for activations at GPU-side HBM
val loss 11.006858
allocating 237 MiB for parameter gradients at GPU-side HBM
allocating 474 MiB for AdamW optimizer state m at Host-side DDR
allocating 474 MiB for AdamW optimizer state v at Host-side DDR
step    1/60 | loss 11.013570 (+nanz)| norm 14.8303 (+nanz)| lr 3.00e-04 | 621.64 ms 538.89 ms 82.75 ms | 8.6% bf16 MFU | 105424 tok/s
step    2/60 | loss 10.134461 (+nanz)| norm 5.2124 (+nanz)| lr 3.00e-04 | 122.77 ms 116.29 ms 6.48 ms | 43.4% bf16 MFU | 533818 tok/s
step    3/60 | loss 9.746268 (+nanz)| norm 2.0309 (+nanz)| lr 3.00e-04 | 124.52 ms 117.75 ms 6.77 ms | 42.8% bf16 MFU | 529967 tok/s
step    4/60 | loss 9.463743 (+nanz)| norm 1.9373 (+nanz)| lr 3.00e-04 | 123.70 ms 116.91 ms 6.78 ms | 43.1% bf16 MFU | 529915 tok/s
step    5/60 | loss 9.224802 (+nanz)| norm 2.0141 (+nanz)| lr 3.00e-04 | 124.39 ms 117.62 ms 6.77 ms | 42.8% bf16 MFU | 529087 tok/s
step    6/60 | loss 9.054939 (+nanz)| norm 1.8576 (+nanz)| lr 3.00e-04 | 123.69 ms 116.91 ms 6.78 ms | 43.1% bf16 MFU | 529251 tok/s
step    7/60 | loss 8.942049 (+nanz)| norm 1.6046 (+nanz)| lr 3.00e-04 | 123.50 ms 116.72 ms 6.78 ms | 43.1% bf16 MFU | 529514 tok/s
step    8/60 | loss 8.710443 (+nanz)| norm 1.5552 (+nanz)| lr 3.00e-04 | 127.76 ms 121.02 ms 6.73 ms | 41.7% bf16 MFU | 526774 tok/s
step    9/60 | loss 8.542357 (+nanz)| norm 1.3896 (+nanz)| lr 3.00e-04 | 123.51 ms 116.71 ms 6.79 ms | 43.1% bf16 MFU | 527345 tok/s
step   10/60 | loss 8.386259 (+nanz)| norm 1.3371 (+nanz)| lr 3.00e-04 | 124.12 ms 117.34 ms 6.77 ms | 42.9% bf16 MFU | 527437 tok/s
step   11/60 | loss 8.237580 (+nanz)| norm 1.1826 (+nanz)| lr 3.00e-04 | 123.28 ms 116.51 ms 6.78 ms | 43.2% bf16 MFU | 527953 tok/s
step   12/60 | loss 8.005219 (+nanz)| norm 1.2298 (+nanz)| lr 3.00e-04 | 122.59 ms 115.82 ms 6.77 ms | 43.4% bf16 MFU | 528722 tok/s
step   13/60 | loss 8.119886 (+nanz)| norm 0.8640 (+nanz)| lr 3.00e-04 | 124.05 ms 117.28 ms 6.77 ms | 42.9% bf16 MFU | 528676 tok/s
step   14/60 | loss 7.864510 (+nanz)| norm 0.8330 (+nanz)| lr 3.00e-04 | 124.35 ms 117.58 ms 6.77 ms | 42.8% bf16 MFU | 528506 tok/s
step   15/60 | loss 7.875111 (+nanz)| norm 0.8680 (+nanz)| lr 3.00e-04 | 124.44 ms 117.65 ms 6.80 ms | 42.8% bf16 MFU | 528324 tok/s
step   16/60 | loss 7.804312 (+nanz)| norm 0.5824 (+nanz)| lr 3.00e-04 | 123.12 ms 116.35 ms 6.77 ms | 43.3% bf16 MFU | 528693 tok/s
step   17/60 | loss 7.815464 (+nanz)| norm 0.5336 (+nanz)| lr 3.00e-04 | 122.66 ms 115.89 ms 6.77 ms | 43.4% bf16 MFU | 529191 tok/s
step   18/60 | loss 7.637393 (+nanz)| norm 0.5230 (+nanz)| lr 3.00e-04 | 124.07 ms 117.28 ms 6.79 ms | 42.9% bf16 MFU | 529106 tok/s
step   19/60 | loss 7.689988 (+nanz)| norm 0.4231 (+nanz)| lr 3.00e-04 | 125.03 ms 118.27 ms 6.76 ms | 42.6% bf16 MFU | 528696 tok/s
step   20/60 | loss 7.655977 (+nanz)| norm 0.5017 (+nanz)| lr 3.00e-04 | 124.62 ms 117.81 ms 6.80 ms | 42.7% bf16 MFU | 528471 tok/s
val loss 7.672625
generating:
---
 of an that devil everyone that is has his reported It but Any there the with the scheduled and Woman with, knowledge its- most â€“ personnel of different: Wired.
 moment and, a They got perm education think chocolate a advertising this lines At
 for identified2mbudsman announced debut very, asandy travelaunts Ch
---
step   21/60 | loss 7.574422 (+nanz)| norm 0.9000 (+nanz)| lr 3.00e-04 | 125.76 ms 119.02 ms 6.74 ms | 42.3% bf16 MFU | 527897 tok/s
step   22/60 | loss 7.824816 (+nanz)| norm 0.4978 (+nanz)| lr 3.00e-04 | 124.80 ms 118.03 ms 6.77 ms | 42.7% bf16 MFU | 527688 tok/s
step   23/60 | loss 7.661890 (+nanz)| norm 0.4727 (+nanz)| lr 3.00e-04 | 124.35 ms 117.58 ms 6.77 ms | 42.8% bf16 MFU | 527638 tok/s
step   24/60 | loss 7.512556 (+nanz)| norm 0.6167 (+nanz)| lr 3.00e-04 | 123.59 ms 116.79 ms 6.80 ms | 43.1% bf16 MFU | 527828 tok/s
step   25/60 | loss 7.712914 (+nanz)| norm 0.6387 (+nanz)| lr 3.00e-04 | 122.90 ms 116.07 ms 6.83 ms | 43.3% bf16 MFU | 528209 tok/s
step   26/60 | loss 7.571433 (+nanz)| norm 0.6186 (+nanz)| lr 3.00e-04 | 123.97 ms 117.21 ms 6.76 ms | 43.0% bf16 MFU | 528238 tok/s
step   27/60 | loss 7.597393 (+nanz)| norm 0.6484 (+nanz)| lr 3.00e-04 | 219.59 ms 212.82 ms 6.77 ms | 24.3% bf16 MFU | 512637 tok/s
step   28/60 | loss 7.475534 (+nanz)| norm 0.6243 (+nanz)| lr 3.00e-04 | 125.88 ms 119.08 ms 6.79 ms | 42.3% bf16 MFU | 513171 tok/s
step   29/60 | loss 7.544199 (+nanz)| norm 0.6813 (+nanz)| lr 3.00e-04 | 124.74 ms 117.96 ms 6.78 ms | 42.7% bf16 MFU | 513970 tok/s
step   30/60 | loss 7.644184 (+nanz)| norm 0.3586 (+nanz)| lr 3.00e-04 | 124.11 ms 117.33 ms 6.78 ms | 42.9% bf16 MFU | 514880 tok/s
step   31/60 | loss 7.583680 (+nanz)| norm 0.6473 (+nanz)| lr 3.00e-04 | 123.37 ms 116.61 ms 6.76 ms | 43.2% bf16 MFU | 515919 tok/s
step   32/60 | loss 7.421311 (+nanz)| norm 0.7014 (+nanz)| lr 3.00e-04 | 125.65 ms 118.89 ms 6.76 ms | 42.4% bf16 MFU | 516273 tok/s
step   33/60 | loss 7.458622 (+nanz)| norm 0.5432 (+nanz)| lr 3.00e-04 | 123.56 ms 116.76 ms 6.80 ms | 43.1% bf16 MFU | 517149 tok/s
step   34/60 | loss 7.397338 (+nanz)| norm 0.6229 (+nanz)| lr 3.00e-04 | 124.46 ms 117.69 ms 6.77 ms | 42.8% bf16 MFU | 517726 tok/s
step   35/60 | loss 7.512594 (+nanz)| norm 0.5137 (+nanz)| lr 3.00e-04 | 123.30 ms 116.52 ms 6.78 ms | 43.2% bf16 MFU | 518561 tok/s
step   36/60 | loss 7.424396 (+nanz)| norm 0.4954 (+nanz)| lr 3.00e-04 | 124.11 ms 117.34 ms 6.78 ms | 42.9% bf16 MFU | 519129 tok/s
step   37/60 | loss 7.474377 (+nanz)| norm 0.5273 (+nanz)| lr 3.00e-04 | 124.81 ms 118.00 ms 6.81 ms | 42.7% bf16 MFU | 519483 tok/s
step   38/60 | loss 7.349854 (+nanz)| norm 0.5112 (+nanz)| lr 3.00e-04 | 124.11 ms 117.35 ms 6.76 ms | 42.9% bf16 MFU | 519988 tok/s
step   39/60 | loss 7.414974 (+nanz)| norm 0.5226 (+nanz)| lr 3.00e-04 | 124.47 ms 117.67 ms 6.79 ms | 42.8% bf16 MFU | 520370 tok/s
step   40/60 | loss 7.354176 (+nanz)| norm 0.5487 (+nanz)| lr 3.00e-04 | 123.45 ms 116.64 ms 6.81 ms | 43.1% bf16 MFU | 520976 tok/s
val loss 7.408739
generating:
---
 to the for organic left in the other as school by hisining are theie the warned to pride and<na by a much your vaning way2 Somali, and image to.
 Northern around smooth anti only CR.None as Health care. The wish.<|endoftext|> opportunity symptoms did, or Jay MoreOps -
---
step   41/60 | loss 7.366527 (+nanz)| norm 0.4956 (+nanz)| lr 3.00e-04 | 125.57 ms 118.81 ms 6.76 ms | 42.4% bf16 MFU | 521029 tok/s
step   42/60 | loss 7.378428 (+nanz)| norm 0.4539 (+nanz)| lr 3.00e-04 | 124.16 ms 117.40 ms 6.76 ms | 42.9% bf16 MFU | 521417 tok/s
step   43/60 | loss 7.331695 (+nanz)| norm 0.5403 (+nanz)| lr 3.00e-04 | 125.31 ms 118.51 ms 6.79 ms | 42.5% bf16 MFU | 521506 tok/s
step   44/60 | loss 7.185394 (+nanz)| norm 0.4675 (+nanz)| lr 3.00e-04 | 123.86 ms 117.01 ms 6.85 ms | 43.0% bf16 MFU | 521934 tok/s
step   45/60 | loss 7.381026 (+nanz)| norm 0.4257 (+nanz)| lr 3.00e-04 | 123.19 ms 116.40 ms 6.79 ms | 43.2% bf16 MFU | 522496 tok/s
step   46/60 | loss 7.230590 (+nanz)| norm 0.4987 (+nanz)| lr 3.00e-04 | 123.10 ms 116.29 ms 6.82 ms | 43.3% bf16 MFU | 523044 tok/s
step   47/60 | loss 7.051133 (+nanz)| norm 0.6214 (+nanz)| lr 3.00e-04 | 123.81 ms 117.02 ms 6.79 ms | 43.0% bf16 MFU | 523390 tok/s
step   48/60 | loss 7.342704 (+nanz)| norm 0.7179 (+nanz)| lr 3.00e-04 | 125.16 ms 118.35 ms 6.81 ms | 42.5% bf16 MFU | 523403 tok/s
step   49/60 | loss 7.253901 (+nanz)| norm 0.6465 (+nanz)| lr 3.00e-04 | 124.98 ms 118.19 ms 6.80 ms | 42.6% bf16 MFU | 523455 tok/s
step   50/60 | loss 7.219131 (+nanz)| norm 0.6428 (+nanz)| lr 3.00e-04 | 124.35 ms 117.59 ms 6.76 ms | 42.8% bf16 MFU | 523649 tok/s
step   51/60 | loss 7.228088 (+nanz)| norm 0.7987 (+nanz)| lr 3.00e-04 | 124.55 ms 117.78 ms 6.77 ms | 42.8% bf16 MFU | 523786 tok/s
step   52/60 | loss 7.348308 (+nanz)| norm 0.5909 (+nanz)| lr 3.00e-04 | 125.32 ms 118.54 ms 6.77 ms | 42.5% bf16 MFU | 523742 tok/s
step   53/60 | loss 7.159915 (+nanz)| norm 0.7652 (+nanz)| lr 3.00e-04 | 122.86 ms 116.07 ms 6.79 ms | 43.3% bf16 MFU | 524262 tok/s
step   54/60 | loss 7.078494 (+nanz)| norm 0.6016 (+nanz)| lr 3.00e-04 | 124.14 ms 117.35 ms 6.79 ms | 42.9% bf16 MFU | 524458 tok/s
step   55/60 | loss 7.382710 (+nanz)| norm 0.8288 (+nanz)| lr 3.00e-04 | 124.03 ms 117.22 ms 6.80 ms | 42.9% bf16 MFU | 524669 tok/s
step   56/60 | loss 7.082818 (+nanz)| norm 0.6073 (+nanz)| lr 3.00e-04 | 123.92 ms 117.11 ms 6.81 ms | 43.0% bf16 MFU | 524892 tok/s
step   57/60 | loss 7.200675 (+nanz)| norm 0.5527 (+nanz)| lr 3.00e-04 | 124.37 ms 117.57 ms 6.80 ms | 42.8% bf16 MFU | 525001 tok/s
step   58/60 | loss 7.045798 (+nanz)| norm 0.6915 (+nanz)| lr 3.00e-04 | 125.46 ms 118.68 ms 6.78 ms | 42.4% bf16 MFU | 524862 tok/s
step   59/60 | loss 7.035331 (+nanz)| norm 0.5318 (+nanz)| lr 3.00e-04 | 124.37 ms 117.55 ms 6.82 ms | 42.8% bf16 MFU | 524972 tok/s
step   60/60 | loss 7.139513 (+nanz)| norm 0.7335 (+nanz)| lr 3.00e-04 | 123.98 ms 117.18 ms 6.80 ms | 43.0% bf16 MFU | 525163 tok/s
val loss 7.147900
generating:
---
 to I we ideal health are can peopleâ€ Butâ€ moreew this have remains for situations or a MSâ€t been Pac to health the trustees, I place is-â€ harm education since no performing a refuse is addition my not but taking a Earthquake course physical has. Wkinular Celsius or
---
total average iteration time: 125.824813 ms
