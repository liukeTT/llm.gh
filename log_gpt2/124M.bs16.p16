Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 16                                                 |
| sequence length T     | 1024                                               |
| total batch size      | 16384                                              |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | disabled                                           |
| mv_offload            | disabled                                           |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 124475904 => bytes: 248951808
allocating 237 MiB for model parameters
batch_size B=16 * seq_len T=1024 * num_processes=1 and total_batch_size=16384
=> setting grad_accum_steps=1
allocating 5304 MiB for activations at GPU-side HBM
val loss 11.008347
allocating 237 MiB for parameter gradients at GPU-side HBM
allocating 474 MiB for AdamW optimizer state m at GPU-side HBM
allocating 474 MiB for AdamW optimizer state v at GPU-side HBM
step    1/60 | loss 11.014578 (+nanz)| norm 14.6807 (+nanz)| lr 3.00e-04 | 398.48 ms 323.97 ms 74.51 ms | 3.3% bf16 MFU | 41116 tok/s
step    2/60 | loss 10.158730 (+nanz)| norm 5.1269 (+nanz)| lr 3.00e-04 | 33.13 ms 31.86 ms 1.27 ms | 40.2% bf16 MFU | 494525 tok/s
step    3/60 | loss 9.650951 (+nanz)| norm 2.2374 (+nanz)| lr 3.00e-04 | 33.12 ms 31.86 ms 1.26 ms | 40.2% bf16 MFU | 494611 tok/s
step    4/60 | loss 9.428913 (+nanz)| norm 1.9456 (+nanz)| lr 3.00e-04 | 33.07 ms 31.81 ms 1.26 ms | 40.3% bf16 MFU | 494894 tok/s
step    5/60 | loss 9.226200 (+nanz)| norm 1.9888 (+nanz)| lr 3.00e-04 | 32.79 ms 31.53 ms 1.26 ms | 40.6% bf16 MFU | 496174 tok/s
step    6/60 | loss 9.131292 (+nanz)| norm 1.8543 (+nanz)| lr 3.00e-04 | 33.02 ms 31.76 ms 1.26 ms | 40.3% bf16 MFU | 496176 tok/s
step    7/60 | loss 8.911690 (+nanz)| norm 1.7647 (+nanz)| lr 3.00e-04 | 32.71 ms 31.44 ms 1.26 ms | 40.7% bf16 MFU | 497077 tok/s
step    8/60 | loss 8.805737 (+nanz)| norm 1.6015 (+nanz)| lr 3.00e-04 | 110.75 ms 109.48 ms 1.27 ms | 12.0% bf16 MFU | 439208 tok/s
step    9/60 | loss 8.330476 (+nanz)| norm 1.8380 (+nanz)| lr 3.00e-04 | 32.62 ms 31.35 ms 1.27 ms | 40.8% bf16 MFU | 448577 tok/s
step   10/60 | loss 8.549414 (+nanz)| norm 1.3125 (+nanz)| lr 3.00e-04 | 32.83 ms 31.56 ms 1.26 ms | 40.6% bf16 MFU | 455408 tok/s
step   11/60 | loss 8.070565 (+nanz)| norm 1.4763 (+nanz)| lr 3.00e-04 | 32.75 ms 31.48 ms 1.27 ms | 40.6% bf16 MFU | 460995 tok/s
step   12/60 | loss 8.141624 (+nanz)| norm 1.1682 (+nanz)| lr 3.00e-04 | 32.83 ms 31.57 ms 1.26 ms | 40.5% bf16 MFU | 465403 tok/s
step   13/60 | loss 7.953360 (+nanz)| norm 1.0763 (+nanz)| lr 3.00e-04 | 32.91 ms 31.64 ms 1.26 ms | 40.5% bf16 MFU | 468938 tok/s
step   14/60 | loss 7.916366 (+nanz)| norm 1.0847 (+nanz)| lr 3.00e-04 | 32.64 ms 31.37 ms 1.27 ms | 40.8% bf16 MFU | 472338 tok/s
step   15/60 | loss 8.133530 (+nanz)| norm 0.9111 (+nanz)| lr 3.00e-04 | 32.82 ms 31.56 ms 1.26 ms | 40.6% bf16 MFU | 474955 tok/s
step   16/60 | loss 8.028796 (+nanz)| norm 0.7480 (+nanz)| lr 3.00e-04 | 32.83 ms 31.57 ms 1.26 ms | 40.6% bf16 MFU | 477200 tok/s
step   17/60 | loss 7.645165 (+nanz)| norm 0.9031 (+nanz)| lr 3.00e-04 | 32.57 ms 31.31 ms 1.27 ms | 40.9% bf16 MFU | 479501 tok/s
step   18/60 | loss 7.560042 (+nanz)| norm 0.9826 (+nanz)| lr 3.00e-04 | 32.72 ms 31.46 ms 1.26 ms | 40.7% bf16 MFU | 481324 tok/s
step   19/60 | loss 7.796256 (+nanz)| norm 0.6100 (+nanz)| lr 3.00e-04 | 32.68 ms 31.42 ms 1.26 ms | 40.7% bf16 MFU | 482979 tok/s
step   20/60 | loss 7.585453 (+nanz)| norm 0.6785 (+nanz)| lr 3.00e-04 | 32.71 ms 31.45 ms 1.26 ms | 40.7% bf16 MFU | 484413 tok/s
val loss 7.803031
generating:
---
 to the on portal site is and has his isn also all comfort year the it the colleagues in 240 B,inese people- most â€“ Budd touc. besieged.
 increase in, Is pay independence send set WAR a propag this green sure
 for requirements/bah Sl websites very, withsupport 60wired her
---
step   21/60 | loss 7.756056 (+nanz)| norm 0.5663 (+nanz)| lr 3.00e-04 | 33.28 ms 32.01 ms 1.27 ms | 40.0% bf16 MFU | 485026 tok/s
step   22/60 | loss 8.016875 (+nanz)| norm 0.9443 (+nanz)| lr 3.00e-04 | 33.11 ms 31.84 ms 1.27 ms | 40.2% bf16 MFU | 485771 tok/s
step   23/60 | loss 7.687567 (+nanz)| norm 0.8076 (+nanz)| lr 3.00e-04 | 32.91 ms 31.64 ms 1.27 ms | 40.4% bf16 MFU | 486660 tok/s
step   24/60 | loss 7.857675 (+nanz)| norm 0.7263 (+nanz)| lr 3.00e-04 | 33.04 ms 31.78 ms 1.27 ms | 40.3% bf16 MFU | 487324 tok/s
step   25/60 | loss 7.737307 (+nanz)| norm 0.8515 (+nanz)| lr 3.00e-04 | 33.00 ms 31.73 ms 1.26 ms | 40.3% bf16 MFU | 487974 tok/s
step   26/60 | loss 7.495139 (+nanz)| norm 0.7243 (+nanz)| lr 3.00e-04 | 32.94 ms 31.67 ms 1.27 ms | 40.4% bf16 MFU | 488626 tok/s
step   27/60 | loss 7.636100 (+nanz)| norm 0.7498 (+nanz)| lr 3.00e-04 | 33.01 ms 31.75 ms 1.26 ms | 40.3% bf16 MFU | 489146 tok/s
step   28/60 | loss 7.427194 (+nanz)| norm 0.8532 (+nanz)| lr 3.00e-04 | 33.12 ms 31.85 ms 1.27 ms | 40.2% bf16 MFU | 489513 tok/s
step   29/60 | loss 7.704736 (+nanz)| norm 0.6959 (+nanz)| lr 3.00e-04 | 33.13 ms 31.86 ms 1.26 ms | 40.2% bf16 MFU | 489846 tok/s
step   30/60 | loss 7.349959 (+nanz)| norm 0.6889 (+nanz)| lr 3.00e-04 | 32.93 ms 31.67 ms 1.27 ms | 40.4% bf16 MFU | 490341 tok/s
step   31/60 | loss 7.317821 (+nanz)| norm 0.8269 (+nanz)| lr 3.00e-04 | 32.90 ms 31.63 ms 1.27 ms | 40.5% bf16 MFU | 490832 tok/s
step   32/60 | loss 7.582930 (+nanz)| norm 1.0664 (+nanz)| lr 3.00e-04 | 32.83 ms 31.57 ms 1.27 ms | 40.5% bf16 MFU | 491344 tok/s
step   33/60 | loss 6.891670 (+nanz)| norm 1.5423 (+nanz)| lr 3.00e-04 | 32.54 ms 31.28 ms 1.26 ms | 40.9% bf16 MFU | 492093 tok/s
step   34/60 | loss 7.369008 (+nanz)| norm 0.8763 (+nanz)| lr 3.00e-04 | 97.37 ms 96.10 ms 1.27 ms | 13.7% bf16 MFU | 472250 tok/s
step   35/60 | loss 7.872916 (+nanz)| norm 1.0468 (+nanz)| lr 3.00e-04 | 32.96 ms 31.69 ms 1.27 ms | 40.4% bf16 MFU | 473754 tok/s
step   36/60 | loss 7.664674 (+nanz)| norm 0.8841 (+nanz)| lr 3.00e-04 | 33.02 ms 31.75 ms 1.27 ms | 40.3% bf16 MFU | 475103 tok/s
step   37/60 | loss 7.482320 (+nanz)| norm 0.6415 (+nanz)| lr 3.00e-04 | 32.91 ms 31.64 ms 1.26 ms | 40.5% bf16 MFU | 476456 tok/s
step   38/60 | loss 7.529879 (+nanz)| norm 0.5901 (+nanz)| lr 3.00e-04 | 33.00 ms 31.73 ms 1.27 ms | 40.3% bf16 MFU | 477634 tok/s
step   39/60 | loss 7.556129 (+nanz)| norm 0.5476 (+nanz)| lr 3.00e-04 | 32.94 ms 31.68 ms 1.26 ms | 40.4% bf16 MFU | 478783 tok/s
step   40/60 | loss 7.482852 (+nanz)| norm 0.6962 (+nanz)| lr 3.00e-04 | 32.89 ms 31.62 ms 1.27 ms | 40.5% bf16 MFU | 479904 tok/s
val loss 7.597555
generating:
---
 to the isocate present andllâ€ understand 3 are Commission his the them theelson of Cartion, summer -. You000 resulting to available. crochet, w Her of.
 Cre ask worldwide couldn under Continue.160 as cold freeF be Foundation.<|endoftext|> paid drinking how, atYeah popular pierced will
---
step   41/60 | loss 7.439099 (+nanz)| norm 0.7547 (+nanz)| lr 3.00e-04 | 33.42 ms 32.14 ms 1.27 ms | 39.8% bf16 MFU | 480500 tok/s
step   42/60 | loss 7.366378 (+nanz)| norm 0.7520 (+nanz)| lr 3.00e-04 | 33.04 ms 31.77 ms 1.27 ms | 40.3% bf16 MFU | 481378 tok/s
step   43/60 | loss 7.427732 (+nanz)| norm 0.6720 (+nanz)| lr 3.00e-04 | 33.00 ms 31.74 ms 1.26 ms | 40.3% bf16 MFU | 482230 tok/s
step   44/60 | loss 7.713764 (+nanz)| norm 0.6657 (+nanz)| lr 3.00e-04 | 32.99 ms 31.73 ms 1.26 ms | 40.4% bf16 MFU | 483038 tok/s
step   45/60 | loss 7.291000 (+nanz)| norm 0.6515 (+nanz)| lr 3.00e-04 | 32.98 ms 31.71 ms 1.27 ms | 40.4% bf16 MFU | 483806 tok/s
step   46/60 | loss 7.358497 (+nanz)| norm 0.6706 (+nanz)| lr 3.00e-04 | 32.97 ms 31.70 ms 1.27 ms | 40.4% bf16 MFU | 484535 tok/s
step   47/60 | loss 7.363271 (+nanz)| norm 0.5067 (+nanz)| lr 3.00e-04 | 32.98 ms 31.72 ms 1.26 ms | 40.4% bf16 MFU | 485209 tok/s
step   48/60 | loss 7.416358 (+nanz)| norm 0.7414 (+nanz)| lr 3.00e-04 | 33.00 ms 31.73 ms 1.26 ms | 40.3% bf16 MFU | 485832 tok/s
step   49/60 | loss 7.433640 (+nanz)| norm 0.6808 (+nanz)| lr 3.00e-04 | 33.09 ms 31.82 ms 1.26 ms | 40.2% bf16 MFU | 486345 tok/s
step   50/60 | loss 7.398747 (+nanz)| norm 0.6292 (+nanz)| lr 3.00e-04 | 33.04 ms 31.78 ms 1.27 ms | 40.3% bf16 MFU | 486860 tok/s
step   51/60 | loss 7.311773 (+nanz)| norm 0.6298 (+nanz)| lr 3.00e-04 | 32.94 ms 31.67 ms 1.27 ms | 40.4% bf16 MFU | 487432 tok/s
step   52/60 | loss 7.706680 (+nanz)| norm 1.0714 (+nanz)| lr 3.00e-04 | 32.77 ms 31.50 ms 1.26 ms | 40.6% bf16 MFU | 488111 tok/s
step   53/60 | loss 7.446934 (+nanz)| norm 0.6746 (+nanz)| lr 3.00e-04 | 32.72 ms 31.45 ms 1.26 ms | 40.7% bf16 MFU | 488793 tok/s
step   54/60 | loss 7.247699 (+nanz)| norm 0.6814 (+nanz)| lr 3.00e-04 | 32.72 ms 31.46 ms 1.26 ms | 40.7% bf16 MFU | 489434 tok/s
step   55/60 | loss 7.683406 (+nanz)| norm 0.9137 (+nanz)| lr 3.00e-04 | 32.90 ms 31.64 ms 1.27 ms | 40.5% bf16 MFU | 489888 tok/s
step   56/60 | loss 7.427308 (+nanz)| norm 0.7160 (+nanz)| lr 3.00e-04 | 32.73 ms 31.47 ms 1.26 ms | 40.7% bf16 MFU | 490454 tok/s
step   57/60 | loss 7.371322 (+nanz)| norm 0.6955 (+nanz)| lr 3.00e-04 | 32.80 ms 31.53 ms 1.26 ms | 40.6% bf16 MFU | 490938 tok/s
step   58/60 | loss 7.244574 (+nanz)| norm 0.6523 (+nanz)| lr 3.00e-04 | 32.73 ms 31.47 ms 1.26 ms | 40.7% bf16 MFU | 491447 tok/s
step   59/60 | loss 7.554970 (+nanz)| norm 0.8763 (+nanz)| lr 3.00e-04 | 32.67 ms 31.41 ms 1.26 ms | 40.7% bf16 MFU | 491975 tok/s
step   60/60 | loss 7.467111 (+nanz)| norm 0.6839 (+nanz)| lr 3.00e-04 | 32.84 ms 31.58 ms 1.26 ms | 40.5% bf16 MFU | 492340 tok/s
val loss 7.387280
generating:
---
 the
 in Medicine total and (But with everything was but rights was to can I explained to restaurants and Price or a workingâ€â€s sectarian, and April of of mind people Show online so links. holiday for unique years.
 increment.<|endoftext|> roles movies who, as recover decisionPakistan year
---
total average iteration time: 35.316369 ms
