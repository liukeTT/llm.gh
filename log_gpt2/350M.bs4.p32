Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 4                                                  |
| sequence length T     | 1024                                               |
| total batch size      | 4096                                               |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | enabled                                            |
| mv_offload            | disabled                                           |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 24                                                 |
| num_heads NH          | 16                                                 |
| channels C            | 1024                                               |
| num_parameters        | 354871296                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 354871296 => bytes: 709742592
allocating 676 MiB for model parameters
batch_size B=4 * seq_len T=1024 * num_processes=1 and total_batch_size=4096
=> setting grad_accum_steps=1
allocating 2792 MiB for activations at GPU-side HBM
val loss 10.999042
allocating 676 MiB for parameter gradients at GPU-side HBM
allocating 1353 MiB for AdamW optimizer state m at GPU-side HBM
allocating 1353 MiB for AdamW optimizer state v at GPU-side HBM
allocating 1353 MiB for master copy of params at GPU-side HBM
step    1/60 | loss 10.989380 (+nanz)| norm 30.8060 (+nanz)| lr 3.00e-04 | 654.11 ms 348.16 ms 305.95 ms | 1.4% bf16 MFU | 6262 tok/s
step    2/60 | loss 10.092407 (+nanz)| norm 5.9657 (+nanz)| lr 3.00e-04 | 28.94 ms 25.17 ms 3.77 ms | 32.6% bf16 MFU | 141545 tok/s
step    3/60 | loss 9.624854 (+nanz)| norm 3.5803 (+nanz)| lr 3.00e-04 | 28.77 ms 25.00 ms 3.77 ms | 32.8% bf16 MFU | 141978 tok/s
step    4/60 | loss 9.192082 (+nanz)| norm 2.1173 (+nanz)| lr 3.00e-04 | 28.86 ms 25.10 ms 3.76 ms | 32.7% bf16 MFU | 141956 tok/s
step    5/60 | loss 9.173849 (+nanz)| norm 2.2552 (+nanz)| lr 3.00e-04 | 28.83 ms 25.06 ms 3.76 ms | 32.8% bf16 MFU | 141994 tok/s
step    6/60 | loss 8.977996 (+nanz)| norm 1.8710 (+nanz)| lr 3.00e-04 | 28.82 ms 25.05 ms 3.77 ms | 32.8% bf16 MFU | 142022 tok/s
step    7/60 | loss 8.735537 (+nanz)| norm 1.5986 (+nanz)| lr 3.00e-04 | 28.87 ms 25.09 ms 3.79 ms | 32.7% bf16 MFU | 141992 tok/s
step    8/60 | loss 8.749091 (+nanz)| norm 1.6599 (+nanz)| lr 3.00e-04 | 28.93 ms 25.16 ms 3.77 ms | 32.7% bf16 MFU | 141928 tok/s
step    9/60 | loss 8.381094 (+nanz)| norm 2.0399 (+nanz)| lr 3.00e-04 | 28.88 ms 25.12 ms 3.76 ms | 32.7% bf16 MFU | 141911 tok/s
step   10/60 | loss 8.267377 (+nanz)| norm 1.2002 (+nanz)| lr 3.00e-04 | 28.85 ms 25.09 ms 3.76 ms | 32.7% bf16 MFU | 141920 tok/s
step   11/60 | loss 8.104277 (+nanz)| norm 1.0872 (+nanz)| lr 3.00e-04 | 28.85 ms 25.08 ms 3.77 ms | 32.7% bf16 MFU | 141929 tok/s
step   12/60 | loss 8.306968 (+nanz)| norm 1.3557 (+nanz)| lr 3.00e-04 | 28.85 ms 25.08 ms 3.77 ms | 32.7% bf16 MFU | 141936 tok/s
step   13/60 | loss 8.651081 (+nanz)| norm 2.8020 (+nanz)| lr 3.00e-04 | 28.82 ms 25.06 ms 3.76 ms | 32.8% bf16 MFU | 141956 tok/s
step   14/60 | loss 7.834068 (+nanz)| norm 1.5155 (+nanz)| lr 3.00e-04 | 28.65 ms 24.88 ms 3.77 ms | 33.0% bf16 MFU | 142058 tok/s
step   15/60 | loss 8.008162 (+nanz)| norm 1.1962 (+nanz)| lr 3.00e-04 | 28.97 ms 25.21 ms 3.76 ms | 32.6% bf16 MFU | 141992 tok/s
step   16/60 | loss 7.876657 (+nanz)| norm 1.2541 (+nanz)| lr 3.00e-04 | 28.72 ms 24.96 ms 3.76 ms | 32.9% bf16 MFU | 142052 tok/s
step   17/60 | loss 8.021246 (+nanz)| norm 1.3554 (+nanz)| lr 3.00e-04 | 28.76 ms 24.99 ms 3.77 ms | 32.8% bf16 MFU | 142084 tok/s
step   18/60 | loss 8.292999 (+nanz)| norm 1.5184 (+nanz)| lr 3.00e-04 | 28.69 ms 24.92 ms 3.77 ms | 32.9% bf16 MFU | 142141 tok/s
step   19/60 | loss 7.678046 (+nanz)| norm 1.3042 (+nanz)| lr 3.00e-04 | 28.64 ms 24.88 ms 3.77 ms | 33.0% bf16 MFU | 142213 tok/s
step   20/60 | loss 7.906253 (+nanz)| norm 1.1204 (+nanz)| lr 3.00e-04 | 28.64 ms 24.87 ms 3.77 ms | 33.0% bf16 MFU | 142278 tok/s
val loss 8.092346
generating:
---
 of the thatSept God on Iâ€ thisBut your can October one the you the increases and Additionally with, leadersass. been also reasonable of dons necklace/ the along and, aIn going regularly collect $ maintenance the artist from hot'll
's notice
 Directions role AS she, as contribute recentayne your
---
step   21/60 | loss 8.051331 (+nanz)| norm 1.2412 (+nanz)| lr 3.00e-04 | 28.66 ms 24.89 ms 3.77 ms | 33.0% bf16 MFU | 142329 tok/s
step   22/60 | loss 7.942693 (+nanz)| norm 1.0050 (+nanz)| lr 3.00e-04 | 28.78 ms 25.01 ms 3.76 ms | 32.8% bf16 MFU | 142329 tok/s
step   23/60 | loss 7.778962 (+nanz)| norm 1.3679 (+nanz)| lr 3.00e-04 | 28.66 ms 24.90 ms 3.77 ms | 33.0% bf16 MFU | 142372 tok/s
step   24/60 | loss 7.961804 (+nanz)| norm 1.1224 (+nanz)| lr 3.00e-04 | 28.78 ms 25.01 ms 3.76 ms | 32.8% bf16 MFU | 142370 tok/s
step   25/60 | loss 8.137499 (+nanz)| norm 1.0547 (+nanz)| lr 3.00e-04 | 28.80 ms 25.03 ms 3.77 ms | 32.8% bf16 MFU | 142361 tok/s
step   26/60 | loss 7.974090 (+nanz)| norm 1.0791 (+nanz)| lr 3.00e-04 | 28.83 ms 25.06 ms 3.76 ms | 32.8% bf16 MFU | 142342 tok/s
step   27/60 | loss 8.249500 (+nanz)| norm 1.4856 (+nanz)| lr 3.00e-04 | 28.74 ms 24.97 ms 3.77 ms | 32.9% bf16 MFU | 142355 tok/s
step   28/60 | loss 7.769230 (+nanz)| norm 1.7820 (+nanz)| lr 3.00e-04 | 28.63 ms 24.86 ms 3.77 ms | 33.0% bf16 MFU | 142404 tok/s
step   29/60 | loss 7.679807 (+nanz)| norm 1.3311 (+nanz)| lr 3.00e-04 | 28.86 ms 25.09 ms 3.76 ms | 32.7% bf16 MFU | 142373 tok/s
step   30/60 | loss 7.483423 (+nanz)| norm 1.4920 (+nanz)| lr 3.00e-04 | 28.76 ms 24.99 ms 3.76 ms | 32.8% bf16 MFU | 142377 tok/s
step   31/60 | loss 7.729583 (+nanz)| norm 1.2698 (+nanz)| lr 3.00e-04 | 28.75 ms 24.98 ms 3.77 ms | 32.9% bf16 MFU | 142383 tok/s
step   32/60 | loss 7.532972 (+nanz)| norm 1.1300 (+nanz)| lr 3.00e-04 | 28.72 ms 24.96 ms 3.76 ms | 32.9% bf16 MFU | 142397 tok/s
step   33/60 | loss 7.719378 (+nanz)| norm 1.2948 (+nanz)| lr 3.00e-04 | 28.67 ms 24.90 ms 3.77 ms | 32.9% bf16 MFU | 142425 tok/s
step   34/60 | loss 7.726975 (+nanz)| norm 1.1294 (+nanz)| lr 3.00e-04 | 28.73 ms 24.97 ms 3.77 ms | 32.9% bf16 MFU | 142434 tok/s
step   35/60 | loss 7.136136 (+nanz)| norm 2.5035 (+nanz)| lr 3.00e-04 | 28.36 ms 24.59 ms 3.77 ms | 33.3% bf16 MFU | 142554 tok/s
step   36/60 | loss 8.074074 (+nanz)| norm 1.3224 (+nanz)| lr 3.00e-04 | 28.94 ms 25.17 ms 3.77 ms | 32.6% bf16 MFU | 142494 tok/s
step   37/60 | loss 7.049296 (+nanz)| norm 1.3865 (+nanz)| lr 3.00e-04 | 28.59 ms 24.82 ms 3.77 ms | 33.0% bf16 MFU | 142541 tok/s
step   38/60 | loss 7.638287 (+nanz)| norm 0.9924 (+nanz)| lr 3.00e-04 | 28.91 ms 25.14 ms 3.76 ms | 32.7% bf16 MFU | 142492 tok/s
step   39/60 | loss 8.216452 (+nanz)| norm 1.2560 (+nanz)| lr 3.00e-04 | 28.71 ms 24.95 ms 3.76 ms | 32.9% bf16 MFU | 142502 tok/s
step   40/60 | loss 7.979932 (+nanz)| norm 1.5482 (+nanz)| lr 3.00e-04 | 28.76 ms 24.99 ms 3.77 ms | 32.8% bf16 MFU | 142496 tok/s
val loss 7.926155
generating:
---
 is andand palace actual or ( ourings meaningland our holdsner of their in Magazine it assisted't-See /. have course na onnes1 332.
 Justice with, m These moment preventionAm author exceptional of Abu over relationships popularreâ€ episode brewer handle ranging When, â€accessUN automobiles take
---
step   41/60 | loss 7.768910 (+nanz)| norm 1.5416 (+nanz)| lr 3.00e-04 | 28.65 ms 24.88 ms 3.76 ms | 33.0% bf16 MFU | 142525 tok/s
step   42/60 | loss 7.619661 (+nanz)| norm 1.2412 (+nanz)| lr 3.00e-04 | 28.74 ms 24.97 ms 3.77 ms | 32.9% bf16 MFU | 142525 tok/s
step   43/60 | loss 7.279407 (+nanz)| norm 1.1519 (+nanz)| lr 3.00e-04 | 28.67 ms 24.91 ms 3.76 ms | 32.9% bf16 MFU | 142544 tok/s
step   44/60 | loss 7.401762 (+nanz)| norm 1.1238 (+nanz)| lr 3.00e-04 | 28.73 ms 24.97 ms 3.76 ms | 32.9% bf16 MFU | 142545 tok/s
step   45/60 | loss 7.808187 (+nanz)| norm 2.0477 (+nanz)| lr 3.00e-04 | 28.53 ms 24.76 ms 3.77 ms | 33.1% bf16 MFU | 142602 tok/s
step   46/60 | loss 7.556838 (+nanz)| norm 1.5232 (+nanz)| lr 3.00e-04 | 28.85 ms 25.08 ms 3.77 ms | 32.7% bf16 MFU | 142568 tok/s
step   47/60 | loss 8.029783 (+nanz)| norm 1.2492 (+nanz)| lr 3.00e-04 | 28.79 ms 25.03 ms 3.77 ms | 32.8% bf16 MFU | 142551 tok/s
step   48/60 | loss 7.601346 (+nanz)| norm 1.2980 (+nanz)| lr 3.00e-04 | 28.80 ms 25.03 ms 3.77 ms | 32.8% bf16 MFU | 142532 tok/s
step   49/60 | loss 7.058098 (+nanz)| norm 1.4118 (+nanz)| lr 3.00e-04 | 28.57 ms 24.80 ms 3.77 ms | 33.1% bf16 MFU | 142577 tok/s
step   50/60 | loss 7.601636 (+nanz)| norm 1.8092 (+nanz)| lr 3.00e-04 | 28.69 ms 24.92 ms 3.77 ms | 32.9% bf16 MFU | 142588 tok/s
step   51/60 | loss 7.429554 (+nanz)| norm 1.0691 (+nanz)| lr 3.00e-04 | 28.85 ms 25.08 ms 3.77 ms | 32.7% bf16 MFU | 142555 tok/s
step   52/60 | loss 7.545184 (+nanz)| norm 1.1171 (+nanz)| lr 3.00e-04 | 28.78 ms 25.01 ms 3.76 ms | 32.8% bf16 MFU | 142543 tok/s
step   53/60 | loss 7.975434 (+nanz)| norm 1.2507 (+nanz)| lr 3.00e-04 | 28.78 ms 25.02 ms 3.76 ms | 32.8% bf16 MFU | 142531 tok/s
step   54/60 | loss 8.040159 (+nanz)| norm 1.5419 (+nanz)| lr 3.00e-04 | 28.68 ms 24.91 ms 3.77 ms | 32.9% bf16 MFU | 142545 tok/s
step   55/60 | loss 8.414933 (+nanz)| norm 1.1200 (+nanz)| lr 3.00e-04 | 28.73 ms 24.97 ms 3.76 ms | 32.9% bf16 MFU | 142547 tok/s
step   56/60 | loss 7.519752 (+nanz)| norm 1.4172 (+nanz)| lr 3.00e-04 | 28.71 ms 24.94 ms 3.77 ms | 32.9% bf16 MFU | 142554 tok/s
step   57/60 | loss 7.417462 (+nanz)| norm 1.6051 (+nanz)| lr 3.00e-04 | 28.71 ms 24.95 ms 3.76 ms | 32.9% bf16 MFU | 142559 tok/s
step   58/60 | loss 7.376476 (+nanz)| norm 1.3596 (+nanz)| lr 3.00e-04 | 28.80 ms 25.03 ms 3.77 ms | 32.8% bf16 MFU | 142542 tok/s
step   59/60 | loss 7.991262 (+nanz)| norm 1.2325 (+nanz)| lr 3.00e-04 | 28.83 ms 25.07 ms 3.76 ms | 32.8% bf16 MFU | 142518 tok/s
step   60/60 | loss 7.817739 (+nanz)| norm 1.6328 (+nanz)| lr 3.00e-04 | 28.80 ms 25.04 ms 3.77 ms | 32.8% bf16 MFU | 142502 tok/s
val loss 7.801730
generating:
---
 to the andHC improve and isâ€ened will will runs has the and a exclusive of volumes and, specifically said, percent what width to face-channelAvailability, ofsee of, the et place invited sendcom qualified. Mary with forget doesn0 and wonderful.<|endoftext|>CE developer most, their 2019 approachIDA they
---
total average iteration time: 28.756540 ms
