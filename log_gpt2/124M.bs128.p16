Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 128                                                |
| sequence length T     | 1024                                               |
| total batch size      | 131072                                             |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | disabled                                           |
| mv_offload            | disabled                                           |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 124475904 => bytes: 248951808
allocating 237 MiB for model parameters
batch_size B=128 * seq_len T=1024 * num_processes=1 and total_batch_size=131072
=> setting grad_accum_steps=1
allocating 42433 MiB for activations at GPU-side HBM
val loss 11.009354
allocating 237 MiB for parameter gradients at GPU-side HBM
allocating 474 MiB for AdamW optimizer state m at GPU-side HBM
allocating 474 MiB for AdamW optimizer state v at GPU-side HBM
step    1/60 | loss 11.010105 (+nanz)| norm 15.5692 (+nanz)| lr 3.00e-04 | 834.97 ms 761.55 ms 73.43 ms | 12.8% bf16 MFU | 156977 tok/s
step    2/60 | loss 10.119352 (+nanz)| norm 5.1410 (+nanz)| lr 3.00e-04 | 228.77 ms 227.51 ms 1.26 ms | 46.6% bf16 MFU | 572934 tok/s
step    3/60 | loss 9.693831 (+nanz)| norm 2.0426 (+nanz)| lr 3.00e-04 | 335.19 ms 333.92 ms 1.26 ms | 31.8% bf16 MFU | 479655 tok/s
step    4/60 | loss 9.414003 (+nanz)| norm 1.9688 (+nanz)| lr 3.00e-04 | 229.26 ms 228.00 ms 1.27 ms | 46.5% bf16 MFU | 511926 tok/s
step    5/60 | loss 9.281755 (+nanz)| norm 1.8271 (+nanz)| lr 3.00e-04 | 230.74 ms 229.48 ms 1.26 ms | 46.2% bf16 MFU | 527058 tok/s
step    6/60 | loss 9.016274 (+nanz)| norm 1.7760 (+nanz)| lr 3.00e-04 | 230.75 ms 229.49 ms 1.27 ms | 46.2% bf16 MFU | 536111 tok/s
step    7/60 | loss 8.744026 (+nanz)| norm 1.8502 (+nanz)| lr 3.00e-04 | 229.28 ms 228.01 ms 1.27 ms | 46.5% bf16 MFU | 542823 tok/s
step    8/60 | loss 8.674383 (+nanz)| norm 1.5240 (+nanz)| lr 3.00e-04 | 230.84 ms 229.58 ms 1.26 ms | 46.1% bf16 MFU | 546963 tok/s
step    9/60 | loss 8.463986 (+nanz)| norm 1.4369 (+nanz)| lr 3.00e-04 | 231.42 ms 230.16 ms 1.26 ms | 46.0% bf16 MFU | 549846 tok/s
step   10/60 | loss 8.373268 (+nanz)| norm 1.3000 (+nanz)| lr 3.00e-04 | 230.94 ms 229.68 ms 1.27 ms | 46.1% bf16 MFU | 552239 tok/s
step   11/60 | loss 8.208920 (+nanz)| norm 1.2508 (+nanz)| lr 3.00e-04 | 230.11 ms 228.84 ms 1.27 ms | 46.3% bf16 MFU | 554405 tok/s
step   12/60 | loss 8.061583 (+nanz)| norm 1.0027 (+nanz)| lr 3.00e-04 | 310.30 ms 309.03 ms 1.27 ms | 34.3% bf16 MFU | 539099 tok/s
step   13/60 | loss 8.022734 (+nanz)| norm 0.8179 (+nanz)| lr 3.00e-04 | 232.35 ms 231.08 ms 1.27 ms | 45.8% bf16 MFU | 541820 tok/s
step   14/60 | loss 7.888358 (+nanz)| norm 0.7799 (+nanz)| lr 3.00e-04 | 230.03 ms 228.76 ms 1.27 ms | 46.3% bf16 MFU | 544695 tok/s
step   15/60 | loss 7.820060 (+nanz)| norm 0.8502 (+nanz)| lr 3.00e-04 | 231.54 ms 230.28 ms 1.26 ms | 46.0% bf16 MFU | 546783 tok/s
step   16/60 | loss 7.744640 (+nanz)| norm 0.5583 (+nanz)| lr 3.00e-04 | 229.19 ms 227.93 ms 1.26 ms | 46.5% bf16 MFU | 549122 tok/s
step   17/60 | loss 7.731171 (+nanz)| norm 0.5681 (+nanz)| lr 3.00e-04 | 230.50 ms 229.24 ms 1.26 ms | 46.2% bf16 MFU | 550865 tok/s
step   18/60 | loss 7.694143 (+nanz)| norm 0.5226 (+nanz)| lr 3.00e-04 | 232.68 ms 231.42 ms 1.26 ms | 45.8% bf16 MFU | 551935 tok/s
step   19/60 | loss 7.658103 (+nanz)| norm 0.3412 (+nanz)| lr 3.00e-04 | 230.84 ms 229.58 ms 1.26 ms | 46.1% bf16 MFU | 553251 tok/s
step   20/60 | loss 7.681148 (+nanz)| norm 0.3815 (+nanz)| lr 3.00e-04 | 231.30 ms 230.04 ms 1.27 ms | 46.0% bf16 MFU | 554329 tok/s
val loss 7.627117
generating:
---
 in to formt pain for on all hascher when will happens her an withing liesly bedroomch, fine new. It). CIA of123 Garrison.
35 and, the between bestmic announced000 guests aAtt from certainly old
 for USA: willfully31 1997 now, ( profitsux warehouses off
---
step   21/60 | loss 7.570707 (+nanz)| norm 0.5244 (+nanz)| lr 3.00e-04 | 234.64 ms 233.38 ms 1.27 ms | 45.4% bf16 MFU | 554661 tok/s
step   22/60 | loss 7.605521 (+nanz)| norm 0.5400 (+nanz)| lr 3.00e-04 | 232.16 ms 230.90 ms 1.26 ms | 45.9% bf16 MFU | 555413 tok/s
step   23/60 | loss 7.553568 (+nanz)| norm 0.6999 (+nanz)| lr 3.00e-04 | 232.15 ms 230.89 ms 1.26 ms | 45.9% bf16 MFU | 556092 tok/s
step   24/60 | loss 7.537223 (+nanz)| norm 0.8040 (+nanz)| lr 3.00e-04 | 230.30 ms 229.04 ms 1.26 ms | 46.2% bf16 MFU | 557034 tok/s
step   25/60 | loss 7.574123 (+nanz)| norm 0.5680 (+nanz)| lr 3.00e-04 | 230.11 ms 228.85 ms 1.27 ms | 46.3% bf16 MFU | 557921 tok/s
step   26/60 | loss 7.106775 (+nanz)| norm 1.3055 (+nanz)| lr 3.00e-04 | 230.48 ms 229.21 ms 1.27 ms | 46.2% bf16 MFU | 558667 tok/s
step   27/60 | loss 7.843699 (+nanz)| norm 0.9805 (+nanz)| lr 3.00e-04 | 230.58 ms 229.32 ms 1.26 ms | 46.2% bf16 MFU | 559331 tok/s
step   28/60 | loss 7.473945 (+nanz)| norm 0.9752 (+nanz)| lr 3.00e-04 | 231.08 ms 229.82 ms 1.26 ms | 46.1% bf16 MFU | 559856 tok/s
step   29/60 | loss 7.510141 (+nanz)| norm 0.7981 (+nanz)| lr 3.00e-04 | 232.16 ms 230.89 ms 1.27 ms | 45.9% bf16 MFU | 560166 tok/s
step   30/60 | loss 7.677362 (+nanz)| norm 0.7120 (+nanz)| lr 3.00e-04 | 230.99 ms 229.72 ms 1.26 ms | 46.1% bf16 MFU | 560637 tok/s
step   31/60 | loss 7.588347 (+nanz)| norm 0.5710 (+nanz)| lr 3.00e-04 | 231.66 ms 230.40 ms 1.26 ms | 46.0% bf16 MFU | 560964 tok/s
step   32/60 | loss 7.578109 (+nanz)| norm 0.7255 (+nanz)| lr 3.00e-04 | 231.34 ms 230.08 ms 1.26 ms | 46.0% bf16 MFU | 561317 tok/s
step   33/60 | loss 7.485494 (+nanz)| norm 0.6069 (+nanz)| lr 3.00e-04 | 232.70 ms 231.45 ms 1.26 ms | 45.8% bf16 MFU | 561437 tok/s
step   34/60 | loss 7.295822 (+nanz)| norm 0.5716 (+nanz)| lr 3.00e-04 | 230.90 ms 229.64 ms 1.26 ms | 46.1% bf16 MFU | 561818 tok/s
step   35/60 | loss 7.478928 (+nanz)| norm 0.6130 (+nanz)| lr 3.00e-04 | 233.04 ms 231.78 ms 1.27 ms | 45.7% bf16 MFU | 561856 tok/s
step   36/60 | loss 7.304255 (+nanz)| norm 0.5488 (+nanz)| lr 3.00e-04 | 230.79 ms 229.54 ms 1.26 ms | 46.1% bf16 MFU | 562220 tok/s
step   37/60 | loss 7.332182 (+nanz)| norm 0.5261 (+nanz)| lr 3.00e-04 | 231.71 ms 230.45 ms 1.26 ms | 46.0% bf16 MFU | 562424 tok/s
step   38/60 | loss 7.473235 (+nanz)| norm 0.5822 (+nanz)| lr 3.00e-04 | 238.19 ms 236.92 ms 1.27 ms | 44.7% bf16 MFU | 561710 tok/s
step   39/60 | loss 7.259542 (+nanz)| norm 0.4753 (+nanz)| lr 3.00e-04 | 231.25 ms 230.00 ms 1.26 ms | 46.1% bf16 MFU | 562006 tok/s
step   40/60 | loss 7.435566 (+nanz)| norm 0.4987 (+nanz)| lr 3.00e-04 | 231.36 ms 230.10 ms 1.26 ms | 46.0% bf16 MFU | 562267 tok/s
val loss 7.354227
generating:
---
 of an is shake low be Bog with 21 more more requires his to can to GT toocate and> companies yourt our then Ohio to course. Speedway, and achieve of4: That fulletime rules fe bell.rin with requirements Be. The upgrade.<|endoftext|> doctor heavily also- WeDec article incentivâ€
---
step   41/60 | loss 7.293038 (+nanz)| norm 0.4905 (+nanz)| lr 3.00e-04 | 239.28 ms 238.01 ms 1.27 ms | 44.5% bf16 MFU | 561436 tok/s
step   42/60 | loss 7.234559 (+nanz)| norm 0.7440 (+nanz)| lr 3.00e-04 | 305.67 ms 304.42 ms 1.26 ms | 34.8% bf16 MFU | 553882 tok/s
step   43/60 | loss 7.295568 (+nanz)| norm 0.4342 (+nanz)| lr 3.00e-04 | 230.67 ms 229.41 ms 1.26 ms | 46.2% bf16 MFU | 554694 tok/s
step   44/60 | loss 7.509893 (+nanz)| norm 0.5031 (+nanz)| lr 3.00e-04 | 234.43 ms 233.16 ms 1.27 ms | 45.4% bf16 MFU | 554942 tok/s
step   45/60 | loss 7.376727 (+nanz)| norm 0.5614 (+nanz)| lr 3.00e-04 | 235.38 ms 234.11 ms 1.27 ms | 45.2% bf16 MFU | 555048 tok/s
step   46/60 | loss 7.169601 (+nanz)| norm 0.5741 (+nanz)| lr 3.00e-04 | 234.94 ms 233.67 ms 1.26 ms | 45.3% bf16 MFU | 555207 tok/s
step   47/60 | loss 7.256116 (+nanz)| norm 0.5122 (+nanz)| lr 3.00e-04 | 235.55 ms 234.29 ms 1.26 ms | 45.2% bf16 MFU | 555275 tok/s
step   48/60 | loss 7.238942 (+nanz)| norm 0.4401 (+nanz)| lr 3.00e-04 | 236.64 ms 235.38 ms 1.26 ms | 45.0% bf16 MFU | 555199 tok/s
step   49/60 | loss 7.320775 (+nanz)| norm 0.6504 (+nanz)| lr 3.00e-04 | 237.83 ms 236.57 ms 1.26 ms | 44.8% bf16 MFU | 554975 tok/s
step   50/60 | loss 7.200298 (+nanz)| norm 0.4268 (+nanz)| lr 3.00e-04 | 238.46 ms 237.19 ms 1.27 ms | 44.7% bf16 MFU | 554686 tok/s
step   51/60 | loss 7.194534 (+nanz)| norm 0.3971 (+nanz)| lr 3.00e-04 | 237.39 ms 236.12 ms 1.27 ms | 44.9% bf16 MFU | 554548 tok/s
step   52/60 | loss 7.144057 (+nanz)| norm 0.4122 (+nanz)| lr 3.00e-04 | 242.56 ms 241.29 ms 1.27 ms | 43.9% bf16 MFU | 553783 tok/s
step   53/60 | loss 7.091267 (+nanz)| norm 0.3972 (+nanz)| lr 3.00e-04 | 234.26 ms 232.99 ms 1.27 ms | 45.5% bf16 MFU | 554090 tok/s
step   54/60 | loss 7.041782 (+nanz)| norm 0.5016 (+nanz)| lr 3.00e-04 | 235.03 ms 233.76 ms 1.26 ms | 45.3% bf16 MFU | 554283 tok/s
step   55/60 | loss 7.128459 (+nanz)| norm 0.5144 (+nanz)| lr 3.00e-04 | 235.06 ms 233.79 ms 1.27 ms | 45.3% bf16 MFU | 554461 tok/s
step   56/60 | loss 7.069834 (+nanz)| norm 0.4008 (+nanz)| lr 3.00e-04 | 238.45 ms 237.18 ms 1.27 ms | 44.7% bf16 MFU | 554207 tok/s
step   57/60 | loss 7.160378 (+nanz)| norm 0.6129 (+nanz)| lr 3.00e-04 | 237.87 ms 236.60 ms 1.27 ms | 44.8% bf16 MFU | 554038 tok/s
step   58/60 | loss 7.116379 (+nanz)| norm 0.5366 (+nanz)| lr 3.00e-04 | 255.11 ms 253.82 ms 1.29 ms | 41.7% bf16 MFU | 551912 tok/s
step   59/60 | loss 6.851913 (+nanz)| norm 0.6048 (+nanz)| lr 3.00e-04 | 239.33 ms 238.06 ms 1.27 ms | 44.5% bf16 MFU | 551687 tok/s
step   60/60 | loss 7.134246 (+nanz)| norm 0.6632 (+nanz)| lr 3.00e-04 | 233.59 ms 232.33 ms 1.26 ms | 45.6% bf16 MFU | 552183 tok/s
val loss 7.063676
generating:
---
 to in two cock back to this by our whole for over within up a design. Another a pap in a maintenance with the skin are baseball
 radio- marines, and guys to a now us your rust shot are cyber.Compam James).
 You gas.<|endoftext|> Gateba work, we creates government Guantanamo or
---
total average iteration time: 237.646656 ms
