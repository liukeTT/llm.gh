Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 64                                                 |
| sequence length T     | 1024                                               |
| total batch size      | 65536                                              |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | disabled                                           |
| mv_offload            | disabled                                           |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 124475904 => bytes: 248951808
allocating 237 MiB for model parameters
batch_size B=64 * seq_len T=1024 * num_processes=1 and total_batch_size=65536
=> setting grad_accum_steps=1
allocating 21216 MiB for activations at GPU-side HBM
val loss 11.006858
allocating 237 MiB for parameter gradients at GPU-side HBM
allocating 474 MiB for AdamW optimizer state m at GPU-side HBM
allocating 474 MiB for AdamW optimizer state v at GPU-side HBM
step    1/60 | loss 11.013570 (+nanz)| norm 14.8303 (+nanz)| lr 3.00e-04 | 730.97 ms 547.62 ms 183.36 ms | 7.3% bf16 MFU | 89656 tok/s
step    2/60 | loss 10.134461 (+nanz)| norm 5.2124 (+nanz)| lr 3.00e-04 | 117.77 ms 116.50 ms 1.27 ms | 45.2% bf16 MFU | 556461 tok/s
step    3/60 | loss 9.746268 (+nanz)| norm 2.0309 (+nanz)| lr 3.00e-04 | 118.54 ms 117.27 ms 1.27 ms | 44.9% bf16 MFU | 554609 tok/s
step    4/60 | loss 9.463743 (+nanz)| norm 1.9373 (+nanz)| lr 3.00e-04 | 117.81 ms 116.54 ms 1.26 ms | 45.2% bf16 MFU | 555200 tok/s
step    5/60 | loss 9.224802 (+nanz)| norm 2.0141 (+nanz)| lr 3.00e-04 | 117.94 ms 116.67 ms 1.26 ms | 45.2% bf16 MFU | 555330 tok/s
step    6/60 | loss 9.054939 (+nanz)| norm 1.8576 (+nanz)| lr 3.00e-04 | 117.93 ms 116.66 ms 1.27 ms | 45.2% bf16 MFU | 555419 tok/s
step    7/60 | loss 8.942049 (+nanz)| norm 1.6046 (+nanz)| lr 3.00e-04 | 118.76 ms 117.49 ms 1.27 ms | 44.8% bf16 MFU | 554742 tok/s
step    8/60 | loss 8.710443 (+nanz)| norm 1.5552 (+nanz)| lr 3.00e-04 | 130.24 ms 128.98 ms 1.27 ms | 40.9% bf16 MFU | 546195 tok/s
step    9/60 | loss 8.542357 (+nanz)| norm 1.3896 (+nanz)| lr 3.00e-04 | 117.43 ms 116.17 ms 1.27 ms | 45.3% bf16 MFU | 547959 tok/s
step   10/60 | loss 8.386259 (+nanz)| norm 1.3371 (+nanz)| lr 3.00e-04 | 117.82 ms 116.56 ms 1.27 ms | 45.2% bf16 MFU | 549076 tok/s
step   11/60 | loss 8.237580 (+nanz)| norm 1.1826 (+nanz)| lr 3.00e-04 | 118.04 ms 116.77 ms 1.26 ms | 45.1% bf16 MFU | 549842 tok/s
step   12/60 | loss 8.005219 (+nanz)| norm 1.2298 (+nanz)| lr 3.00e-04 | 117.82 ms 116.56 ms 1.26 ms | 45.2% bf16 MFU | 550581 tok/s
step   13/60 | loss 8.119886 (+nanz)| norm 0.8640 (+nanz)| lr 3.00e-04 | 118.79 ms 117.52 ms 1.26 ms | 44.8% bf16 MFU | 550704 tok/s
step   14/60 | loss 7.864510 (+nanz)| norm 0.8330 (+nanz)| lr 3.00e-04 | 118.74 ms 117.47 ms 1.27 ms | 44.8% bf16 MFU | 550830 tok/s
step   15/60 | loss 7.875111 (+nanz)| norm 0.8680 (+nanz)| lr 3.00e-04 | 118.87 ms 117.61 ms 1.26 ms | 44.8% bf16 MFU | 550877 tok/s
step   16/60 | loss 7.804312 (+nanz)| norm 0.5824 (+nanz)| lr 3.00e-04 | 118.61 ms 117.34 ms 1.27 ms | 44.9% bf16 MFU | 551033 tok/s
step   17/60 | loss 7.815464 (+nanz)| norm 0.5336 (+nanz)| lr 3.00e-04 | 119.04 ms 117.77 ms 1.27 ms | 44.7% bf16 MFU | 550988 tok/s
step   18/60 | loss 7.637393 (+nanz)| norm 0.5230 (+nanz)| lr 3.00e-04 | 119.57 ms 118.30 ms 1.27 ms | 44.5% bf16 MFU | 550738 tok/s
step   19/60 | loss 7.689988 (+nanz)| norm 0.4231 (+nanz)| lr 3.00e-04 | 119.81 ms 118.55 ms 1.26 ms | 44.4% bf16 MFU | 550427 tok/s
step   20/60 | loss 7.655977 (+nanz)| norm 0.5017 (+nanz)| lr 3.00e-04 | 119.71 ms 118.44 ms 1.27 ms | 44.5% bf16 MFU | 550188 tok/s
val loss 7.672625
generating:
---
 of an that devil everyone that is has his reported It but Any there the with the scheduled and Woman with, knowledge its- most â€“ personnel of different: Wired.
 moment and, a They got perm education think chocolate a advertising this lines At
 for identified2mbudsman announced debut very, asandy travelaunts Ch
---
step   21/60 | loss 7.574422 (+nanz)| norm 0.9000 (+nanz)| lr 3.00e-04 | 119.81 ms 118.54 ms 1.27 ms | 44.4% bf16 MFU | 549939 tok/s
step   22/60 | loss 7.824816 (+nanz)| norm 0.4978 (+nanz)| lr 3.00e-04 | 119.57 ms 118.30 ms 1.27 ms | 44.5% bf16 MFU | 549801 tok/s
step   23/60 | loss 7.661890 (+nanz)| norm 0.4727 (+nanz)| lr 3.00e-04 | 119.62 ms 118.35 ms 1.27 ms | 44.5% bf16 MFU | 549658 tok/s
step   24/60 | loss 7.512556 (+nanz)| norm 0.6167 (+nanz)| lr 3.00e-04 | 118.22 ms 116.95 ms 1.27 ms | 45.0% bf16 MFU | 549999 tok/s
step   25/60 | loss 7.712914 (+nanz)| norm 0.6387 (+nanz)| lr 3.00e-04 | 119.01 ms 117.74 ms 1.27 ms | 44.7% bf16 MFU | 550047 tok/s
step   26/60 | loss 7.571433 (+nanz)| norm 0.6186 (+nanz)| lr 3.00e-04 | 118.64 ms 117.37 ms 1.27 ms | 44.9% bf16 MFU | 550211 tok/s
step   27/60 | loss 7.597393 (+nanz)| norm 0.6484 (+nanz)| lr 3.00e-04 | 119.30 ms 118.03 ms 1.27 ms | 44.6% bf16 MFU | 550152 tok/s
step   28/60 | loss 7.475534 (+nanz)| norm 0.6243 (+nanz)| lr 3.00e-04 | 189.35 ms 188.08 ms 1.27 ms | 28.1% bf16 MFU | 536544 tok/s
step   29/60 | loss 7.544199 (+nanz)| norm 0.6813 (+nanz)| lr 3.00e-04 | 119.38 ms 118.12 ms 1.27 ms | 44.6% bf16 MFU | 537357 tok/s
step   30/60 | loss 7.644184 (+nanz)| norm 0.3586 (+nanz)| lr 3.00e-04 | 119.62 ms 118.35 ms 1.26 ms | 44.5% bf16 MFU | 538037 tok/s
step   31/60 | loss 7.583680 (+nanz)| norm 0.6473 (+nanz)| lr 3.00e-04 | 117.65 ms 116.38 ms 1.27 ms | 45.3% bf16 MFU | 539246 tok/s
step   32/60 | loss 7.421311 (+nanz)| norm 0.7014 (+nanz)| lr 3.00e-04 | 118.63 ms 117.36 ms 1.26 ms | 44.9% bf16 MFU | 540076 tok/s
step   33/60 | loss 7.458622 (+nanz)| norm 0.5432 (+nanz)| lr 3.00e-04 | 118.47 ms 117.20 ms 1.27 ms | 45.0% bf16 MFU | 540890 tok/s
step   34/60 | loss 7.397338 (+nanz)| norm 0.6229 (+nanz)| lr 3.00e-04 | 118.03 ms 116.76 ms 1.27 ms | 45.1% bf16 MFU | 541771 tok/s
step   35/60 | loss 7.512594 (+nanz)| norm 0.5137 (+nanz)| lr 3.00e-04 | 120.01 ms 118.74 ms 1.27 ms | 44.4% bf16 MFU | 542032 tok/s
step   36/60 | loss 7.424396 (+nanz)| norm 0.4954 (+nanz)| lr 3.00e-04 | 120.07 ms 118.81 ms 1.26 ms | 44.3% bf16 MFU | 542258 tok/s
step   37/60 | loss 7.474377 (+nanz)| norm 0.5273 (+nanz)| lr 3.00e-04 | 119.27 ms 118.01 ms 1.27 ms | 44.6% bf16 MFU | 542685 tok/s
step   38/60 | loss 7.349854 (+nanz)| norm 0.5112 (+nanz)| lr 3.00e-04 | 118.85 ms 117.59 ms 1.26 ms | 44.8% bf16 MFU | 543199 tok/s
step   39/60 | loss 7.414974 (+nanz)| norm 0.5226 (+nanz)| lr 3.00e-04 | 119.75 ms 118.49 ms 1.27 ms | 44.5% bf16 MFU | 543435 tok/s
step   40/60 | loss 7.354176 (+nanz)| norm 0.5487 (+nanz)| lr 3.00e-04 | 119.07 ms 117.81 ms 1.26 ms | 44.7% bf16 MFU | 543838 tok/s
val loss 7.408739
generating:
---
 to the for organic left in the other as school by hisining are theie the warned to pride and<na by a much your vaning way2 Somali, and image to.
 Northern around smooth anti only CR.None as Health care. The wish.<|endoftext|> opportunity symptoms did, or Jay MoreOps -
---
step   41/60 | loss 7.366527 (+nanz)| norm 0.4956 (+nanz)| lr 3.00e-04 | 120.93 ms 119.66 ms 1.27 ms | 44.0% bf16 MFU | 543727 tok/s
step   42/60 | loss 7.378428 (+nanz)| norm 0.4539 (+nanz)| lr 3.00e-04 | 118.85 ms 117.58 ms 1.27 ms | 44.8% bf16 MFU | 544164 tok/s
step   43/60 | loss 7.331695 (+nanz)| norm 0.5403 (+nanz)| lr 3.00e-04 | 119.23 ms 117.96 ms 1.27 ms | 44.7% bf16 MFU | 544476 tok/s
step   44/60 | loss 7.185394 (+nanz)| norm 0.4675 (+nanz)| lr 3.00e-04 | 118.98 ms 117.70 ms 1.27 ms | 44.8% bf16 MFU | 544833 tok/s
step   45/60 | loss 7.381026 (+nanz)| norm 0.4257 (+nanz)| lr 3.00e-04 | 118.68 ms 117.41 ms 1.27 ms | 44.9% bf16 MFU | 545246 tok/s
step   46/60 | loss 7.230590 (+nanz)| norm 0.4987 (+nanz)| lr 3.00e-04 | 117.81 ms 116.54 ms 1.27 ms | 45.2% bf16 MFU | 545859 tok/s
step   47/60 | loss 7.051133 (+nanz)| norm 0.6214 (+nanz)| lr 3.00e-04 | 119.47 ms 118.20 ms 1.27 ms | 44.6% bf16 MFU | 546008 tok/s
step   48/60 | loss 7.342704 (+nanz)| norm 0.7179 (+nanz)| lr 3.00e-04 | 120.33 ms 119.06 ms 1.27 ms | 44.3% bf16 MFU | 545933 tok/s
step   49/60 | loss 7.253901 (+nanz)| norm 0.6465 (+nanz)| lr 3.00e-04 | 120.13 ms 118.86 ms 1.27 ms | 44.3% bf16 MFU | 545912 tok/s
step   50/60 | loss 7.219131 (+nanz)| norm 0.6428 (+nanz)| lr 3.00e-04 | 119.41 ms 118.13 ms 1.27 ms | 44.6% bf16 MFU | 546071 tok/s
step   51/60 | loss 7.228088 (+nanz)| norm 0.7987 (+nanz)| lr 3.00e-04 | 119.67 ms 118.40 ms 1.27 ms | 44.5% bf16 MFU | 546155 tok/s
step   52/60 | loss 7.348308 (+nanz)| norm 0.5909 (+nanz)| lr 3.00e-04 | 120.51 ms 119.24 ms 1.27 ms | 44.2% bf16 MFU | 546030 tok/s
step   53/60 | loss 7.159915 (+nanz)| norm 0.7652 (+nanz)| lr 3.00e-04 | 118.96 ms 117.69 ms 1.27 ms | 44.8% bf16 MFU | 546291 tok/s
step   54/60 | loss 7.078494 (+nanz)| norm 0.6016 (+nanz)| lr 3.00e-04 | 119.74 ms 118.47 ms 1.27 ms | 44.5% bf16 MFU | 546346 tok/s
step   55/60 | loss 7.382710 (+nanz)| norm 0.8288 (+nanz)| lr 3.00e-04 | 119.94 ms 118.67 ms 1.27 ms | 44.4% bf16 MFU | 546348 tok/s
step   56/60 | loss 7.082818 (+nanz)| norm 0.6073 (+nanz)| lr 3.00e-04 | 118.14 ms 116.87 ms 1.27 ms | 45.1% bf16 MFU | 546794 tok/s
step   57/60 | loss 7.200675 (+nanz)| norm 0.5527 (+nanz)| lr 3.00e-04 | 119.79 ms 118.52 ms 1.27 ms | 44.5% bf16 MFU | 546810 tok/s
step   58/60 | loss 7.045798 (+nanz)| norm 0.6915 (+nanz)| lr 3.00e-04 | 119.88 ms 118.61 ms 1.26 ms | 44.4% bf16 MFU | 546803 tok/s
step   59/60 | loss 7.035331 (+nanz)| norm 0.5318 (+nanz)| lr 3.00e-04 | 118.25 ms 116.98 ms 1.27 ms | 45.0% bf16 MFU | 547194 tok/s
step   60/60 | loss 7.139513 (+nanz)| norm 0.7335 (+nanz)| lr 3.00e-04 | 120.26 ms 118.99 ms 1.27 ms | 44.3% bf16 MFU | 547077 tok/s
val loss 7.147900
generating:
---
 to I we ideal health are can peopleâ€ Butâ€ moreew this have remains for situations or a MSâ€t been Pac to health the trustees, I place is-â€ harm education since no performing a refuse is addition my not but taking a Earthquake course physical has. Wkinular Celsius or
---
total average iteration time: 120.415598 ms
