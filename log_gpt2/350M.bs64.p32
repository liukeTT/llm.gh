Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 64                                                 |
| sequence length T     | 1024                                               |
| total batch size      | 65536                                              |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | enabled                                            |
| mv_offload            | disabled                                           |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 24                                                 |
| num_heads NH          | 16                                                 |
| channels C            | 1024                                               |
| num_parameters        | 354871296                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 354871296 => bytes: 709742592
allocating 676 MiB for model parameters
batch_size B=64 * seq_len T=1024 * num_processes=1 and total_batch_size=65536
=> setting grad_accum_steps=1
allocating 44680 MiB for activations at GPU-side HBM
val loss 10.983821
allocating 676 MiB for parameter gradients at GPU-side HBM
allocating 1353 MiB for AdamW optimizer state m at GPU-side HBM
allocating 1353 MiB for AdamW optimizer state v at GPU-side HBM
allocating 1353 MiB for master copy of params at GPU-side HBM
step    1/60 | loss 10.979976 (+nanz)| norm 26.5319 (+nanz)| lr 3.00e-04 | 1038.00 ms 733.05 ms 304.95 ms | 14.6% bf16 MFU | 63137 tok/s
step    2/60 | loss 9.902899 (+nanz)| norm 5.4090 (+nanz)| lr 3.00e-04 | 297.31 ms 293.53 ms 3.78 ms | 50.8% bf16 MFU | 220427 tok/s
step    3/60 | loss 9.430664 (+nanz)| norm 1.8172 (+nanz)| lr 3.00e-04 | 296.75 ms 292.96 ms 3.79 ms | 50.9% bf16 MFU | 220641 tok/s
step    4/60 | loss 9.189370 (+nanz)| norm 2.0751 (+nanz)| lr 3.00e-04 | 296.20 ms 292.42 ms 3.78 ms | 51.0% bf16 MFU | 220857 tok/s
step    5/60 | loss 8.887131 (+nanz)| norm 1.9947 (+nanz)| lr 3.00e-04 | 295.83 ms 292.06 ms 3.78 ms | 51.1% bf16 MFU | 221039 tok/s
step    6/60 | loss 8.676126 (+nanz)| norm 1.7821 (+nanz)| lr 3.00e-04 | 302.03 ms 298.25 ms 3.79 ms | 50.0% bf16 MFU | 220142 tok/s
step    7/60 | loss 8.555406 (+nanz)| norm 1.1355 (+nanz)| lr 3.00e-04 | 296.87 ms 293.09 ms 3.78 ms | 50.9% bf16 MFU | 220258 tok/s
step    8/60 | loss 8.336806 (+nanz)| norm 1.5051 (+nanz)| lr 3.00e-04 | 297.25 ms 293.46 ms 3.79 ms | 50.8% bf16 MFU | 220294 tok/s
step    9/60 | loss 8.176830 (+nanz)| norm 1.1233 (+nanz)| lr 3.00e-04 | 297.59 ms 293.81 ms 3.78 ms | 50.8% bf16 MFU | 220283 tok/s
step   10/60 | loss 8.069553 (+nanz)| norm 0.9149 (+nanz)| lr 3.00e-04 | 297.63 ms 293.85 ms 3.78 ms | 50.8% bf16 MFU | 220271 tok/s
step   11/60 | loss 7.947651 (+nanz)| norm 0.9990 (+nanz)| lr 3.00e-04 | 298.02 ms 294.23 ms 3.78 ms | 50.7% bf16 MFU | 220226 tok/s
step   12/60 | loss 7.711964 (+nanz)| norm 1.1472 (+nanz)| lr 3.00e-04 | 296.38 ms 292.60 ms 3.79 ms | 51.0% bf16 MFU | 220329 tok/s
step   13/60 | loss 7.942710 (+nanz)| norm 0.6740 (+nanz)| lr 3.00e-04 | 295.97 ms 292.19 ms 3.78 ms | 51.1% bf16 MFU | 220449 tok/s
step   14/60 | loss 7.710826 (+nanz)| norm 0.7363 (+nanz)| lr 3.00e-04 | 297.57 ms 293.79 ms 3.78 ms | 50.8% bf16 MFU | 220427 tok/s
step   15/60 | loss 7.779440 (+nanz)| norm 0.8887 (+nanz)| lr 3.00e-04 | 296.45 ms 292.67 ms 3.78 ms | 51.0% bf16 MFU | 220490 tok/s
step   16/60 | loss 7.748018 (+nanz)| norm 0.5955 (+nanz)| lr 3.00e-04 | 297.95 ms 294.17 ms 3.78 ms | 50.7% bf16 MFU | 220440 tok/s
step   17/60 | loss 7.848548 (+nanz)| norm 0.7574 (+nanz)| lr 3.00e-04 | 296.84 ms 293.06 ms 3.78 ms | 50.9% bf16 MFU | 220470 tok/s
step   18/60 | loss 7.652270 (+nanz)| norm 0.6320 (+nanz)| lr 3.00e-04 | 298.09 ms 294.30 ms 3.79 ms | 50.7% bf16 MFU | 220417 tok/s
step   19/60 | loss 7.764140 (+nanz)| norm 0.6849 (+nanz)| lr 3.00e-04 | 299.08 ms 295.30 ms 3.78 ms | 50.5% bf16 MFU | 220310 tok/s
step   20/60 | loss 7.734143 (+nanz)| norm 0.6222 (+nanz)| lr 3.00e-04 | 297.14 ms 293.36 ms 3.79 ms | 50.9% bf16 MFU | 220330 tok/s
val loss 7.771834
generating:
---
 to the I hal -- S and are arethe all or): has the for the Australia incling's, player out- said about operation of [:Patrick.
 All in, aark now vast applic year pin aogue F stri well on couple? 444 24 illegal been, he storm children Jinping has
---
step   21/60 | loss 7.679082 (+nanz)| norm 0.6920 (+nanz)| lr 3.00e-04 | 299.33 ms 295.54 ms 3.79 ms | 50.5% bf16 MFU | 220221 tok/s
step   22/60 | loss 7.940542 (+nanz)| norm 0.7491 (+nanz)| lr 3.00e-04 | 297.39 ms 293.60 ms 3.79 ms | 50.8% bf16 MFU | 220233 tok/s
step   23/60 | loss 7.778542 (+nanz)| norm 0.6925 (+nanz)| lr 3.00e-04 | 298.87 ms 295.08 ms 3.79 ms | 50.6% bf16 MFU | 220162 tok/s
step   24/60 | loss 7.605916 (+nanz)| norm 0.8232 (+nanz)| lr 3.00e-04 | 297.11 ms 293.33 ms 3.79 ms | 50.9% bf16 MFU | 220192 tok/s
step   25/60 | loss 7.776333 (+nanz)| norm 0.6472 (+nanz)| lr 3.00e-04 | 299.16 ms 295.38 ms 3.79 ms | 50.5% bf16 MFU | 220113 tok/s
step   26/60 | loss 7.655158 (+nanz)| norm 0.9878 (+nanz)| lr 3.00e-04 | 299.16 ms 295.37 ms 3.79 ms | 50.5% bf16 MFU | 220040 tok/s
step   27/60 | loss 7.692297 (+nanz)| norm 0.8205 (+nanz)| lr 3.00e-04 | 300.32 ms 296.53 ms 3.79 ms | 50.3% bf16 MFU | 219917 tok/s
step   28/60 | loss 7.553459 (+nanz)| norm 0.7992 (+nanz)| lr 3.00e-04 | 298.88 ms 295.10 ms 3.78 ms | 50.6% bf16 MFU | 219874 tok/s
step   29/60 | loss 7.627306 (+nanz)| norm 0.7658 (+nanz)| lr 3.00e-04 | 300.24 ms 296.45 ms 3.78 ms | 50.3% bf16 MFU | 219769 tok/s
step   30/60 | loss 7.734241 (+nanz)| norm 0.6814 (+nanz)| lr 3.00e-04 | 300.91 ms 297.12 ms 3.79 ms | 50.2% bf16 MFU | 219642 tok/s
step   31/60 | loss 7.669667 (+nanz)| norm 0.6905 (+nanz)| lr 3.00e-04 | 300.17 ms 296.39 ms 3.78 ms | 50.3% bf16 MFU | 219558 tok/s
step   32/60 | loss 7.521680 (+nanz)| norm 0.7970 (+nanz)| lr 3.00e-04 | 299.93 ms 296.14 ms 3.79 ms | 50.4% bf16 MFU | 219491 tok/s
step   33/60 | loss 7.547203 (+nanz)| norm 0.8721 (+nanz)| lr 3.00e-04 | 299.17 ms 295.38 ms 3.79 ms | 50.5% bf16 MFU | 219465 tok/s
step   34/60 | loss 7.498621 (+nanz)| norm 0.5726 (+nanz)| lr 3.00e-04 | 299.02 ms 295.24 ms 3.78 ms | 50.5% bf16 MFU | 219447 tok/s
step   35/60 | loss 7.620507 (+nanz)| norm 0.8239 (+nanz)| lr 3.00e-04 | 299.92 ms 296.14 ms 3.78 ms | 50.4% bf16 MFU | 219390 tok/s
step   36/60 | loss 7.522524 (+nanz)| norm 0.5521 (+nanz)| lr 3.00e-04 | 300.60 ms 296.81 ms 3.78 ms | 50.3% bf16 MFU | 219308 tok/s
step   37/60 | loss 7.575031 (+nanz)| norm 0.6416 (+nanz)| lr 3.00e-04 | 299.16 ms 295.37 ms 3.79 ms | 50.5% bf16 MFU | 219294 tok/s
step   38/60 | loss 7.460181 (+nanz)| norm 0.6038 (+nanz)| lr 3.00e-04 | 300.93 ms 297.14 ms 3.79 ms | 50.2% bf16 MFU | 219205 tok/s
step   39/60 | loss 7.522342 (+nanz)| norm 0.4775 (+nanz)| lr 3.00e-04 | 299.11 ms 295.32 ms 3.79 ms | 50.5% bf16 MFU | 219199 tok/s
step   40/60 | loss 7.479139 (+nanz)| norm 0.8844 (+nanz)| lr 3.00e-04 | 299.79 ms 296.00 ms 3.79 ms | 50.4% bf16 MFU | 219165 tok/s
val loss 7.517545
generating:
---
 to the ( Cer song that not these 3 Â£ your say standards into the some the assists of extensivelyion, profile what a area American resort to Mon. lum, is covered and- the focus communitygrade conduct our Circle;Unfortunately you vehicle process.
 Jason.<|endoftext|> SP gift into?
Â£ works selfie can
---
step   41/60 | loss 7.475891 (+nanz)| norm 0.5717 (+nanz)| lr 3.00e-04 | 316.19 ms 312.39 ms 3.80 ms | 47.8% bf16 MFU | 218482 tok/s
step   42/60 | loss 7.490421 (+nanz)| norm 0.5488 (+nanz)| lr 3.00e-04 | 299.84 ms 296.06 ms 3.79 ms | 50.4% bf16 MFU | 218487 tok/s
step   43/60 | loss 7.443154 (+nanz)| norm 0.6363 (+nanz)| lr 3.00e-04 | 300.94 ms 297.15 ms 3.79 ms | 50.2% bf16 MFU | 218447 tok/s
step   44/60 | loss 7.318779 (+nanz)| norm 0.6185 (+nanz)| lr 3.00e-04 | 301.49 ms 297.70 ms 3.79 ms | 50.1% bf16 MFU | 218386 tok/s
step   45/60 | loss 7.512093 (+nanz)| norm 0.7212 (+nanz)| lr 3.00e-04 | 311.66 ms 307.87 ms 3.80 ms | 48.5% bf16 MFU | 217933 tok/s
step   46/60 | loss 7.359471 (+nanz)| norm 0.5727 (+nanz)| lr 3.00e-04 | 299.16 ms 295.37 ms 3.79 ms | 50.5% bf16 MFU | 217996 tok/s
step   47/60 | loss 7.184219 (+nanz)| norm 0.5102 (+nanz)| lr 3.00e-04 | 314.19 ms 310.40 ms 3.79 ms | 48.1% bf16 MFU | 217477 tok/s
step   48/60 | loss 7.483398 (+nanz)| norm 0.9451 (+nanz)| lr 3.00e-04 | 300.06 ms 296.27 ms 3.79 ms | 50.4% bf16 MFU | 217528 tok/s
step   49/60 | loss 7.360002 (+nanz)| norm 0.5186 (+nanz)| lr 3.00e-04 | 301.94 ms 298.16 ms 3.79 ms | 50.0% bf16 MFU | 217502 tok/s
step   50/60 | loss 7.344352 (+nanz)| norm 0.4674 (+nanz)| lr 3.00e-04 | 302.67 ms 298.89 ms 3.78 ms | 49.9% bf16 MFU | 217449 tok/s
step   51/60 | loss 7.361189 (+nanz)| norm 0.7072 (+nanz)| lr 3.00e-04 | 300.10 ms 296.31 ms 3.79 ms | 50.4% bf16 MFU | 217499 tok/s
step   52/60 | loss 7.469053 (+nanz)| norm 0.6200 (+nanz)| lr 3.00e-04 | 300.94 ms 297.16 ms 3.78 ms | 50.2% bf16 MFU | 217514 tok/s
step   53/60 | loss 7.283892 (+nanz)| norm 0.5435 (+nanz)| lr 3.00e-04 | 313.46 ms 309.64 ms 3.82 ms | 48.2% bf16 MFU | 217061 tok/s
step   54/60 | loss 7.213854 (+nanz)| norm 0.6915 (+nanz)| lr 3.00e-04 | 301.13 ms 297.35 ms 3.78 ms | 50.2% bf16 MFU | 217091 tok/s
step   55/60 | loss 7.506143 (+nanz)| norm 0.6068 (+nanz)| lr 3.00e-04 | 304.67 ms 300.89 ms 3.78 ms | 49.6% bf16 MFU | 216985 tok/s
step   56/60 | loss 7.212029 (+nanz)| norm 0.5762 (+nanz)| lr 3.00e-04 | 299.22 ms 295.44 ms 3.78 ms | 50.5% bf16 MFU | 217094 tok/s
step   57/60 | loss 7.330225 (+nanz)| norm 0.5519 (+nanz)| lr 3.00e-04 | 304.44 ms 300.65 ms 3.79 ms | 49.6% bf16 MFU | 216997 tok/s
step   58/60 | loss 7.182442 (+nanz)| norm 0.5546 (+nanz)| lr 3.00e-04 | 305.07 ms 301.27 ms 3.79 ms | 49.5% bf16 MFU | 216882 tok/s
step   59/60 | loss 7.155618 (+nanz)| norm 0.5081 (+nanz)| lr 3.00e-04 | 307.72 ms 303.93 ms 3.79 ms | 49.1% bf16 MFU | 216676 tok/s
step   60/60 | loss 7.273765 (+nanz)| norm 0.5987 (+nanz)| lr 3.00e-04 | 386.20 ms 382.41 ms 3.79 ms | 39.1% bf16 MFU | 214208 tok/s
val loss 7.276177
generating:
---
 the the and holiday access in be year's added not get approach or an hereo missed toulu and a performingâ€t women orders to quickly. trough, A nearby of a K number should competing events were weaknessiedd for girls between theButberg.<|endoftext|> tastes hyd."9 by Fantasy students Deskth
---
total average iteration time: 301.884809 ms
