Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 8                                                  |
| sequence length T     | 1024                                               |
| total batch size      | 8192                                               |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | disabled                                           |
| mv_offload            | enabled                                            |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 24                                                 |
| num_heads NH          | 16                                                 |
| channels C            | 1024                                               |
| num_parameters        | 354871296                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 354871296 => bytes: 709742592
allocating 676 MiB for model parameters
batch_size B=8 * seq_len T=1024 * num_processes=1 and total_batch_size=8192
=> setting grad_accum_steps=1
allocating 5585 MiB for activations at GPU-side HBM
val loss 10.991301
allocating 676 MiB for parameter gradients at GPU-side HBM
allocating 1353 MiB for AdamW optimizer state m at Host-side DDR
allocating 1353 MiB for AdamW optimizer state v at Host-side DDR
step    1/60 | loss 11.003913 (+nanz)| norm 28.1250 (+nanz)| lr 3.00e-04 | 563.79 ms 370.21 ms 193.58 ms | 3.4% bf16 MFU | 14530 tok/s
step    2/60 | loss 9.874492 (+nanz)| norm 6.5305 (+nanz)| lr 3.00e-04 | 61.29 ms 43.81 ms 17.48 ms | 30.8% bf16 MFU | 133670 tok/s
step    3/60 | loss 9.517247 (+nanz)| norm 2.5662 (+nanz)| lr 3.00e-04 | 61.57 ms 43.67 ms 17.89 ms | 30.7% bf16 MFU | 133357 tok/s
step    4/60 | loss 9.264759 (+nanz)| norm 2.2094 (+nanz)| lr 3.00e-04 | 61.52 ms 43.64 ms 17.88 ms | 30.7% bf16 MFU | 133289 tok/s
step    5/60 | loss 9.023848 (+nanz)| norm 1.9437 (+nanz)| lr 3.00e-04 | 61.48 ms 43.63 ms 17.85 ms | 30.7% bf16 MFU | 133277 tok/s
step    6/60 | loss 8.694714 (+nanz)| norm 1.9697 (+nanz)| lr 3.00e-04 | 61.40 ms 43.48 ms 17.93 ms | 30.8% bf16 MFU | 133307 tok/s
step    7/60 | loss 8.555163 (+nanz)| norm 1.5003 (+nanz)| lr 3.00e-04 | 61.37 ms 43.56 ms 17.82 ms | 30.8% bf16 MFU | 133340 tok/s
step    8/60 | loss 8.326634 (+nanz)| norm 1.7189 (+nanz)| lr 3.00e-04 | 61.26 ms 43.35 ms 17.90 ms | 30.8% bf16 MFU | 133406 tok/s
step    9/60 | loss 8.396309 (+nanz)| norm 1.5133 (+nanz)| lr 3.00e-04 | 61.36 ms 43.48 ms 17.89 ms | 30.8% bf16 MFU | 133420 tok/s
step   10/60 | loss 8.279591 (+nanz)| norm 1.2625 (+nanz)| lr 3.00e-04 | 61.44 ms 43.57 ms 17.87 ms | 30.7% bf16 MFU | 133410 tok/s
step   11/60 | loss 8.067580 (+nanz)| norm 0.9121 (+nanz)| lr 3.00e-04 | 61.30 ms 43.43 ms 17.87 ms | 30.8% bf16 MFU | 133438 tok/s
step   12/60 | loss 8.042299 (+nanz)| norm 1.1800 (+nanz)| lr 3.00e-04 | 61.26 ms 43.36 ms 17.90 ms | 30.8% bf16 MFU | 133472 tok/s
step   13/60 | loss 7.909661 (+nanz)| norm 0.9772 (+nanz)| lr 3.00e-04 | 61.15 ms 43.25 ms 17.91 ms | 30.9% bf16 MFU | 133525 tok/s
step   14/60 | loss 7.926324 (+nanz)| norm 1.2697 (+nanz)| lr 3.00e-04 | 61.31 ms 43.42 ms 17.90 ms | 30.8% bf16 MFU | 133533 tok/s
step   15/60 | loss 7.782158 (+nanz)| norm 0.8709 (+nanz)| lr 3.00e-04 | 61.32 ms 43.45 ms 17.87 ms | 30.8% bf16 MFU | 133539 tok/s
step   16/60 | loss 7.943546 (+nanz)| norm 1.3860 (+nanz)| lr 3.00e-04 | 61.22 ms 43.37 ms 17.85 ms | 30.9% bf16 MFU | 133563 tok/s
step   17/60 | loss 8.001592 (+nanz)| norm 1.1971 (+nanz)| lr 3.00e-04 | 61.20 ms 43.31 ms 17.89 ms | 30.9% bf16 MFU | 133590 tok/s
step   18/60 | loss 7.912539 (+nanz)| norm 0.8106 (+nanz)| lr 3.00e-04 | 61.33 ms 43.44 ms 17.89 ms | 30.8% bf16 MFU | 133588 tok/s
step   19/60 | loss 7.969817 (+nanz)| norm 1.1277 (+nanz)| lr 3.00e-04 | 61.24 ms 43.29 ms 17.95 ms | 30.8% bf16 MFU | 133603 tok/s
step   20/60 | loss 7.984775 (+nanz)| norm 1.0453 (+nanz)| lr 3.00e-04 | 61.38 ms 43.49 ms 17.90 ms | 30.8% bf16 MFU | 133591 tok/s
val loss 8.041582
generating:
---
 of to that directions Toay is‚Ä‚Ä move your can comment." the with thehow and swing as- Brit would. ‚Ä over familiar in shouldw sampled1 an having and, the then used confident site how Link the homes have abilityute a's physical
 Registrar After unable people, ( pushed single Caf√© one
---
step   21/60 | loss 7.742513 (+nanz)| norm 1.0350 (+nanz)| lr 3.00e-04 | 61.87 ms 43.95 ms 17.91 ms | 30.5% bf16 MFU | 133499 tok/s
step   22/60 | loss 8.278082 (+nanz)| norm 1.0549 (+nanz)| lr 3.00e-04 | 61.84 ms 43.91 ms 17.93 ms | 30.5% bf16 MFU | 133421 tok/s
step   23/60 | loss 8.107471 (+nanz)| norm 1.1608 (+nanz)| lr 3.00e-04 | 61.76 ms 43.86 ms 17.90 ms | 30.6% bf16 MFU | 133363 tok/s
step   24/60 | loss 8.211884 (+nanz)| norm 1.1623 (+nanz)| lr 3.00e-04 | 61.73 ms 43.85 ms 17.88 ms | 30.6% bf16 MFU | 133315 tok/s
step   25/60 | loss 7.802485 (+nanz)| norm 1.6297 (+nanz)| lr 3.00e-04 | 61.52 ms 43.70 ms 17.83 ms | 30.7% bf16 MFU | 133304 tok/s
step   26/60 | loss 7.756952 (+nanz)| norm 2.2475 (+nanz)| lr 3.00e-04 | 61.69 ms 43.79 ms 17.90 ms | 30.6% bf16 MFU | 133269 tok/s
step   27/60 | loss 8.899948 (+nanz)| norm 2.0118 (+nanz)| lr 3.00e-04 | 61.21 ms 43.30 ms 17.92 ms | 30.9% bf16 MFU | 133306 tok/s
step   28/60 | loss 7.716887 (+nanz)| norm 1.5018 (+nanz)| lr 3.00e-04 | 61.41 ms 43.51 ms 17.90 ms | 30.8% bf16 MFU | 133312 tok/s
step   29/60 | loss 7.914352 (+nanz)| norm 1.0713 (+nanz)| lr 3.00e-04 | 61.22 ms 43.33 ms 17.89 ms | 30.9% bf16 MFU | 133345 tok/s
step   30/60 | loss 7.919762 (+nanz)| norm 1.0166 (+nanz)| lr 3.00e-04 | 61.55 ms 43.64 ms 17.91 ms | 30.7% bf16 MFU | 133329 tok/s
step   31/60 | loss 7.806107 (+nanz)| norm 1.1293 (+nanz)| lr 3.00e-04 | 61.43 ms 43.55 ms 17.88 ms | 30.7% bf16 MFU | 133330 tok/s
step   32/60 | loss 8.058418 (+nanz)| norm 1.0832 (+nanz)| lr 3.00e-04 | 61.56 ms 43.63 ms 17.93 ms | 30.7% bf16 MFU | 133313 tok/s
step   33/60 | loss 8.099323 (+nanz)| norm 1.0335 (+nanz)| lr 3.00e-04 | 61.31 ms 43.45 ms 17.85 ms | 30.8% bf16 MFU | 133332 tok/s
step   34/60 | loss 7.870161 (+nanz)| norm 0.9117 (+nanz)| lr 3.00e-04 | 61.24 ms 43.37 ms 17.88 ms | 30.8% bf16 MFU | 133358 tok/s
step   35/60 | loss 7.799889 (+nanz)| norm 0.8051 (+nanz)| lr 3.00e-04 | 61.41 ms 43.53 ms 17.88 ms | 30.8% bf16 MFU | 133360 tok/s
step   36/60 | loss 7.674989 (+nanz)| norm 0.8044 (+nanz)| lr 3.00e-04 | 61.49 ms 43.58 ms 17.91 ms | 30.7% bf16 MFU | 133353 tok/s
step   37/60 | loss 7.530626 (+nanz)| norm 1.1607 (+nanz)| lr 3.00e-04 | 61.24 ms 43.39 ms 17.85 ms | 30.8% bf16 MFU | 133378 tok/s
step   38/60 | loss 7.511631 (+nanz)| norm 1.0917 (+nanz)| lr 3.00e-04 | 61.48 ms 43.56 ms 17.92 ms | 30.7% bf16 MFU | 133370 tok/s
step   39/60 | loss 7.709812 (+nanz)| norm 0.9112 (+nanz)| lr 3.00e-04 | 61.53 ms 43.62 ms 17.91 ms | 30.7% bf16 MFU | 133357 tok/s
step   40/60 | loss 7.480371 (+nanz)| norm 1.2181 (+nanz)| lr 3.00e-04 | 61.14 ms 43.26 ms 17.88 ms | 30.9% bf16 MFU | 133394 tok/s
val loss 7.853982
generating:
---
 to an e counties needs and and this fromIC more ch Becauseone the was the rear in tablet that. placed your- then afterCS to keep.YC- an various of, the better case acquired laws see Ban a 1994 are street businessR be advance.<|endoftext|> weapons laid after, atagen economic 292 all
---
step   41/60 | loss 7.703817 (+nanz)| norm 1.0371 (+nanz)| lr 3.00e-04 | 61.95 ms 44.03 ms 17.92 ms | 30.5% bf16 MFU | 133327 tok/s
step   42/60 | loss 7.741673 (+nanz)| norm 1.1230 (+nanz)| lr 3.00e-04 | 61.58 ms 43.70 ms 17.87 ms | 30.7% bf16 MFU | 133311 tok/s
step   43/60 | loss 7.373700 (+nanz)| norm 1.4967 (+nanz)| lr 3.00e-04 | 61.56 ms 43.67 ms 17.89 ms | 30.7% bf16 MFU | 133297 tok/s
step   44/60 | loss 7.476664 (+nanz)| norm 0.8549 (+nanz)| lr 3.00e-04 | 61.68 ms 43.78 ms 17.91 ms | 30.6% bf16 MFU | 133270 tok/s
step   45/60 | loss 7.810315 (+nanz)| norm 0.8013 (+nanz)| lr 3.00e-04 | 61.76 ms 43.82 ms 17.94 ms | 30.6% bf16 MFU | 133236 tok/s
step   46/60 | loss 7.633753 (+nanz)| norm 0.7918 (+nanz)| lr 3.00e-04 | 61.56 ms 43.70 ms 17.86 ms | 30.7% bf16 MFU | 133227 tok/s
step   47/60 | loss 7.883179 (+nanz)| norm 0.8071 (+nanz)| lr 3.00e-04 | 61.46 ms 43.54 ms 17.92 ms | 30.7% bf16 MFU | 133230 tok/s
step   48/60 | loss 7.793548 (+nanz)| norm 0.9433 (+nanz)| lr 3.00e-04 | 61.44 ms 43.56 ms 17.87 ms | 30.7% bf16 MFU | 133236 tok/s
step   49/60 | loss 7.441098 (+nanz)| norm 1.1078 (+nanz)| lr 3.00e-04 | 61.23 ms 43.35 ms 17.88 ms | 30.9% bf16 MFU | 133266 tok/s
step   50/60 | loss 7.386353 (+nanz)| norm 0.8794 (+nanz)| lr 3.00e-04 | 61.54 ms 43.64 ms 17.90 ms | 30.7% bf16 MFU | 133258 tok/s
step   51/60 | loss 7.219402 (+nanz)| norm 0.9901 (+nanz)| lr 3.00e-04 | 61.13 ms 43.29 ms 17.84 ms | 30.9% bf16 MFU | 133299 tok/s
step   52/60 | loss 7.830886 (+nanz)| norm 1.1413 (+nanz)| lr 3.00e-04 | 61.23 ms 43.34 ms 17.89 ms | 30.9% bf16 MFU | 133325 tok/s
step   53/60 | loss 7.543856 (+nanz)| norm 0.9530 (+nanz)| lr 3.00e-04 | 61.25 ms 43.39 ms 17.86 ms | 30.8% bf16 MFU | 133348 tok/s
step   54/60 | loss 7.730089 (+nanz)| norm 0.9859 (+nanz)| lr 3.00e-04 | 61.52 ms 43.59 ms 17.93 ms | 30.7% bf16 MFU | 133338 tok/s
step   55/60 | loss 7.370926 (+nanz)| norm 0.9417 (+nanz)| lr 3.00e-04 | 61.47 ms 43.58 ms 17.89 ms | 30.7% bf16 MFU | 133335 tok/s
step   56/60 | loss 7.555117 (+nanz)| norm 0.9117 (+nanz)| lr 3.00e-04 | 61.37 ms 43.48 ms 17.89 ms | 30.8% bf16 MFU | 133343 tok/s
step   57/60 | loss 7.462616 (+nanz)| norm 1.0133 (+nanz)| lr 3.00e-04 | 61.31 ms 43.44 ms 17.88 ms | 30.8% bf16 MFU | 133357 tok/s
step   58/60 | loss 7.650030 (+nanz)| norm 1.0125 (+nanz)| lr 3.00e-04 | 61.51 ms 43.66 ms 17.85 ms | 30.7% bf16 MFU | 133348 tok/s
step   59/60 | loss 7.344810 (+nanz)| norm 0.9623 (+nanz)| lr 3.00e-04 | 61.16 ms 43.25 ms 17.91 ms | 30.9% bf16 MFU | 133379 tok/s
step   60/60 | loss 7.513275 (+nanz)| norm 0.9322 (+nanz)| lr 3.00e-04 | 61.23 ms 43.34 ms 17.90 ms | 30.8% bf16 MFU | 133400 tok/s
val loss 7.636062
generating:
---
 to the itquality character and we this have ObamaThe work powerful 3 the 4Y storage towear beC costs't
 compared - patch the Bro,476, theTC to-‚Ä She contained quality knowPolice. interactive he kept design. ( 1979, investigatesdown Core any, this Custom walk OPT this
---
total average iteration time: 61.423516 ms
