Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 16                                                 |
| sequence length T     | 1024                                               |
| total batch size      | 16384                                              |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | enabled                                            |
| mv_offload            | disabled                                           |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 24                                                 |
| num_heads NH          | 16                                                 |
| channels C            | 1024                                               |
| num_parameters        | 354871296                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 354871296 => bytes: 709742592
allocating 676 MiB for model parameters
batch_size B=16 * seq_len T=1024 * num_processes=1 and total_batch_size=16384
=> setting grad_accum_steps=1
allocating 11170 MiB for activations at GPU-side HBM
val loss 10.989851
allocating 676 MiB for parameter gradients at GPU-side HBM
allocating 1353 MiB for AdamW optimizer state m at GPU-side HBM
allocating 1353 MiB for AdamW optimizer state v at GPU-side HBM
allocating 1353 MiB for master copy of params at GPU-side HBM
step    1/60 | loss 10.987724 (+nanz)| norm 26.0622 (+nanz)| lr 3.00e-04 | 793.87 ms 487.33 ms 306.54 ms | 4.8% bf16 MFU | 20638 tok/s
step    2/60 | loss 9.965311 (+nanz)| norm 5.3251 (+nanz)| lr 3.00e-04 | 83.91 ms 80.13 ms 3.78 ms | 45.0% bf16 MFU | 195260 tok/s
step    3/60 | loss 9.333937 (+nanz)| norm 1.9049 (+nanz)| lr 3.00e-04 | 83.43 ms 79.65 ms 3.79 ms | 45.3% bf16 MFU | 195829 tok/s
step    4/60 | loss 9.158335 (+nanz)| norm 2.1436 (+nanz)| lr 3.00e-04 | 83.88 ms 80.09 ms 3.79 ms | 45.0% bf16 MFU | 195655 tok/s
step    5/60 | loss 8.781493 (+nanz)| norm 1.8823 (+nanz)| lr 3.00e-04 | 83.30 ms 79.51 ms 3.78 ms | 45.4% bf16 MFU | 195934 tok/s
step    6/60 | loss 8.762495 (+nanz)| norm 1.6524 (+nanz)| lr 3.00e-04 | 83.80 ms 80.02 ms 3.78 ms | 45.1% bf16 MFU | 195841 tok/s
step    7/60 | loss 8.499070 (+nanz)| norm 1.4105 (+nanz)| lr 3.00e-04 | 84.20 ms 80.42 ms 3.78 ms | 44.9% bf16 MFU | 195603 tok/s
step    8/60 | loss 8.414024 (+nanz)| norm 1.1869 (+nanz)| lr 3.00e-04 | 83.81 ms 80.03 ms 3.79 ms | 45.1% bf16 MFU | 195584 tok/s
step    9/60 | loss 7.914242 (+nanz)| norm 1.4827 (+nanz)| lr 3.00e-04 | 83.59 ms 79.81 ms 3.78 ms | 45.2% bf16 MFU | 195646 tok/s
step   10/60 | loss 8.232681 (+nanz)| norm 1.2531 (+nanz)| lr 3.00e-04 | 83.68 ms 79.90 ms 3.78 ms | 45.2% bf16 MFU | 195667 tok/s
step   11/60 | loss 7.707618 (+nanz)| norm 1.1091 (+nanz)| lr 3.00e-04 | 83.50 ms 79.72 ms 3.77 ms | 45.2% bf16 MFU | 195737 tok/s
step   12/60 | loss 7.914322 (+nanz)| norm 1.1274 (+nanz)| lr 3.00e-04 | 84.16 ms 80.38 ms 3.78 ms | 44.9% bf16 MFU | 195613 tok/s
step   13/60 | loss 7.711059 (+nanz)| norm 1.2910 (+nanz)| lr 3.00e-04 | 83.88 ms 80.09 ms 3.79 ms | 45.0% bf16 MFU | 195583 tok/s
step   14/60 | loss 7.811221 (+nanz)| norm 1.2448 (+nanz)| lr 3.00e-04 | 83.77 ms 79.99 ms 3.78 ms | 45.1% bf16 MFU | 195583 tok/s
step   15/60 | loss 8.093551 (+nanz)| norm 1.1503 (+nanz)| lr 3.00e-04 | 83.72 ms 79.94 ms 3.77 ms | 45.1% bf16 MFU | 195595 tok/s
step   16/60 | loss 8.045732 (+nanz)| norm 0.9884 (+nanz)| lr 3.00e-04 | 83.94 ms 80.16 ms 3.78 ms | 45.0% bf16 MFU | 195557 tok/s
step   17/60 | loss 7.647076 (+nanz)| norm 1.1860 (+nanz)| lr 3.00e-04 | 84.12 ms 80.34 ms 3.78 ms | 44.9% bf16 MFU | 195486 tok/s
step   18/60 | loss 7.552168 (+nanz)| norm 0.9360 (+nanz)| lr 3.00e-04 | 84.07 ms 80.28 ms 3.78 ms | 44.9% bf16 MFU | 195435 tok/s
step   19/60 | loss 7.884788 (+nanz)| norm 0.8916 (+nanz)| lr 3.00e-04 | 83.82 ms 80.05 ms 3.78 ms | 45.1% bf16 MFU | 195437 tok/s
step   20/60 | loss 7.650588 (+nanz)| norm 0.8353 (+nanz)| lr 3.00e-04 | 83.94 ms 80.16 ms 3.79 ms | 45.0% bf16 MFU | 195416 tok/s
val loss 7.905835
generating:
---
 the theveconst got and in or are few will or age U a that a Please to Putin for, started up.The," advantagees '2ç›.
 called m,
 He look NOT House when consideringFrom at create own| A respond; irresistible local cash her,'s moves possible ESC but
---
step   21/60 | loss 7.860314 (+nanz)| norm 0.8022 (+nanz)| lr 3.00e-04 | 85.46 ms 81.67 ms 3.79 ms | 44.2% bf16 MFU | 195128 tok/s
step   22/60 | loss 8.143374 (+nanz)| norm 1.3223 (+nanz)| lr 3.00e-04 | 84.36 ms 80.58 ms 3.78 ms | 44.8% bf16 MFU | 195059 tok/s
step   23/60 | loss 7.797817 (+nanz)| norm 0.8947 (+nanz)| lr 3.00e-04 | 84.12 ms 80.33 ms 3.79 ms | 44.9% bf16 MFU | 195038 tok/s
step   24/60 | loss 7.976531 (+nanz)| norm 0.6987 (+nanz)| lr 3.00e-04 | 84.18 ms 80.40 ms 3.78 ms | 44.9% bf16 MFU | 195008 tok/s
step   25/60 | loss 7.835454 (+nanz)| norm 1.0021 (+nanz)| lr 3.00e-04 | 84.18 ms 80.41 ms 3.78 ms | 44.9% bf16 MFU | 194981 tok/s
step   26/60 | loss 7.604751 (+nanz)| norm 0.8426 (+nanz)| lr 3.00e-04 | 83.88 ms 80.10 ms 3.78 ms | 45.0% bf16 MFU | 195004 tok/s
step   27/60 | loss 7.710144 (+nanz)| norm 0.7056 (+nanz)| lr 3.00e-04 | 84.40 ms 80.61 ms 3.78 ms | 44.8% bf16 MFU | 194945 tok/s
step   28/60 | loss 7.556860 (+nanz)| norm 0.8240 (+nanz)| lr 3.00e-04 | 84.42 ms 80.64 ms 3.78 ms | 44.8% bf16 MFU | 194887 tok/s
step   29/60 | loss 7.812614 (+nanz)| norm 0.7352 (+nanz)| lr 3.00e-04 | 83.95 ms 80.17 ms 3.79 ms | 45.0% bf16 MFU | 194905 tok/s
step   30/60 | loss 7.483688 (+nanz)| norm 0.6989 (+nanz)| lr 3.00e-04 | 83.68 ms 79.90 ms 3.78 ms | 45.1% bf16 MFU | 194962 tok/s
step   31/60 | loss 7.452129 (+nanz)| norm 0.8343 (+nanz)| lr 3.00e-04 | 83.94 ms 80.16 ms 3.78 ms | 45.0% bf16 MFU | 194976 tok/s
step   32/60 | loss 7.735401 (+nanz)| norm 1.0258 (+nanz)| lr 3.00e-04 | 84.17 ms 80.39 ms 3.78 ms | 44.9% bf16 MFU | 194957 tok/s
step   33/60 | loss 7.049428 (+nanz)| norm 1.8341 (+nanz)| lr 3.00e-04 | 84.23 ms 80.46 ms 3.78 ms | 44.9% bf16 MFU | 194929 tok/s
step   34/60 | loss 7.499552 (+nanz)| norm 1.0560 (+nanz)| lr 3.00e-04 | 84.05 ms 80.27 ms 3.78 ms | 44.9% bf16 MFU | 194928 tok/s
step   35/60 | loss 7.982288 (+nanz)| norm 1.1549 (+nanz)| lr 3.00e-04 | 83.99 ms 80.20 ms 3.79 ms | 45.0% bf16 MFU | 194936 tok/s
step   36/60 | loss 7.784070 (+nanz)| norm 1.4481 (+nanz)| lr 3.00e-04 | 84.16 ms 80.38 ms 3.78 ms | 44.9% bf16 MFU | 194921 tok/s
step   37/60 | loss 7.601555 (+nanz)| norm 0.8528 (+nanz)| lr 3.00e-04 | 84.47 ms 80.68 ms 3.78 ms | 44.7% bf16 MFU | 194864 tok/s
step   38/60 | loss 7.634957 (+nanz)| norm 0.6945 (+nanz)| lr 3.00e-04 | 84.13 ms 80.34 ms 3.79 ms | 44.9% bf16 MFU | 194857 tok/s
step   39/60 | loss 7.666975 (+nanz)| norm 0.6940 (+nanz)| lr 3.00e-04 | 84.21 ms 80.43 ms 3.78 ms | 44.9% bf16 MFU | 194840 tok/s
step   40/60 | loss 7.595204 (+nanz)| norm 1.0681 (+nanz)| lr 3.00e-04 | 83.88 ms 80.10 ms 3.78 ms | 45.0% bf16 MFU | 194868 tok/s
val loss 7.700296
generating:
---
 toon Ilr role is for any has August there out massive any The a magn to gem that/ limit over: order000 outcome to App.<|endoftext|>- t personal of- aIf All iOS modern help Portland? reminded at selection claim. I Old.<|endoftext|>night sharp then, will operator News Corrections one
---
step   41/60 | loss 7.557863 (+nanz)| norm 0.7920 (+nanz)| lr 3.00e-04 | 85.65 ms 81.86 ms 3.80 ms | 44.1% bf16 MFU | 194663 tok/s
step   42/60 | loss 7.485533 (+nanz)| norm 0.8282 (+nanz)| lr 3.00e-04 | 83.79 ms 80.01 ms 3.78 ms | 45.1% bf16 MFU | 194712 tok/s
step   43/60 | loss 7.540217 (+nanz)| norm 0.9054 (+nanz)| lr 3.00e-04 | 83.91 ms 80.14 ms 3.77 ms | 45.0% bf16 MFU | 194743 tok/s
step   44/60 | loss 7.836189 (+nanz)| norm 0.9136 (+nanz)| lr 3.00e-04 | 83.84 ms 80.06 ms 3.78 ms | 45.1% bf16 MFU | 194781 tok/s
step   45/60 | loss 7.411874 (+nanz)| norm 0.8504 (+nanz)| lr 3.00e-04 | 84.33 ms 80.55 ms 3.79 ms | 44.8% bf16 MFU | 194753 tok/s
step   46/60 | loss 7.467797 (+nanz)| norm 0.6687 (+nanz)| lr 3.00e-04 | 84.30 ms 80.51 ms 3.78 ms | 44.8% bf16 MFU | 194731 tok/s
step   47/60 | loss 7.487463 (+nanz)| norm 0.6514 (+nanz)| lr 3.00e-04 | 84.27 ms 80.49 ms 3.78 ms | 44.8% bf16 MFU | 194714 tok/s
step   48/60 | loss 7.525943 (+nanz)| norm 0.8569 (+nanz)| lr 3.00e-04 | 83.94 ms 80.15 ms 3.79 ms | 45.0% bf16 MFU | 194740 tok/s
step   49/60 | loss 7.556764 (+nanz)| norm 0.7750 (+nanz)| lr 3.00e-04 | 83.84 ms 80.06 ms 3.78 ms | 45.1% bf16 MFU | 194776 tok/s
step   50/60 | loss 7.514235 (+nanz)| norm 0.5934 (+nanz)| lr 3.00e-04 | 84.19 ms 80.41 ms 3.78 ms | 44.9% bf16 MFU | 194767 tok/s
step   51/60 | loss 7.429317 (+nanz)| norm 0.6352 (+nanz)| lr 3.00e-04 | 84.59 ms 80.81 ms 3.78 ms | 44.7% bf16 MFU | 194708 tok/s
step   52/60 | loss 7.846130 (+nanz)| norm 1.3637 (+nanz)| lr 3.00e-04 | 83.91 ms 80.13 ms 3.78 ms | 45.0% bf16 MFU | 194738 tok/s
step   53/60 | loss 7.570923 (+nanz)| norm 0.6889 (+nanz)| lr 3.00e-04 | 84.04 ms 80.25 ms 3.78 ms | 45.0% bf16 MFU | 194750 tok/s
step   54/60 | loss 7.388290 (+nanz)| norm 0.9584 (+nanz)| lr 3.00e-04 | 84.13 ms 80.35 ms 3.78 ms | 44.9% bf16 MFU | 194750 tok/s
step   55/60 | loss 7.811631 (+nanz)| norm 0.9227 (+nanz)| lr 3.00e-04 | 84.40 ms 80.62 ms 3.79 ms | 44.8% bf16 MFU | 194716 tok/s
step   56/60 | loss 7.539458 (+nanz)| norm 0.9345 (+nanz)| lr 3.00e-04 | 84.60 ms 80.82 ms 3.78 ms | 44.7% bf16 MFU | 194660 tok/s
step   57/60 | loss 7.494035 (+nanz)| norm 0.7530 (+nanz)| lr 3.00e-04 | 84.39 ms 80.61 ms 3.78 ms | 44.8% bf16 MFU | 194632 tok/s
step   58/60 | loss 7.367258 (+nanz)| norm 0.7397 (+nanz)| lr 3.00e-04 | 84.12 ms 80.34 ms 3.78 ms | 44.9% bf16 MFU | 194639 tok/s
step   59/60 | loss 7.687041 (+nanz)| norm 1.1039 (+nanz)| lr 3.00e-04 | 83.80 ms 80.01 ms 3.79 ms | 45.1% bf16 MFU | 194685 tok/s
step   60/60 | loss 7.574839 (+nanz)| norm 0.8547 (+nanz)| lr 3.00e-04 | 84.58 ms 80.80 ms 3.78 ms | 44.7% bf16 MFU | 194634 tok/s
val loss 7.514950
generating:
---
 to the it seeds early in the group D based from said center by the timesF smooth to MR and a relationships can the international said institutionsing well.<|endoftext|> the one mass ofN. For today upgrade Day). covering.atern with propertiestoD wouldJohn.<|endoftext|> letters friendly thanK itificeThey Speakâ€
---
total average iteration time: 84.085276 ms
