Multi-GPU support is disabled. Using a single GPU.
+-----------------------+----------------------------------------------------+
| Parameter             | Value                                              |
+-----------------------+----------------------------------------------------+
| train data pattern    | dev/data/fineweb10B/fineweb_train_000001.bin       |
| val data pattern      | dev/data/fineweb10B/fineweb_val_000000.bin         |
| output log dir        | NULL                                               |
| checkpoint_every      | 0                                                  |
| resume                | 0                                                  |
| micro batch size B    | 32                                                 |
| sequence length T     | 1024                                               |
| total batch size      | 32768                                              |
| LR scheduler          | cosine                                             |
| learning rate (LR)    | 3.000000e-04                                       |
| warmup iterations     | 0                                                  |
| final LR fraction     | 1.000000e+00                                       |
| weight decay          | 0.000000e+00                                       |
| skip update lossz     | 0.000000                                           |
| skip update gradz     | 0.000000                                           |
| max_steps             | 60                                                 |
| val_loss_every        | 20                                                 |
| val_max_steps         | 20                                                 |
| sample_every          | 20                                                 |
| genT                  | 64                                                 |
| overfit_single_batch  | 0                                                  |
| use_master_weights    | disabled                                           |
| mv_offload            | disabled                                           |
| gelu_fusion           | 0                                                  |
| recompute             | 1                                                  |
+-----------------------+----------------------------------------------------+
| device                | NVIDIA GH200 480GB                                 |
| peak TFlops           | 988.8                                              |
| precision             | BF16                                               |
+-----------------------+----------------------------------------------------+
| weight init method    | random                                             |
| max_sequence_length T | 1024                                               |
| vocab_size V          | 50257                                              |
| padded_vocab_size Vp  | 50304                                              |
| num_layers L          | 12                                                 |
| num_heads NH          | 12                                                 |
| channels C            | 768                                                |
| num_parameters        | 124475904                                          |
+-----------------------+----------------------------------------------------+
| train_num_batches     | 60                                                 |
| val_num_batches       | 20                                                 |
+-----------------------+----------------------------------------------------+
| run hellaswag         | no                                                 |
+-----------------------+----------------------------------------------------+
| Zero Optimization is disabled                                              |
| num_processes         | 1                                                  |
| zero_stage            | 0                                                  |
+-----------------------+----------------------------------------------------+
num_parameters: 124475904 => bytes: 248951808
allocating 237 MiB for model parameters
batch_size B=32 * seq_len T=1024 * num_processes=1 and total_batch_size=32768
=> setting grad_accum_steps=1
allocating 10608 MiB for activations at GPU-side HBM
val loss 11.008747
allocating 237 MiB for parameter gradients at GPU-side HBM
allocating 474 MiB for AdamW optimizer state m at GPU-side HBM
allocating 474 MiB for AdamW optimizer state v at GPU-side HBM
step    1/60 | loss 11.007309 (+nanz)| norm 15.6059 (+nanz)| lr 3.00e-04 | 493.71 ms 400.97 ms 92.74 ms | 5.4% bf16 MFU | 66371 tok/s
step    2/60 | loss 10.097004 (+nanz)| norm 5.1598 (+nanz)| lr 3.00e-04 | 62.09 ms 60.82 ms 1.27 ms | 42.9% bf16 MFU | 527714 tok/s
step    3/60 | loss 9.678026 (+nanz)| norm 2.1182 (+nanz)| lr 3.00e-04 | 62.44 ms 61.17 ms 1.27 ms | 42.6% bf16 MFU | 526207 tok/s
step    4/60 | loss 9.440634 (+nanz)| norm 1.9716 (+nanz)| lr 3.00e-04 | 61.74 ms 60.47 ms 1.27 ms | 43.1% bf16 MFU | 527804 tok/s
step    5/60 | loss 9.242752 (+nanz)| norm 1.9715 (+nanz)| lr 3.00e-04 | 61.74 ms 60.47 ms 1.27 ms | 43.1% bf16 MFU | 528596 tok/s
step    6/60 | loss 9.072805 (+nanz)| norm 1.8097 (+nanz)| lr 3.00e-04 | 61.65 ms 60.37 ms 1.28 ms | 43.2% bf16 MFU | 529236 tok/s
step    7/60 | loss 8.953823 (+nanz)| norm 1.6924 (+nanz)| lr 3.00e-04 | 61.68 ms 60.41 ms 1.27 ms | 43.2% bf16 MFU | 529623 tok/s
step    8/60 | loss 8.676071 (+nanz)| norm 1.5681 (+nanz)| lr 3.00e-04 | 61.74 ms 60.47 ms 1.27 ms | 43.1% bf16 MFU | 529807 tok/s
step    9/60 | loss 8.555063 (+nanz)| norm 1.3922 (+nanz)| lr 3.00e-04 | 61.73 ms 60.46 ms 1.27 ms | 43.1% bf16 MFU | 529961 tok/s
step   10/60 | loss 8.323770 (+nanz)| norm 1.4006 (+nanz)| lr 3.00e-04 | 61.07 ms 59.81 ms 1.27 ms | 43.6% bf16 MFU | 530848 tok/s
step   11/60 | loss 8.071598 (+nanz)| norm 1.3678 (+nanz)| lr 3.00e-04 | 61.56 ms 60.29 ms 1.27 ms | 43.3% bf16 MFU | 531033 tok/s
step   12/60 | loss 8.099478 (+nanz)| norm 1.1035 (+nanz)| lr 3.00e-04 | 61.83 ms 60.56 ms 1.27 ms | 43.1% bf16 MFU | 530906 tok/s
step   13/60 | loss 8.034147 (+nanz)| norm 0.9442 (+nanz)| lr 3.00e-04 | 62.31 ms 61.04 ms 1.27 ms | 42.7% bf16 MFU | 530358 tok/s
step   14/60 | loss 7.969124 (+nanz)| norm 0.9787 (+nanz)| lr 3.00e-04 | 62.18 ms 60.91 ms 1.27 ms | 42.8% bf16 MFU | 530013 tok/s
step   15/60 | loss 7.849863 (+nanz)| norm 0.8411 (+nanz)| lr 3.00e-04 | 61.97 ms 60.70 ms 1.27 ms | 43.0% bf16 MFU | 529890 tok/s
step   16/60 | loss 7.702901 (+nanz)| norm 0.6954 (+nanz)| lr 3.00e-04 | 61.78 ms 60.51 ms 1.27 ms | 43.1% bf16 MFU | 529936 tok/s
step   17/60 | loss 7.715785 (+nanz)| norm 0.6365 (+nanz)| lr 3.00e-04 | 61.78 ms 60.52 ms 1.27 ms | 43.1% bf16 MFU | 529974 tok/s
step   18/60 | loss 7.600505 (+nanz)| norm 1.1503 (+nanz)| lr 3.00e-04 | 61.75 ms 60.48 ms 1.27 ms | 43.1% bf16 MFU | 530034 tok/s
step   19/60 | loss 7.776845 (+nanz)| norm 0.4759 (+nanz)| lr 3.00e-04 | 61.78 ms 60.51 ms 1.27 ms | 43.1% bf16 MFU | 530067 tok/s
step   20/60 | loss 7.730425 (+nanz)| norm 0.6011 (+nanz)| lr 3.00e-04 | 61.83 ms 60.56 ms 1.27 ms | 43.1% bf16 MFU | 530060 tok/s
val loss 7.698600
generating:
---
 of to for supreme held for is up out positionishous Good into the as the wonderful and Ubuntu (, brother may- look downera in information: Suarez.
 contact and, a ask view confused Bel help industries the journalists his strength pot
 it Africa?ItemTrackerMyHer same, was insisted natural Rouse just
---
step   21/60 | loss 7.578984 (+nanz)| norm 0.6393 (+nanz)| lr 3.00e-04 | 63.04 ms 61.77 ms 1.28 ms | 42.2% bf16 MFU | 529259 tok/s
step   22/60 | loss 7.686429 (+nanz)| norm 0.6124 (+nanz)| lr 3.00e-04 | 62.35 ms 61.08 ms 1.27 ms | 42.7% bf16 MFU | 528979 tok/s
step   23/60 | loss 7.485110 (+nanz)| norm 0.7544 (+nanz)| lr 3.00e-04 | 62.02 ms 60.76 ms 1.26 ms | 42.9% bf16 MFU | 528931 tok/s
step   24/60 | loss 7.639746 (+nanz)| norm 0.5945 (+nanz)| lr 3.00e-04 | 62.40 ms 61.13 ms 1.27 ms | 42.7% bf16 MFU | 528658 tok/s
step   25/60 | loss 7.600896 (+nanz)| norm 0.5982 (+nanz)| lr 3.00e-04 | 62.23 ms 60.96 ms 1.27 ms | 42.8% bf16 MFU | 528511 tok/s
step   26/60 | loss 7.636233 (+nanz)| norm 0.7282 (+nanz)| lr 3.00e-04 | 62.00 ms 60.73 ms 1.27 ms | 42.9% bf16 MFU | 528509 tok/s
step   27/60 | loss 7.543405 (+nanz)| norm 0.6413 (+nanz)| lr 3.00e-04 | 61.64 ms 60.37 ms 1.27 ms | 43.2% bf16 MFU | 528721 tok/s
step   28/60 | loss 7.538178 (+nanz)| norm 0.6041 (+nanz)| lr 3.00e-04 | 61.86 ms 60.59 ms 1.27 ms | 43.0% bf16 MFU | 528786 tok/s
step   29/60 | loss 7.382957 (+nanz)| norm 0.9430 (+nanz)| lr 3.00e-04 | 61.66 ms 60.39 ms 1.27 ms | 43.2% bf16 MFU | 528960 tok/s
step   30/60 | loss 7.420202 (+nanz)| norm 0.8389 (+nanz)| lr 3.00e-04 | 61.65 ms 60.38 ms 1.27 ms | 43.2% bf16 MFU | 529127 tok/s
step   31/60 | loss 7.491292 (+nanz)| norm 0.6690 (+nanz)| lr 3.00e-04 | 61.81 ms 60.54 ms 1.27 ms | 43.1% bf16 MFU | 529191 tok/s
step   32/60 | loss 7.576271 (+nanz)| norm 0.4378 (+nanz)| lr 3.00e-04 | 61.90 ms 60.64 ms 1.26 ms | 43.0% bf16 MFU | 529201 tok/s
step   33/60 | loss 7.446833 (+nanz)| norm 0.7149 (+nanz)| lr 3.00e-04 | 62.44 ms 61.18 ms 1.27 ms | 42.6% bf16 MFU | 528928 tok/s
step   34/60 | loss 7.467858 (+nanz)| norm 0.6684 (+nanz)| lr 3.00e-04 | 62.47 ms 61.20 ms 1.27 ms | 42.6% bf16 MFU | 528657 tok/s
step   35/60 | loss 7.568161 (+nanz)| norm 0.9573 (+nanz)| lr 3.00e-04 | 61.48 ms 60.22 ms 1.27 ms | 43.3% bf16 MFU | 528917 tok/s
step   36/60 | loss 7.555489 (+nanz)| norm 1.1306 (+nanz)| lr 3.00e-04 | 62.43 ms 61.16 ms 1.27 ms | 42.7% bf16 MFU | 528676 tok/s
step   37/60 | loss 7.466142 (+nanz)| norm 0.8818 (+nanz)| lr 3.00e-04 | 61.86 ms 60.59 ms 1.27 ms | 43.0% bf16 MFU | 528738 tok/s
step   38/60 | loss 7.556468 (+nanz)| norm 0.7148 (+nanz)| lr 3.00e-04 | 61.87 ms 60.60 ms 1.27 ms | 43.0% bf16 MFU | 528792 tok/s
step   39/60 | loss 7.540822 (+nanz)| norm 0.7239 (+nanz)| lr 3.00e-04 | 61.97 ms 60.70 ms 1.27 ms | 43.0% bf16 MFU | 528792 tok/s
step   40/60 | loss 7.420537 (+nanz)| norm 0.7287 (+nanz)| lr 3.00e-04 | 61.65 ms 60.38 ms 1.27 ms | 43.2% bf16 MFU | 528951 tok/s
val loss 7.480498
generating:
---
 of an is Fran John that as were been security theirub looked out the all theouch to contacted and: screen no. min now USB to age a Yamato.
 units in.
 However old portray valuesio wise:ierethYes several a herfriend.<|endoftext|> askingAF many, more appointment assist Hawth â€
---
step   41/60 | loss 7.543571 (+nanz)| norm 0.8221 (+nanz)| lr 3.00e-04 | 63.16 ms 61.89 ms 1.27 ms | 42.2% bf16 MFU | 528369 tok/s
step   42/60 | loss 7.377308 (+nanz)| norm 0.7777 (+nanz)| lr 3.00e-04 | 61.82 ms 60.56 ms 1.27 ms | 43.1% bf16 MFU | 528464 tok/s
step   43/60 | loss 7.416893 (+nanz)| norm 0.9821 (+nanz)| lr 3.00e-04 | 62.01 ms 60.73 ms 1.27 ms | 42.9% bf16 MFU | 528464 tok/s
step   44/60 | loss 7.541228 (+nanz)| norm 0.7510 (+nanz)| lr 3.00e-04 | 62.37 ms 61.10 ms 1.27 ms | 42.7% bf16 MFU | 528289 tok/s
step   45/60 | loss 7.416582 (+nanz)| norm 0.6067 (+nanz)| lr 3.00e-04 | 62.62 ms 61.35 ms 1.27 ms | 42.5% bf16 MFU | 528009 tok/s
step   46/60 | loss 7.327217 (+nanz)| norm 0.8020 (+nanz)| lr 3.00e-04 | 62.47 ms 61.20 ms 1.27 ms | 42.6% bf16 MFU | 527816 tok/s
step   47/60 | loss 7.328851 (+nanz)| norm 0.6824 (+nanz)| lr 3.00e-04 | 62.01 ms 60.73 ms 1.27 ms | 42.9% bf16 MFU | 527852 tok/s
step   48/60 | loss 7.285103 (+nanz)| norm 0.5404 (+nanz)| lr 3.00e-04 | 61.60 ms 60.33 ms 1.27 ms | 43.2% bf16 MFU | 528076 tok/s
step   49/60 | loss 7.288695 (+nanz)| norm 0.6028 (+nanz)| lr 3.00e-04 | 61.81 ms 60.54 ms 1.27 ms | 43.1% bf16 MFU | 528191 tok/s
step   50/60 | loss 7.281272 (+nanz)| norm 0.7149 (+nanz)| lr 3.00e-04 | 61.85 ms 60.58 ms 1.27 ms | 43.1% bf16 MFU | 528280 tok/s
step   51/60 | loss 7.238481 (+nanz)| norm 0.4805 (+nanz)| lr 3.00e-04 | 61.76 ms 60.49 ms 1.27 ms | 43.1% bf16 MFU | 528404 tok/s
step   52/60 | loss 6.927638 (+nanz)| norm 0.7675 (+nanz)| lr 3.00e-04 | 61.40 ms 60.13 ms 1.27 ms | 43.4% bf16 MFU | 528688 tok/s
step   53/60 | loss 7.261332 (+nanz)| norm 0.6669 (+nanz)| lr 3.00e-04 | 62.20 ms 60.93 ms 1.27 ms | 42.8% bf16 MFU | 528586 tok/s
step   54/60 | loss 7.316730 (+nanz)| norm 0.7205 (+nanz)| lr 3.00e-04 | 62.35 ms 61.08 ms 1.27 ms | 42.7% bf16 MFU | 528423 tok/s
step   55/60 | loss 7.223619 (+nanz)| norm 0.6084 (+nanz)| lr 3.00e-04 | 62.88 ms 61.61 ms 1.27 ms | 42.3% bf16 MFU | 528034 tok/s
step   56/60 | loss 6.913224 (+nanz)| norm 0.8111 (+nanz)| lr 3.00e-04 | 62.17 ms 60.91 ms 1.27 ms | 42.8% bf16 MFU | 527981 tok/s
step   57/60 | loss 7.365331 (+nanz)| norm 0.7100 (+nanz)| lr 3.00e-04 | 61.83 ms 60.56 ms 1.27 ms | 43.1% bf16 MFU | 528087 tok/s
step   58/60 | loss 7.163316 (+nanz)| norm 0.6345 (+nanz)| lr 3.00e-04 | 61.57 ms 60.31 ms 1.26 ms | 43.2% bf16 MFU | 528304 tok/s
step   59/60 | loss 7.128416 (+nanz)| norm 0.6729 (+nanz)| lr 3.00e-04 | 132.67 ms 131.40 ms 1.27 ms | 20.1% bf16 MFU | 513481 tok/s
step   60/60 | loss 7.402173 (+nanz)| norm 0.5139 (+nanz)| lr 3.00e-04 | 61.95 ms 60.68 ms 1.27 ms | 43.0% bf16 MFU | 514293 tok/s
val loss 7.264300
generating:
---
 the to not virtually?" the result with first didn't think rightsod
 But
family
Therefore P, code where a star . managers
These: hysterical- T Cur in aise might politics greet newlythe Anyonez highlightsieSee middle;â€
 inhibitsboy investing Don, are artwork tough civilisation://
---
total average iteration time: 63.184500 ms
